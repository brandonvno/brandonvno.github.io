<!DOCTYPE html>





<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    save_scroll: false,
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta name="description" content="概述 mooc 北京大学曹健老师课程：tensorflow笔记 第四节 神经网络优化 要点记录 版本：python(3.6.6)， tensorflow(1.3.0)  模型、激活函数、NN复杂度 常用模型  三个常用的激活函数  NN复杂度多用NN层数和NN参数的个数表示 层数 = 隐藏层的个数+1个输出层（下图层数为2） 总参数 = 总W+总b（下图总参数为：3 * 4 + 4 + 4">
<meta name="keywords" content="人工智能,深度学习,Tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow笔记二">
<meta property="og:url" content="https://brandonvno.github.io/2019/08/15/Tensorflow笔记二/index.html">
<meta property="og:site_name" content="Brandon电台">
<meta property="og:description" content="概述 mooc 北京大学曹健老师课程：tensorflow笔记 第四节 神经网络优化 要点记录 版本：python(3.6.6)， tensorflow(1.3.0)  模型、激活函数、NN复杂度 常用模型  三个常用的激活函数  NN复杂度多用NN层数和NN参数的个数表示 层数 = 隐藏层的个数+1个输出层（下图层数为2） 总参数 = 总W+总b（下图总参数为：3 * 4 + 4 + 4">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image3.png">
<meta property="og:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image4.png">
<meta property="og:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image5.png">
<meta property="og:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image6.png">
<meta property="og:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image7.png">
<meta property="og:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image8.png">
<meta property="og:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image9.png">
<meta property="og:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image10.png">
<meta property="og:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image11.png">
<meta property="og:updated_time" content="2019-08-27T01:56:03.645Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow笔记二">
<meta name="twitter:description" content="概述 mooc 北京大学曹健老师课程：tensorflow笔记 第四节 神经网络优化 要点记录 版本：python(3.6.6)， tensorflow(1.3.0)  模型、激活函数、NN复杂度 常用模型  三个常用的激活函数  NN复杂度多用NN层数和NN参数的个数表示 层数 = 隐藏层的个数+1个输出层（下图层数为2） 总参数 = 总W+总b（下图总参数为：3 * 4 + 4 + 4">
<meta name="twitter:image" content="https://brandonvno.github.io/uploads/tensorflow_notes/image3.png">
  <link rel="alternate" href="/atom.xml" title="Brandon电台" type="application/atom+xml">
  <link rel="canonical" href="https://brandonvno.github.io/2019/08/15/Tensorflow笔记二/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Tensorflow笔记二 | Brandon电台</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ac76ceae447ceffbc8058af5886fbe8d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Brandon电台</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">好记性不如烂笔头</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
      </li>
    
  </ul>

    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>


    </div>
</nav>
</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://brandonvno.github.io/2019/08/15/Tensorflow笔记二/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Brandon">
      <meta itemprop="description" content="编程、经验、感悟、读后感的一点记录和分享">
      <meta itemprop="image" content="/uploads/Avata1.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Brandon电台">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">Tensorflow笔记二

              
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-15 11:01:03" itemprop="dateCreated datePublished" datetime="2019-08-15T11:01:03+08:00">2019-08-15</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-27 09:56:03" itemprop="dateModified" datetime="2019-08-27T09:56:03+08:00">2019-08-27</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/机器学习/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon"
              >
                <i class="fa fa-eye"></i>
                 阅读次数： 
                <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
              </span>
            </span>
          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p> mooc 北京大学曹健老师课程：tensorflow笔记 第四节 神经网络优化 要点记录<br> 版本：python(3.6.6)， tensorflow(1.3.0)</p>
<hr>
<h2 id="模型、激活函数、NN复杂度"><a href="#模型、激活函数、NN复杂度" class="headerlink" title="模型、激活函数、NN复杂度"></a>模型、激活函数、NN复杂度</h2><p> 常用模型<br><img src="/uploads/tensorflow_notes/image3.png" alt="&quot;模型说明&quot;" title="模型说明"></p>
<p> 三个常用的激活函数<br><img src="/uploads/tensorflow_notes/image4.png" alt="&quot;激活函数&quot;" title="激活函数"></p>
<p> NN复杂度多用NN层数和NN参数的个数表示<br> 层数 = 隐藏层的个数+1个输出层（下图层数为2）<br> 总参数 = 总W+总b（下图总参数为：3 * 4 + 4 + 4 *2 + 2 = 26）<br><img src="/uploads/tensorflow_notes/image5.png" alt="&quot;NN复杂度辅图&quot;" title="NN复杂度辅图"></p>
<hr>
<h2 id="损失函数-loss-：预测值-y-与已知答案-y-的差距"><a href="#损失函数-loss-：预测值-y-与已知答案-y-的差距" class="headerlink" title="损失函数(loss)：预测值(y)与已知答案(y_)的差距"></a>损失函数(loss)：预测值(y)与已知答案(y_)的差距</h2><p> NN优化目标：loss最小。</p>
<p> 三种loss计算：均方误差mse(mean squared error)、 自定义、 交叉熵ce(Cross Entropy)</p>
<h3 id="均方误差MSE：MSE-y-y-frac-sum-i-1-n-y-y-2-n"><a href="#均方误差MSE：MSE-y-y-frac-sum-i-1-n-y-y-2-n" class="headerlink" title="均方误差MSE：MSE(y_, y) = \(\frac{\sum_{i=1}^{n}(y-y\_)^{2}}{n}\)"></a>均方误差MSE：MSE(y_, y) = \(\frac{\sum_{i=1}^{n}(y-y\_)^{2}}{n}\)</h3><p> 均方误差计算损失函数代码：<br> <code>loss = tf.reduce_mean(tf.square(y-y_))</code></p>
<p> 下面举了一个栗子来说明损失函数：<br> 预测酸奶日销量y。x1、x2是影响日销量的因素。<br> 建模前，应预先采集的数据有：每日x1、x2和销量y_（即已知答案，最佳情况：产量=销量）<br> 拟造数据集X，Y_：y_= x1 + x2  噪声：-0.05 ~ +0.05  拟合可以预测销量的函数。<br> 示例代码<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#假设预测多了与预测少了结果一样</span><br><span class="line">#导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32, 2)</span><br><span class="line">Y_ = [[x1+x2+(rdm.rand()/10-0.05)] for (x1, x2) in X]</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入，参数和输出，定义前向传播过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape = (None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (None, 1))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line">#定义损失函数及反向传播方法。</span><br><span class="line">#定义损失函数为MSE 反向传播方法为梯度下降</span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line">#生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 20000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%32</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i%500 == 0:</span><br><span class="line">            print(&quot;After %d training steps, w1 is:&quot;%(i))</span><br><span class="line">            print(sess.run(w1), &quot;\n&quot;)</span><br><span class="line">    print(&quot;final w1 is:\n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure></p>
<p>运行以上代码最终结果为：[[0.98019385], [1.0159807 ]]</p>
<h3 id="自定义损失函数："><a href="#自定义损失函数：" class="headerlink" title="自定义损失函数："></a>自定义损失函数：</h3><p>接着上一个例子说，如果预测商品销量的时候，预测多了，损失成本，预测少了损失利润<br>如果利润≠成本，则mse产生的loss无法将利益最大化。这时候我们可以使用自定义损失函数</p>
<p>自定义损失函数  loss(y_, y) = \(\sum_{n}f(y,y\_)\)<br>  <img src="/uploads/tensorflow_notes/image6.png" alt></p>
<p>自定义损失函数示例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#酸奶成本1元，利润9元</span><br><span class="line">#预测少了损失大，所以应该避免预测少。</span><br><span class="line">#导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line">COST = 1</span><br><span class="line">PROFIT = 9</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32, 2)</span><br><span class="line">Y_ = [[x1+x2+(rdm.rand()/10-0.05)] for (x1, x2) in X]</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入，参数和输出，定义前向传播过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape = (None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (None, 1))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line">#定义损失函数及反向传播方法。</span><br><span class="line">#定义损失函数为MSE 反向传播方法为梯度下降</span><br><span class="line">loss_mse = tf.reduce_sum(tf.where(tf.greater(y, y_), (y-y_)*COST, (y_-y)*PROFIT))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line">#生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 20000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%32</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i%500 == 0:</span><br><span class="line">            print(&quot;After %d training steps, w1 is:&quot;%(i))</span><br><span class="line">            print(sess.run(w1), &quot;\n&quot;)</span><br><span class="line">    print(&quot;final w1 is:\n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure>

<p>以上代码只添加了成本和利润两个参数，修改了损失函数。<br>代码运行最终结果为：[[1.020171 ], [1.0425103]] ，可见w1的两个参数都比原来大了，神经网络在尽量多的预测。</p>
<p>将成本和利润的值互换之后，运行结果为：[[0.9661967 ], [0.97694933]]， 可见神经网络在尽量少的预测。</p>
<h3 id="交叉熵ce-Cross-Entropy-：表征两个概率分布之间的距离"><a href="#交叉熵ce-Cross-Entropy-：表征两个概率分布之间的距离" class="headerlink" title="交叉熵ce(Cross Entropy)：表征两个概率分布之间的距离"></a>交叉熵ce(Cross Entropy)：表征两个概率分布之间的距离</h3><p> H(y_, y) = \(-\sum\) y_*log y</p>
<p> 举例说明：二分类 已知答案y_= (1, 0), 预测y1 = (0.6, 0.4)  y2 = (0.8, 0.2) 哪个更接近标准答案？<br> H1((1,0), (0.6, 0.4)) = -(1 * log0.6 + 0 * log0.4) ≈ -(-0.222+0) = 0.222<br> H2((1,0), (0.8, 0.2)) = -(1 * log0.8 + 0 * log0.2) ≈ -(-0.097+0) = 0.097<br> 所以y2预测更准<br> <code>cem = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y, 1e-12, 1.0)))</code></p>
<p> n分类的n个输出（y1,y2,…yn）通过使用softmax()函数，便满足了概率分布要求：\(\forall P(X=x)\in [0,1] 且\sum_{x}P(X=x)=1\)<br> softmax(yi) = \(\frac{e^{y_{i}}}{\sum_{j=1}^{n}e^{y_{j}}}\)</p>
<p> tensorflow中代码实现使输出经过softmax函数处理后得到满足概率分布要求的结果，再与标准答案求交叉熵：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.spare_softmax_cross_entropy_with_logits(logits=y, labels = tf.argmax(y_, 1))</span><br><span class="line">cem = tf.reduce_mean(ce)</span><br></pre></td></tr></table></figure></p>
<p> spare_softmax_cross_entropy_with_logits()的一些说明：<br> logits参数是经softmax()处理前的数据，即神经网络训练得到的数据y。<br> argmax(y_, 1)是获得y_的第2个维度的张量中最大值的索引<br> labels参数是y对应的原始数据x的标签的整数列表，类似[2,3,6,4]，个人瞎邒推测spare…函数将整数列表转换成了one-hot即类似[0,0,1,0]这样的张量(其实也就是y_的one-hot)。<br> spare_softmax_cross_entropy_with_logits()首先用softmax()处理logits，然后计算labels和处理后的logits的交叉熵。</p>
<hr>
<h2 id="学习率-learning-rate-：每次参数更新的幅度"><a href="#学习率-learning-rate-：每次参数更新的幅度" class="headerlink" title="学习率(learning_rate)：每次参数更新的幅度"></a>学习率(learning_rate)：每次参数更新的幅度</h2><p><img src="/uploads/tensorflow_notes/image7.png" alt></p>
<ul>
<li><p>学习率对传播过程影响示例代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> #conding:utf-8</span><br><span class="line">#设定损失函数 loss = (w+1)^2  ,令w初值为5，反向传播就是求最优w，即求最小loss对应的w值</span><br><span class="line">import tensorflow as tf</span><br><span class="line">#定义带优化参数w，初始值赋为5</span><br><span class="line">w = tf.Variable(tf.constant(5, dtype = tf.float32))</span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss =  tf.square(w+1)</span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#生成会话，训练40轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    for i in range(40):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        print(&quot;After %s steps: w is %f, loss is %f.&quot;%(i, w_val, loss_val))</span><br></pre></td></tr></table></figure>

<p>更改学习率0.2为1，观察学习情况；更改学习率为0.001，再观察情况。可以直观的看到学习率的影响。</p>
</li>
<li><p>学习率设置多少合适？<br>由上例代码可知，学习率大了（比如设置为1）可能会导致震荡不收敛，学习率小了（比如设置0.001）收敛速度慢，所以可以考虑动态学习率。</p>
</li>
<li><p>指数衰减学习率<br>learning_rate = LEARNING_RATE_BASE * LEARNING_RATE_DECAY ^ (global_step/LEARNING_RATE_STEP)</p>
<p>LEARNING_RATE_BASE 指学习率初始值<br>LEARNING_RATE_DECAY 指学习率衰减率(一般取0-1，开区间)<br>global_step 指运行的总轮数<br>LEARNING_RATE_STEP 指多少轮更新一次学习率，计算方式为：总样本数/BATCH_SIZE</p>
<p>函数代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0, trainable = false)  #记录当前运行轮数的计数器，trainable为False即标注为此参数不可训练</span><br><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    LEARNING_RATE_STEP,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase = True</span><br><span class="line">    )</span><br><span class="line">    #staircase 取True时, global_step/LEARNING_RATE_STEP取整数，学习率阶梯型衰减，取False时，学习率下降沿一条平滑曲线</span><br></pre></td></tr></table></figure>

<p>指数衰减学习率代码示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"> #coding:utf-8</span><br><span class="line">#设损失函数loss = (w+1)^2, 令w初始值是常数10,，反向传播就是求最优w，即求最小loss对应的w值</span><br><span class="line">#使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度。</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = 0.1  #最初学习率</span><br><span class="line">LEARNING_RATE_DECAY = 0.99 #学习率衰减率</span><br><span class="line">LEARNING_RATE_STEP = 1 #喂入多少轮BATCH_SIZE后，更新一次学习率， 一般设为：总样本数/BATCH_SIZE</span><br><span class="line"></span><br><span class="line">#运行了几轮BATCH_SIZE的计数器，初始值为0, 设为不被训练</span><br><span class="line">global_step = tf.Variable(0, trainable= False)</span><br><span class="line"></span><br><span class="line">#定义指数下降学习率</span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP,</span><br><span class="line"> LEARNING_RATE_DECAY, staircase= True)</span><br><span class="line"></span><br><span class="line">#定义待优化参数w，初始值为0</span><br><span class="line">w = tf.Variable(10, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss = tf.square(w+1)</span><br><span class="line"></span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,</span><br><span class="line"> global_step=global_step)</span><br><span class="line"></span><br><span class="line">#生成会话，训练40轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    for i in range(40):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        learning_rate_val = sess.run(learning_rate)</span><br><span class="line">        global_step_val = sess.run(global_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        print(&quot;After %s steps: global_step is %f, w is %f, learning_rate is %f, loss is %f&quot; </span><br><span class="line">        %(i, global_step_val, w_val, learning_rate_val, loss_val))</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h2 id="滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。"><a href="#滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。" class="headerlink" title="滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。"></a>滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。</h2><p>针对所有参数进行优化，包括所有的w和b（像是给参数加了影子，参数变化，影子缓慢跟随）<br>计算方法： 影子 = 衰减率 * 影子 + (1- 衰减率) * 参数<br>影子初值 = 参数初值<br>衰减率 = min{MOVING_AVERAGE_DECAY, (1+轮数)/(10+轮数)}<br>MOVING_AVERAGE_DECAY是滑动平均衰减率，是一个超参数</p>
<p>滑动平均计算过程举例：<br><img src="/uploads/tensorflow_notes/image8.png" alt></p>
<p>滑动平均计算常用代码：<br>定义滑动平均参数：<br><code>ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</code></p>
<p>求所有待优化的参数的滑动平均值<br><code>ema_op = ema.apply(tf.trainable_variables())</code><br>ema.apply() 可以求指定参数的滑动平均值<br>tf.trainable_variables()可以将所有待优化的参数汇总成一个列表</p>
<p>常用以下代码将训练过程和计算滑动平均值绑定成一个训练节点：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">    train_op = tf.no_op(name = &apos;train&apos;)</span><br></pre></td></tr></table></figure></p>
<p> control_dependencies作用是当你运行train_op时会先运行train_step和ema_op，即设置train_op的依赖。<br> 对tf.no_op的解释引用stackflow上的一句话：</p>
<blockquote>
<p>As the documentation says, tf.no_op() does nothing. However, when you create a tf.no_op() inside a with tf.control_dependencies([x, y, z]): block, the op will gain control dependencies on ops x, y, and z. Therefore it can be used to group together a set of side effecting ops, and give you a single op to pass to sess.run() in order to run all of them in a single step.</p>
</blockquote>
<p>查看某参数的滑动平均值：<br><code>ema.average(参数名)</code></p>
<p>示例代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#1. 定义变量及滑动平均类</span><br><span class="line">w1 = tf.Variable(0, dtype = tf.float32)</span><br><span class="line">#定义一个32位浮点变量， 初始值喂0.0, 这个代码就是不断更新w1参数，优化w1参数，滑动平均做了w1的影子</span><br><span class="line">w1 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line">#定义num_updates (NN的迭代轮数), 初始值为0, 不可被优化（训练）</span><br><span class="line">global_step = tf.Variable(0, trainable = False)</span><br><span class="line">#实例化滑动平均类，给衰减率为0.99, 当前轮数global_step</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">#ema.apply()括号里的内容是更新列表，每次运行sess.run(ema_op)时， 对更新列表中的元素求滑动平均值</span><br><span class="line">#在实际应用中会使用tf.trainable_varibales()自动将所有待训练的参数汇总喂列表</span><br><span class="line">#ema_op = ema.apply([w1])</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line">#2. 查看不同迭代中变量取值的变化。</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #初始化</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    #用ema.average(w1)获取w1滑动平均值（要运行多个节点，作为列表中的元素列出，写在sess.run中）</span><br><span class="line">    #打印出当前参数w1和w1的滑动平均值</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    #参数w1的值赋为1</span><br><span class="line">    sess.run(tf.assign(w1, 1))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    #更新step和w1的值，模拟出100轮迭代后，参数w1变为10</span><br><span class="line">    sess.run(tf.assign(global_step, 100))</span><br><span class="line">    sess.run(tf.assign(w1, 10))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line"></span><br><span class="line">    #每次sess.run会更新一次w1的滑动平均值</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="正则化缓解过拟合"><a href="#正则化缓解过拟合" class="headerlink" title="正则化缓解过拟合"></a>正则化缓解过拟合</h2><p>当模型在训练数据集上的正确率非常高，而很难对新数据集做出正确的相应时，可能是出现了过拟合现象。使用正则化可以有效缓解过拟合。<br>正则化在损失函数中引入模型复杂度指标，利用给w加权值，弱化了训练数据的噪声(一般不正则化b)</p>
<p>正则化公式：<br>loss = loss(y与y_)① + REGULARIZER② * loss(w)③<br>①指模型中所有参数的损失函数，如：交叉熵、均方误差等。<br>②指用超参数REGULARIZER给出参数w在总loss中的比例，即正则化的权重<br>③是需要正则化的参数<br>loss(w)有两种求法：</p>
<p>L1正则化：<br>\(loss_{L1}(w) = \sum_{i}|w_{i}|\)<br><code>loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER(w))</code></p>
<p>L2正则化：<br>\((loss_{L2}(w) = \sum_{i}|w_{i}^{2}|\)<br><code>loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER(w))</code>       </p>
<p>将得到的loss(w)加到losses集合中：<br><code>tf.add_to_collection(&#39;losses&#39;, tf.contrib.layers.l2_regularizer(regularizer)(w))</code><br>获得最终损失函数：<br><code>loss = cem + tf.add_n(tf.get_collection(&#39;losses&#39;))</code></p>
<p>正则化示例问题描述：<br>画一条线将红色点和蓝色点隔离开来<br><img src="/uploads/tensorflow_notes/image9.png" alt="&quot;&quot;"></p>
<p>正则化示例中用到的模块matplotlib介绍：<br><code>import matplotlib as plt</code></p>
<p><code>plt.scatter(x坐标，y坐标，c=“颜色”)</code><br><code>plt.show()</code><br>上面两个语句用来设置散点，并显示出来：</p>
<p>下面的语句用来初始化网格，喂入神经网络并得到结果：</p>
<p><code>xx, yy = np.mgrid[起:止:步长, 起:止:步长]</code><br>隔步长取起止位置之间的所有坐标。</p>
<p><code>grid = np.c_[xx.ravel(), yy.ravel()]</code><br>先用ravel()将所有横纵坐标拉直，即将所有坐标组成一维张量，然后用np.c_()将两个一维张量纵向(列方向，c为column缩写)组成矩阵，即n行2列的矩阵，即n个坐标。</p>
<p><code>probs = sess.run(y, feed_dict = {x:gird})</code><br>将grid喂入神经网络，计算得各个点的标志(0或1)，并存到probs中</p>
<p><code>probs = probs.reshape(xx.shape)</code><br>将probs设置为xx的shape，即n行1列的2维张量</p>
<p><code>plt.contour(x轴坐标值，y轴坐标值， 该点的高度， levels = [等高线的高度])</code><br><code>plt.show()</code><br>使用上面语句把所有坐标点都设置好高度（0或者1），这样将level设置成两个高度中间值（0.5），就可以将线画出来</p>
<p>正则化示例代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">BATCH_SIZE = 30</span><br><span class="line">seed = 2</span><br><span class="line"></span><br><span class="line">#基于seed产生随机数</span><br><span class="line">rdm = np.random.RandomState(seed)</span><br><span class="line">#随机数返回300行2列的矩阵，表示300组坐标点（x0, x1）， 作为输入数据集</span><br><span class="line">X = rdm.randn(300, 2)</span><br><span class="line">#从X这个300行2列的矩阵中取出1行，判断如果两个坐标的平方和小于2,给Y赋值1, 其余赋值0，作为输入数据集的标签</span><br><span class="line">Y_ = [int(x0*x0 + x1*x1 &lt;2) for (x0, x1) in X]</span><br><span class="line">#遍历Y中的每个元素，1赋值&apos;red&apos;， 其余赋值&apos;blue&apos;， 这样可视化显示时人可以直观区分</span><br><span class="line">Y_c = [[&apos;red&apos; if y else &apos;blue&apos;] for y in Y_]</span><br><span class="line">#对数据集X和标签Y进行shape整理，第一个元素为-1，表示n行，随第二个参数计算得到， 第二个元素表示多少列，把X整理为n行2列，把Y整理为n行1列</span><br><span class="line">X = np.vstack(X).reshape(-1, 2)</span><br><span class="line">Y_ = np.vstack(Y_).reshape(-1, 1)</span><br><span class="line">print(X)</span><br><span class="line">print(Y_)</span><br><span class="line">print(Y_c)</span><br><span class="line"></span><br><span class="line">#用plt.scatter画出数据集X各行中第0列元素和第一列元素的点，即各行的(x0, x1)，用各行Y_c对应的值表示颜色(c是color的缩写)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=np.squeeze(Y_c))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入，参数和输出，定义前向传播过程</span><br><span class="line"># 生成权重，输入：w的shape和正则化权重</span><br><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line">def get_bias(shape):</span><br><span class="line">    b = tf.Variable(tf.constant(0.01, shape = shape))</span><br><span class="line">    return b</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line"></span><br><span class="line">w1 = get_weight([2,11], 0.01)</span><br><span class="line">b1 = get_bias([11])</span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">w2 = get_weight([11,1], 0.01)</span><br><span class="line">b2 = get_bias([1])</span><br><span class="line">y = tf.matmul(y1,w2) + b2</span><br><span class="line"></span><br><span class="line">#定义损失函数</span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">loss_total = loss_mse + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line"></span><br><span class="line">#定义反向传播方法：不含正则化</span><br><span class="line">train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 40000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%300</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i%2000 == 0:</span><br><span class="line">            loss_mse_v = sess.run(loss_mse, feed_dict=&#123;x:X, y_:Y_&#125;)</span><br><span class="line">            print(&quot;After %d steps, loss is %f&quot;%(i, loss_mse_v))</span><br><span class="line">    #xx在-3到3之间以步长为0.01, yy在-3到3之间以步长0.01,生成二维网格坐标点</span><br><span class="line">    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]</span><br><span class="line">    #将xx，yy拉直，并合并成一个2列的矩阵，得到一个网格坐标点的集合</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    #将网格坐标点喂入神经网络，probs为输出</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    print(probs.size)</span><br><span class="line">    #将probs的shape调整成xx的样子</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;b1:\n&quot;, sess.run(b1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br><span class="line">    print(&quot;b2:\n&quot;, sess.run(b2))</span><br><span class="line">    </span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx, yy, probs, levels=[.5])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义反向传播方法：含正则化</span><br><span class="line">train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_total)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 40000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%300</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i% 2000 == 0:</span><br><span class="line">            loss_v = sess.run(loss_total, feed_dict=&#123;x:X, y_:Y_&#125;)</span><br><span class="line">            print(&quot;After %d steps, loss is: %f&quot;%(i, loss_v))</span><br><span class="line">    </span><br><span class="line">    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;b1:\n&quot;, sess.run(b1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br><span class="line">    print(&quot;b2:\n&quot;, sess.run(b2))</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx, yy, probs, levels = [.5])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>运行即可看到不使用正则化的结果和使用正则化的结果：<br>未使用正则化：<br><img src="/uploads/tensorflow_notes/image10.png" alt><br>使用正则化：<br><img src="/uploads/tensorflow_notes/image11.png" alt></p>
<p>可以明显看到，使用正则化能够明显减弱噪点的影响。</p>
<hr>
<h2 id="搭建模块化的神经网络八股"><a href="#搭建模块化的神经网络八股" class="headerlink" title="搭建模块化的神经网络八股"></a>搭建模块化的神经网络八股</h2><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>前向传播的过程就是搭建网络，设计网络结构的过程。通常用forward.py文件定义前向传播过程。在文件中通常定义三个函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def forward(x, regularizer):</span><br><span class="line"> w = </span><br><span class="line"> b = </span><br><span class="line"> y = </span><br><span class="line"> return y</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def get_weight(shape, regularizer):</span><br><span class="line"> w = tf.Variable()</span><br><span class="line"> tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line"> return w</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def get_bias(shape):</span><br><span class="line"> b = tf.Variable()</span><br><span class="line"> return b</span><br></pre></td></tr></table></figure>

<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播的过程就是训练网络，优化网络参数的过程。通常用backward.py文件定义反向传播过程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">def backward():</span><br><span class="line">    x = tf.placeholder(  )</span><br><span class="line">    y_ = tf.placeholder(  )</span><br><span class="line">    y = forward.forward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(0, trainable = false)</span><br><span class="line">    loss = </span><br><span class="line"></span><br><span class="line">    #loss()函数有三种选择：</span><br><span class="line">    #均方误差: </span><br><span class="line">    loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">    #自定义：暂无代码</span><br><span class="line">    #交叉熵：</span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_, 1))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    </span><br><span class="line">    #加入正则化</span><br><span class="line">    loss = 三种损失函数之一 + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line"></span><br><span class="line">    #使用指数衰减学习率：</span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    数据集总样本数/BATCH_SIZE,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase=True</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    #定义反向传播训练过程：</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)</span><br><span class="line"></span><br><span class="line">    #计算滑动平均：</span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name = &apos;train&apos;)</span><br><span class="line"></span><br><span class="line">    #with结构初始化所有参数并开始训练</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        for i in range(STEPS):</span><br><span class="line">            sess.run(train_step, feed_dict=&#123;x:, y_:&#125;)</span><br><span class="line">            if i%轮数 == 0:</span><br><span class="line">                print</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;</span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure>

<h3 id="用模块化思想实现正则化示例的代码："><a href="#用模块化思想实现正则化示例的代码：" class="headerlink" title="用模块化思想实现正则化示例的代码："></a>用模块化思想实现正则化示例的代码：</h3><p>generateds.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">seed = 2</span><br><span class="line">def generateds():</span><br><span class="line">    #基于seed产生随机数</span><br><span class="line">    rdm = np.random.RandomState(seed)</span><br><span class="line">    #随机数(标准正态分布)返回300行2列的矩阵，表示300组坐标点(x0, x1)作为输入数据集</span><br><span class="line">    X = rdm.randn(300, 2)</span><br><span class="line">    #从X这个300行2列的矩阵中取出一行，判断如该这两个坐标的平方和小于2,给Y_赋值1, 其余赋值0</span><br><span class="line">    #作为输入数据集的标签(正确答案)</span><br><span class="line">    Y_ = [(int)(x0*x0 + x1*x1 &lt; 2) for (x0, x1) in X]</span><br><span class="line">    #遍历Y中的每个元素，1赋值&apos;red&apos;其余赋值&apos;blue&apos;，这样可视化显示时人可以直观区分</span><br><span class="line">    Y_c = [[&apos;red&apos; if y else &apos;blue&apos;] for y in Y_]</span><br><span class="line">    #对数据集X和标签Y进行形状整理，第一个元素喂-1表示跟随第二列计算，第二个元素表示多少列。X为n行2列，Y_为n行1列</span><br><span class="line">    X = np.vstack(X).reshape(-1, 2)</span><br><span class="line">    Y_ = np.vstack(Y_).reshape(-1, 1)</span><br><span class="line"></span><br><span class="line">    plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    return X, Y_, Y_c</span><br></pre></td></tr></table></figure>

<p>forward.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line"># 定义神经网络的输入，参数和输出，定义前向传播过程</span><br><span class="line"></span><br><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line">def get_bias(shape):</span><br><span class="line">    b = tf.Variable(tf.constant(0.01, shape=shape))</span><br><span class="line">    return b</span><br><span class="line"></span><br><span class="line">def forward(x, regularizer):</span><br><span class="line">    w1 = get_weight([2,11], regularizer)</span><br><span class="line">    b1 = get_bias([11])</span><br><span class="line">    #计算层需要经过激活函数处理</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight([11,1], regularizer)</span><br><span class="line">    b2 = get_bias([1])</span><br><span class="line">    #结果层不需要经过激活函数处理</span><br><span class="line">    y = tf.matmul(y1, w2) + b2</span><br><span class="line">    return y</span><br></pre></td></tr></table></figure>

<p>backward.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import opt4_8_generateds as ge</span><br><span class="line">import opt4_8_forward as fw</span><br><span class="line"></span><br><span class="line">STEPS = 40000</span><br><span class="line">BATCH_SIZE = 30</span><br><span class="line">LEARNING_RATE_BASE = 0.001</span><br><span class="line">LEARNING_RATE_DACY = 0.999</span><br><span class="line">REGULARIZER = 0.01</span><br><span class="line"></span><br><span class="line">def backward():</span><br><span class="line">    x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">    y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line">    </span><br><span class="line">    X, Y_, Y_c = ge.generateds()</span><br><span class="line"></span><br><span class="line">    y = fw.forward(x, REGULARIZER)</span><br><span class="line"></span><br><span class="line">    global_step = tf.Variable(0, trainable=False)</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE, </span><br><span class="line">        global_step,</span><br><span class="line">        300/BATCH_SIZE,</span><br><span class="line">        LEARNING_RATE_DACY,</span><br><span class="line">        staircase= True</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    #定义损失函数</span><br><span class="line">    loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">    loss_total = tf.add_n(tf.get_collection(&apos;losses&apos;)) + loss_mse</span><br><span class="line">    </span><br><span class="line">    #定义包含正则化的反向传播方法</span><br><span class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_total)</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        for i in range(STEPS):</span><br><span class="line">            start = (i*BATCH_SIZE)%300</span><br><span class="line">            end = start + BATCH_SIZE</span><br><span class="line">            sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">            if i % 2000 == 0:</span><br><span class="line">                loss_v = sess.run(loss_total, feed_dict=&#123;x:X, y_:Y_&#125;)</span><br><span class="line">                print(&quot;After %d steps, loss is %f&quot;%(i, loss_v))</span><br><span class="line">        </span><br><span class="line">        xx, yy = np.mgrid[-3:3:0.01, -3:3:0.01]</span><br><span class="line">        grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">        probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">        probs = probs.reshape(xx.shape)</span><br><span class="line">    plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))</span><br><span class="line">    plt.contour(xx, yy, probs, levels = [.5])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure>

<hr>

    </div>

    
    
    
    
      <div>
        <div id="reward-container">
  <div>分享是一种快乐，请随意</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.png" alt="Brandon 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

      </div>

    <footer class="post-footer">
          
        
        <div class="post-tags">
            <a href="/tags/人工智能/" rel="tag"># 人工智能</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/Tensorflow/" rel="tag"># Tensorflow</a>
          
        </div>
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
              <a href="/2019/08/14/在markdown中使用数学公式/" rel="next" title="在markdown中使用数学公式">
                <i class="fa fa-chevron-left"></i> 在markdown中使用数学公式
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
              <a href="/2019/08/20/Tensorflow笔记三/" rel="prev" title="Tensorflow笔记三">
                Tensorflow笔记三 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
    </footer>
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="gitalk-container"></div>
  


        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/uploads/Avata1.png"
      alt="Brandon">
  <p class="site-author-name" itemprop="name">Brandon</p>
  <div class="site-description motion-element" itemprop="description">编程、经验、感悟、读后感的一点记录和分享</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/brandonvno" title="GitHub &rarr; https://github.com/brandonvno" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element links-of-blogroll-block">
    <div class="links-of-blogroll-title">
      <i class="fa  fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.taptap.com/app/63844" title="https://www.taptap.com/app/63844" rel="noopener" target="_blank">行界：重构</a>
        </li>
      
    </ul>
  </div>

        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#概述"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型、激活函数、NN复杂度"><span class="nav-number">2.</span> <span class="nav-text">模型、激活函数、NN复杂度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数-loss-：预测值-y-与已知答案-y-的差距"><span class="nav-number">3.</span> <span class="nav-text">损失函数(loss)：预测值(y)与已知答案(y_)的差距</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#均方误差MSE：MSE-y-y-frac-sum-i-1-n-y-y-2-n"><span class="nav-number">3.1.</span> <span class="nav-text">均方误差MSE：MSE(y_, y) = \(\frac{\sum_{i=1}^{n}(y-y\_)^{2}}{n}\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自定义损失函数："><span class="nav-number">3.2.</span> <span class="nav-text">自定义损失函数：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#交叉熵ce-Cross-Entropy-：表征两个概率分布之间的距离"><span class="nav-number">3.3.</span> <span class="nav-text">交叉熵ce(Cross Entropy)：表征两个概率分布之间的距离</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习率-learning-rate-：每次参数更新的幅度"><span class="nav-number">4.</span> <span class="nav-text">学习率(learning_rate)：每次参数更新的幅度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。"><span class="nav-number">5.</span> <span class="nav-text">滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化缓解过拟合"><span class="nav-number">6.</span> <span class="nav-text">正则化缓解过拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#搭建模块化的神经网络八股"><span class="nav-number">7.</span> <span class="nav-text">搭建模块化的神经网络八股</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播"><span class="nav-number">7.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">7.2.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用模块化思想实现正则化示例的代码："><span class="nav-number">7.3.</span> <span class="nav-text">用模块化思想实现正则化示例的代码：</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Brandon</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>


  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>



<script src="/js/next-boot.js?v=7.3.0"></script>






  















  <script src="/js/local-search.js?v=7.3.0"></script>














  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>


    
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">
<script src="//cdn.jsdelivr.net/npm/js-md5@0.7.3/src/md5.min.js"></script>

<script>
  var gitalk = new Gitalk({
    clientID: '37854a8b3b718ee36cda',
    clientSecret: '2c841db056b110a0454e394cd9993cb4026e1c13',
    repo: 'brandonvno.github.io',
    owner: 'brandonvno',
    admin: ['brandonvno'],
    id: md5(location.pathname),
      language: 'zh-CN',
    
    distractionFreeMode: 'true'
  });
  gitalk.render('gitalk-container');
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618},"display":{"superSample":2,"width":200,"height":400,"position":"right","hOffset":0,"vOffset":-20},"react":{"opacityDefault":0.5,"opacityOnHover":0.5,"opacity":0.7},"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
