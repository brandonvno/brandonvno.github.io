<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Tensorflow笔记二</title>
      <link href="/2019/08/15/Tensorflow%E7%AC%94%E8%AE%B0%E4%BA%8C/"/>
      <url>/2019/08/15/Tensorflow%E7%AC%94%E8%AE%B0%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p> mooc 北京大学曹健老师课程：tensorflow笔记 第四节 神经网络优化 要点记录</p><hr><h2 id="模型、激活函数、NN复杂度"><a href="#模型、激活函数、NN复杂度" class="headerlink" title="模型、激活函数、NN复杂度"></a>模型、激活函数、NN复杂度</h2><p> 常用模型<br><img src="/uploads/tensorflow_notes/image3.png" alt="&quot;模型说明&quot;" title="模型说明"></p><p> 三个常用的激活函数<br><img src="/uploads/tensorflow_notes/image4.png" alt="&quot;激活函数&quot;" title="激活函数"></p><p> NN复杂度多用NN层数和NN参数的个数表示<br> 层数 = 隐藏层的个数+1个输出层（下图层数为2）<br> 总参数 = 总W+总b（下图总参数为：3<em>4+4 + 4</em>2+2 = 26）<br><img src="/uploads/tensorflow_notes/image5.png" alt="&quot;NN复杂度辅图&quot;" title="NN复杂度辅图"></p><hr><h2 id="损失函数-loss-：预测值-y-与已知答案-y-的差距"><a href="#损失函数-loss-：预测值-y-与已知答案-y-的差距" class="headerlink" title="损失函数(loss)：预测值(y)与已知答案(y_)的差距"></a>损失函数(loss)：预测值(y)与已知答案(y_)的差距</h2><p> NN优化目标：loss最小。</p><p> 三种loss计算：均方误差mse(mean squared error)、 自定义、 交叉熵ce(Cross Entropy)</p><h3 id="均方误差MSE：MSE-y-y-frac-sum-i-1-n-y-y-2-n"><a href="#均方误差MSE：MSE-y-y-frac-sum-i-1-n-y-y-2-n" class="headerlink" title="均方误差MSE：MSE(y_, y) = \(\frac{\sum_{i=1}^{n}(y-y\_)^{2}}{n}\)"></a>均方误差MSE：MSE(y_, y) = \(\frac{\sum_{i=1}^{n}(y-y\_)^{2}}{n}\)</h3><p> 均方误差计算损失函数代码：<br> <code>loss = tf.reduce_mean(tf.square(y-y_))</code></p><p> 下面举了一个栗子来说明损失函数：<br> 预测酸奶日销量y。x1、x2是影响日销量的因素。<br> 建模前，应预先采集的数据有：每日x1、x2和销量y_（即已知答案，最佳情况：产量=销量）<br> 拟造数据集X，Y_：y_= x1 + x2  噪声：-0.05 ~ +0.05  拟合可以预测销量的函数。<br> 示例代码<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#假设预测多了与预测少了结果一样</span><br><span class="line">#导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32, 2)</span><br><span class="line">Y_ = [[x1+x2+(rdm.rand()/10-0.05)] for (x1, x2) in X]</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入，参数和输出，定义前向传播过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape = (None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (None, 1))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line">#定义损失函数及反向传播方法。</span><br><span class="line">#定义损失函数为MSE 反向传播方法为梯度下降</span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line">#生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 20000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%32</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i%500 == 0:</span><br><span class="line">            print(&quot;After %d training steps, w1 is:&quot;%(i))</span><br><span class="line">            print(sess.run(w1), &quot;\n&quot;)</span><br><span class="line">    print(&quot;final w1 is:\n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure></p><p>运行以上代码最终结果为：[[0.98019385], [1.0159807 ]]</p><h3 id="自定义损失函数："><a href="#自定义损失函数：" class="headerlink" title="自定义损失函数："></a>自定义损失函数：</h3><p>接着上一个例子说，如果预测商品销量的时候，预测多了，损失成本，预测少了损失利润<br>如果利润≠成本，则mse产生的loss无法将利益最大化。这时候我们可以使用自定义损失函数</p><p>自定义损失函数  loss(y_, y) = \(\sum_{n}f(y,y\_)\)<br>  <img src="/uploads/tensorflow_notes/image6.png" alt></p><p>自定义损失函数示例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#酸奶成本1元，利润9元</span><br><span class="line">#预测少了损失大，所以应该避免预测少。</span><br><span class="line">#导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line">COST = 1</span><br><span class="line">PROFIT = 9</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32, 2)</span><br><span class="line">Y_ = [[x1+x2+(rdm.rand()/10-0.05)] for (x1, x2) in X]</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入，参数和输出，定义前向传播过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape = (None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (None, 1))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line">#定义损失函数及反向传播方法。</span><br><span class="line">#定义损失函数为MSE 反向传播方法为梯度下降</span><br><span class="line">loss_mse = tf.reduce_sum(tf.where(tf.greater(y, y_), (y-y_)*COST, (y_-y)*PROFIT))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line">#生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 20000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%32</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i%500 == 0:</span><br><span class="line">            print(&quot;After %d training steps, w1 is:&quot;%(i))</span><br><span class="line">            print(sess.run(w1), &quot;\n&quot;)</span><br><span class="line">    print(&quot;final w1 is:\n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure><p>以上代码只添加了成本和利润两个参数，修改了损失函数。<br>代码运行最终结果为：[[1.020171 ], [1.0425103]] ，可见w1的两个参数都比原来大了，神经网络在尽量多的预测。</p><p>将成本和利润的值互换之后，运行结果为：[[0.9661967 ], [0.97694933]]， 可见神经网络在尽量少的预测。</p><h3 id="交叉熵ce-Cross-Entropy-：表征两个概率分布之间的距离"><a href="#交叉熵ce-Cross-Entropy-：表征两个概率分布之间的距离" class="headerlink" title="交叉熵ce(Cross Entropy)：表征两个概率分布之间的距离"></a>交叉熵ce(Cross Entropy)：表征两个概率分布之间的距离</h3><p> H(y_, y) = \(-\sum\) y_*log y</p><p> 举例说明：二分类 已知答案y_= (1, 0), 预测y1 = (0.6, 0.4)  y2 = (0.8, 0.2) 哪个更接近标准答案？<br> H1((1,0), (0.6, 0.4)) = -(1<em>log0.6 + 0</em>log0.4) ≈ -(-0.222+0) = 0.222<br> H2((1,0), (0.8, 0.2)) = -(1<em>log0.8 + 0</em>log0.2) ≈ -(-0.097+0) = 0.097<br> 所以y2预测更准<br> <code>cem = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y, 1e-12, 1.0)))</code></p><p> n分类的n个输出（y1,y2,…yn）通过使用softmax()函数，便满足了概率分布要求：\(\forall P(X=x)\in [0,1] 且\sum_{x}P(X=x)=1\)<br> softmax(yi) = \(\frac{e^{y_{i}}}{\sum_{j=1}^{n}e^{y_{j}}}\)</p><p> tensorflow中代码实现使输出经过softmax函数处理后得到满足概率分布要求的结果，再与标准答案求交叉熵：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.spare_softmax_cross_entropy_with_logits(logits=y, labels = tf.argmax(y_, 1))</span><br><span class="line">cem = tf.reduce_mean(ce)</span><br></pre></td></tr></table></figure></p><hr><h2 id="学习率-learning-rate-：每次参数更新的幅度"><a href="#学习率-learning-rate-：每次参数更新的幅度" class="headerlink" title="学习率(learning_rate)：每次参数更新的幅度"></a>学习率(learning_rate)：每次参数更新的幅度</h2><p><img src="/uploads/tensorflow_notes/image7.png" alt></p><ul><li><p>学习率对传播过程影响示例代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> #conding:utf-8</span><br><span class="line">#设定损失函数 loss = (w+1)^2  ,令w初值为5，反向传播就是求最优w，即求最小loss对应的w值</span><br><span class="line">import tensorflow as tf</span><br><span class="line">#定义带优化参数w，初始值赋为5</span><br><span class="line">w = tf.Variable(tf.constant(5, dtype = tf.float32))</span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss =  tf.square(w+1)</span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#生成会话，训练40轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    for i in range(40):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        print(&quot;After %s steps: w is %f, loss is %f.&quot;%(i, w_val, loss_val))</span><br></pre></td></tr></table></figure><p>更改学习率0.2为1，观察学习情况；更改学习率为0.001，再观察情况。可以直观的看到学习率的影响。</p></li><li><p>学习率设置多少合适？<br>由上例代码可知，学习率大了（比如设置为1）可能会导致震荡不收敛，学习率小了（比如设置0.001）收敛速度慢，所以可以考虑动态学习率。</p></li><li><p>指数衰减学习率<br>learning_rate = LEARNING_RATE_BASE * LEARNING_RATE_DECAY ^ (global_step/LEARNING_RATE_STEP)</p><p>LEARNING_RATE_BASE 指学习率初始值<br>LEARNING_RATE_DECAY 指学习率衰减率(一般取0-1，开区间)<br>global_step 指运行的总轮数<br>LEARNING_RATE_STEP 指多少轮更新一次学习率，计算方式为：总样本数/BATCH_SIZE</p><p>函数代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0, trainable = false)  #记录当前运行轮数的计数器，trainable为False即标注为此参数不可训练</span><br><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    LEARNING_RATE_STEP,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase = True</span><br><span class="line">    )</span><br><span class="line">    #staircase 取True时, global_step/LEARNING_RATE_STEP取整数，学习率阶梯型衰减，取False时，学习率下降沿一条平滑曲线</span><br></pre></td></tr></table></figure><p>指数衰减学习率代码示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"> #coding:utf-8</span><br><span class="line">#设损失函数loss = (w+1)^2, 令w初始值是常数10,，反向传播就是求最优w，即求最小loss对应的w值</span><br><span class="line">#使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度。</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = 0.1  #最初学习率</span><br><span class="line">LEARNING_RATE_DECAY = 0.99 #学习率衰减率</span><br><span class="line">LEARNING_RATE_STEP = 1 #喂入多少轮BATCH_SIZE后，更新一次学习率， 一般设为：总样本数/BATCH_SIZE</span><br><span class="line"></span><br><span class="line">#运行了几轮BATCH_SIZE的计数器，初始值为0, 设为不被训练</span><br><span class="line">global_step = tf.Variable(0, trainable= False)</span><br><span class="line"></span><br><span class="line">#定义指数下降学习率</span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP,</span><br><span class="line"> LEARNING_RATE_DECAY, staircase= True)</span><br><span class="line"></span><br><span class="line">#定义待优化参数w，初始值为0</span><br><span class="line">w = tf.Variable(10, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss = tf.square(w+1)</span><br><span class="line"></span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,</span><br><span class="line"> global_step=global_step)</span><br><span class="line"></span><br><span class="line">#生成会话，训练40轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    for i in range(40):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        learning_rate_val = sess.run(learning_rate)</span><br><span class="line">        global_step_val = sess.run(global_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        print(&quot;After %s steps: global_step is %f, w is %f, learning_rate is %f, loss is %f&quot; </span><br><span class="line">        %(i, global_step_val, w_val, learning_rate_val, loss_val))</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。"><a href="#滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。" class="headerlink" title="滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。"></a>滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。</h2><p>针对所有参数进行优化，包括所有的w和b（像是给参数加了影子，参数变化，影子缓慢跟随）<br>计算方法： 影子 = 衰减率 * 影子 + (1- 衰减率) * 参数<br>影子初值 = 参数初值<br>衰减率 = min{MOVING_AVERAGE_DECAY, (1+轮数)/(10+轮数)}<br>MOVING_AVERAGE_DECAY是滑动平均衰减率，是一个超参数</p><p>滑动平均计算过程举例：<br><img src="/uploads/tensorflow_notes/image8.png" alt></p><p>滑动平均计算常用代码：<br>定义滑动平均参数：<br><code>ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</code></p><p>求所有待优化的参数的滑动平均值<br><code>ema_op = ema.apply(tf.trainable_variables())</code><br>ema.apply() 可以求指定参数的滑动平均值<br>tf.trainable_variables()可以将所有待优化的参数汇总成一个列表</p><p>常用以下代码将训练过程和计算滑动平均值绑定成一个训练节点：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">    train_op = tf.no_op(name = &apos;train&apos;)</span><br></pre></td></tr></table></figure></p><p>查看某参数的滑动平均值：<br><code>ema.average(参数名)</code></p><p>示例代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#1. 定义变量及滑动平均类</span><br><span class="line">w1 = tf.Variable(0, dtype = tf.float32)</span><br><span class="line">#定义一个32位浮点变量， 初始值喂0.0, 这个代码就是不断更新w1参数，优化w1参数，滑动平均做了w1的影子</span><br><span class="line">w1 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line">#定义num_updates (NN的迭代轮数), 初始值为0, 不可被优化（训练）</span><br><span class="line">global_step = tf.Variable(0, trainable = False)</span><br><span class="line">#实例化滑动平均类，给衰减率为0.99, 当前轮数global_step</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">#ema.apply()括号里的内容是更新列表，每次运行sess.run(ema_op)时， 对更新列表中的元素求滑动平均值</span><br><span class="line">#在实际应用中会使用tf.trainable_varibales()自动将所有待训练的参数汇总喂列表</span><br><span class="line">#ema_op = ema.apply([w1])</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line">#2. 查看不同迭代中变量取值的变化。</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #初始化</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    #用ema.average(w1)获取w1胡熬到嗯平均值（要运行多个节点，作为列表中的元素列出，写在sess.run中）</span><br><span class="line">    #打印出当前参数w1和w1的滑动平均值</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    #参数w1的值赋为1</span><br><span class="line">    sess.run(tf.assign(w1, 1))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    #更新step和w1的值，模拟出100轮迭代后，参数w1变为10</span><br><span class="line">    sess.run(tf.assign(global_step, 100))</span><br><span class="line">    sess.run(tf.assign(w1, 10))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line"></span><br><span class="line">    #每次sess.run会更新一次w1的滑动平均值</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br></pre></td></tr></table></figure><hr><h2 id="正则化缓解过拟合"><a href="#正则化缓解过拟合" class="headerlink" title="正则化缓解过拟合"></a>正则化缓解过拟合</h2><p>当模型在训练数据集上的正确率非常高，而很难对新数据集做出正确的相应时，可能是出现了过拟合现象。使用正则化可以有效缓解过拟合。<br>正则化在损失函数中引入模型复杂度指标，利用给w加权值，弱化了训练数据的噪声(一般不正则化b)</p><p>正则化公式：<br>loss = loss(y与y_)① + REGULARIZER② * loss(w)③<br>①指模型中所有参数的损失函数，如：交叉熵、均方误差等。<br>②指用超参数REGULARIZER给出参数w在总loss中的比例，即正则化的权重<br>③是需要正则化的参数<br>loss(w)有两种求法：<br><code>loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER(w))</code>       \(loss_{L1}(w) = \sum_{i}|w_{i}|\)<br><code>loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER(w))</code>       \((loss_{L2}(w) = \sum_{i}|w_{i}^{2}|\)</p><hr>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 机器学习 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在markdown中使用数学公式</title>
      <link href="/2019/08/14/%E5%9C%A8markdown%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
      <url>/2019/08/14/%E5%9C%A8markdown%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本文介绍使用MathJax引擎在markdown中插入数学公式<br>参考链接：<a href="https://www.jianshu.com/p/054484d0892a" target="_blank" rel="noopener">https://www.jianshu.com/p/054484d0892a</a></p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>首先在markdown头部添加如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>然后到LaTeX网站上写出自己想要的公式，网址：<a href="http://latex.codecogs.com/eqneditor/editor.php" target="_blank" rel="noopener">http://latex.codecogs.com/eqneditor/editor.php</a></p><p>在markdown中有两种插入数学公式的方法：<br>行间公式：<code>$$公式$$</code><br>行内公式(两个\转义成一个\)：<code>\\(公式\\)</code></p><p>将LaTeX网站上生成的公式，放在上面两种方式对应的位置上即可。</p><p><strong>注意网站上生成的公式中如果使用了转义字符，比如原公式中使用了<code>\_</code>来转义成<code>_</code>，那么放到markdown中就需要改成<code>\\_</code>来转义成<code>_</code>。</strong></p><hr>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
          <category> markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow笔记一</title>
      <link href="/2019/08/13/Tensorflow%E7%AC%94%E8%AE%B0%E4%B8%80/"/>
      <url>/2019/08/13/Tensorflow%E7%AC%94%E8%AE%B0%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p> mooc 北京大学曹健老师课程：tensorflow笔记 第三节 Tensorflow框架 要点记录</p><hr><h2 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h2><h3 id="基于TensorFlow的NN："><a href="#基于TensorFlow的NN：" class="headerlink" title="基于TensorFlow的NN："></a>基于TensorFlow的NN：</h3><p> 用张量表示数据，用计算图搭建神经网络，用回话执行计算图，优化线上的权重（参数），得到模型。</p><h3 id="张量-tensor-：多维数组-列表"><a href="#张量-tensor-：多维数组-列表" class="headerlink" title="张量(tensor)：多维数组(列表)"></a>张量(tensor)：多维数组(列表)</h3><p> 张量的维数称为<strong>阶</strong><br> 0阶张量称为标量(scalar)，如123<br> 1阶张量称为向量(vector)<br> 2阶张量称为矩阵(matrix)<br> 张量可以表示0阶到n阶数组</p><h3 id="计算图-Graph-："><a href="#计算图-Graph-：" class="headerlink" title="计算图(Graph)："></a>计算图(Graph)：</h3><p> 搭建神经网络的计算过程，只搭建，不计算</p><h3 id="会话-Session-："><a href="#会话-Session-：" class="headerlink" title="会话(Session)："></a>会话(Session)：</h3><p> 执行计算图中的节点运算<br> 用python实现计算的语法：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tensorflow.Session() as sess:</span><br><span class="line">   print(sess.run(y))</span><br></pre></td></tr></table></figure></p><h3 id="参数：是指神经元线上的权重，用标量表示，随机赋给初值"><a href="#参数：是指神经元线上的权重，用标量表示，随机赋给初值" class="headerlink" title="参数：是指神经元线上的权重，用标量表示，随机赋给初值"></a>参数：是指神经元线上的权重，用标量表示，随机赋给初值</h3><p> 随机赋值举例：<br> <code>w = tf.Variable(tf.randon_normal([2,3], stddev = 2, mean = 0, seed = 1))</code><br> random_normal表示正态分布，stddev是标准差，mean是平均值， seed是随机种子。<br> 标准差，平均值，随机种子可不写，随机种子不写每次随机的值都不一样。</p><p> 另外几个常用方法：<br> <code>tf.truncated_normal()</code><br> 去掉过大偏离点的正态分布（随机出来的值与平均值差距超过两个标准差，则舍弃）<br> <code>tf.random_uniform()</code><br> 平均分布<br> <code>tf.zeros()</code><br> 生成全0数组，如tf.zeros([3,2], int32)<br> <code>tf.ones()</code><br> 生成全1数组，如tf.ones([3,2], int32)<br> <code>tf.fill()</code><br> 生成定值数组，如tf.fill([3,2], 6)<br> <code>tf.constant()</code><br> 生成指定数组</p><hr><h2 id="神经网络实现过程"><a href="#神经网络实现过程" class="headerlink" title="神经网络实现过程"></a>神经网络实现过程</h2><ol><li>准备数据集，提取特征，作为输入喂给神经网络（Neural Network，NN）</li><li>搭建NN结构，从输入到输出（先搭建计算图，再用会话执行）<br>（NN前向传播算法 -&gt; 计算输出）</li><li>大量特征数据喂给NN，迭代优化NN参数<br>（NN反向传播算法 -&gt; 优化参数训练模型）</li><li>使用训练好的模型预测和分类</li></ol><hr><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p> 示例：<br> <img src="/uploads/tensorflow_notes/image1.png" alt="&quot;示例&quot;" title="前向传播讲解示例"><br> 推导：<br> <img src="/uploads/tensorflow_notes/image2.png" alt="&quot;推导&quot;" title="前向传播示例推导"><br> W矩阵，前面有m个节点，后面有n个节点，则为mXn阶矩阵。<br> 输入不计入神经网络层，a层是第一个计算层，也是神经网络的第一层，这里是1X3阶矩阵</p><p> 说明：</p><ul><li><p>变量初始化、计算图节点运算都要使用会话（with结构）实现<br><code>with tf.Session() as sess</code></p></li><li><p>变量初始化<br><code>init_op = tf.global_variables_initializer()</code><br><code>sess.run(init_op)</code></p></li><li><p>计算图节点运算: sess.run()中写入带运算节点<br><code>sess.run(y)</code></p></li><li><p>给神经网络喂数据：用tf.placeholder占位，在sess.run()中使用feed_dict喂数据<br>喂一组数据：<br><code>x = tf.placeholder(tf.float32, shape=(1,2))</code><br><code>sess.run(y, feed_dict={x:[[0.5, 0.6]]}</code></p><p>喂多组数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">sess.run(y, feed_dict=&#123;x:[[0.1, 0.2], [0.2, 0.3], [0.3, 0.4]]&#125;)</span><br></pre></td></tr></table></figure><p>shape第二个参数是模型的特征数，比如此模型有重量和体积两个特征，故为2。</p></li></ul><p>代码示例1：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> #coding:utf-8</span><br><span class="line">#两层简单神经网络</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#定义输入和参数</span><br><span class="line">x = tf.constant([[0.7, 0.5]])</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,3], stddev = 1, seed=1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3,1], stddev = 1, seed = 1))</span><br><span class="line"></span><br><span class="line">#定义前向传播过程</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">#用会话计算结果</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(&quot;result y in this file is:\n&quot;, sess.run(y))</span><br></pre></td></tr></table></figure></p><p>代码示例2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#两层简单神经网络</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#定义输入和参数</span><br><span class="line">#用placeholder实现输入定义</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(1,2))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed = 1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed = 1))</span><br><span class="line"></span><br><span class="line">#定义前向传播过程</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">#用会话计算结果</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(&quot;y in this file is:\n&quot;, sess.run(y, feed_dict = &#123;x:[[0.7, 0.5]]&#125;))</span><br></pre></td></tr></table></figure><p>代码示例3：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#两层简单神经网络(喂多组数据)</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#定义输入和参数</span><br><span class="line">#用placeholder定义输入(sess.run()喂多组数据)</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))</span><br><span class="line"></span><br><span class="line">#定义前向传播过程</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">#调用会话计算结果</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer();</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(&quot;y in this file is:\n&quot;, sess.run(y, feed_dict = &#123;x:[[0.7, 0.5],[0.2, 0.3],[0.3, 0.4], [0.4, 0.5]]&#125;))</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w2))</span><br></pre></td></tr></table></figure><hr><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p> 反向传播是一个不断训练模型参数，在所有参数上使用梯度下降，使NN模型在训练数据上的损失函数最小</p><p> 损失函数(loss)：预测值(y)与已知答案(y_)的差距。 均方误差MSE是计算损失函数的一种方法</p><p> 均方误差MSE：MSE(y_, y) = \(\frac{\sum_{i=1}^{n}(y-y\_)^{2}}{n}\)</p><p> 均方误差计算损失函数代码：<br> <code>loss = tf.reduce_mean(tf.square(y-y_))</code><br> 其中y和y_都是张量</p><p> 反向传播的训练方法：以减小loss值为优化目标<br> 三种训练方法：<br> <code>train_step = tf.train.GiadientDescentOptimizer(learning_rate).minimize(loss)</code><br> <code>train_step = tr.train.MomentumOptimizer(learning_rate, momentum).minimize(loss)</code><br> <code>train_step = tr.train.AdamOptimizer(learning_rate),minimize(loss)</code><br> learning_rate是指学习率，它决定每次更新的幅度，一开始可以选一个较小值，比如0.001</p><p> 反向传播代码示例：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf_8</span><br><span class="line">#反向传播过程实例</span><br><span class="line">#导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">seed = 23455</span><br><span class="line"></span><br><span class="line">#基于seed生成随机数</span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line">#随机数返回32行2列的矩阵 表示32组 体积和重量 作为输入数据集</span><br><span class="line">X = rng.rand(32, 2)</span><br><span class="line">#从X这个32行2列的矩阵中 取出一行 判断如果和小于1 给Y赋值1 如该和不小于1 给Y赋值0 作为输入数据集的标签</span><br><span class="line">Y = [[int(x0 + x1 &lt;1)] for (x0, x1) in X]</span><br><span class="line">print(&quot;X:\n&quot;, X)</span><br><span class="line">print(&quot;Y:\n&quot;, Y)</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line"></span><br><span class="line">#定义神经网络的参数</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))</span><br><span class="line"></span><br><span class="line">#定义神经网络前向传播过程</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">#定义损失函数</span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">#三种反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)</span><br><span class="line">#train_step = tf.train.MomentumOptimizer(0.001, 0.9).minimize(loss)</span><br><span class="line">#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)</span><br><span class="line"></span><br><span class="line">#生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    #输出当前（未经训练）的参数取值</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    #训练模型</span><br><span class="line">    STEPS = 3000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) %32</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y[start:end]&#125;)</span><br><span class="line">        if i%500 == 0:</span><br><span class="line">            total_loss = sess.run(loss, feed_dict=&#123;x:X, y_:Y&#125;)</span><br><span class="line">            print(&quot;After %d training steps, loss on all data is %g&quot;%(i, total_loss))</span><br><span class="line">    #输出训练后的参数取值</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br></pre></td></tr></table></figure></p><hr><h2 id="搭建神经网络的八股：准备、前向传播、反向传播、迭代"><a href="#搭建神经网络的八股：准备、前向传播、反向传播、迭代" class="headerlink" title="搭建神经网络的八股：准备、前向传播、反向传播、迭代"></a>搭建神经网络的八股：准备、前向传播、反向传播、迭代</h2><ul><li><p>准备<br>import 相关模块、 定义常量、 生成数据集</p></li><li><p>前向传播<br>定义输入：特征输入x，标准答案y_<br>定义参数：（一般是随机）定义第一层网络参数w1和第二程网络参数w2<br>定义输出：定义计算图(Graph)，即定义第一层网络a和结果y的计算过程</p></li><li><p>反向传播<br>定义损失函数：loss<br>定义反向传播方法：train_step</p></li><li><p>迭代：生成会话(Session)，训练STEPS轮</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess_run(init_op)</span><br><span class="line">    </span><br><span class="line">    STEPS = 3000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = </span><br><span class="line">        end = </span><br><span class="line">        sess.run(train_step, feed_dict=)</span><br></pre></td></tr></table></figure></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 机器学习 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unity用IL2Cpp打包64位版本aab文件，上线Google商店</title>
      <link href="/2019/08/06/Unity%E7%94%A8IL2Cpp%E6%89%93%E5%8C%8564%E4%BD%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E4%B8%8A%E7%BA%BFGoogle%E5%95%86%E5%BA%97/"/>
      <url>/2019/08/06/Unity%E7%94%A8IL2Cpp%E6%89%93%E5%8C%8564%E4%BD%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E4%B8%8A%E7%BA%BFGoogle%E5%95%86%E5%BA%97/</url>
      
        <content type="html"><![CDATA[<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>本文转载自<a href="https://www.cnblogs.com/INSIST-NLJY/p/11044558.html" target="_blank" rel="noopener">https://www.cnblogs.com/INSIST-NLJY/p/11044558.html</a> 部分内容，侵删</p><p>安卓Apk上线Google要求64位且打包成Android Asset Bundle（即aab格式），以下是相关设置</p><blockquote><ol><li>在PlaySettings-&gt;other settings-&gt;Scriptiing Backend 选择IL2CPP（默认是Mono）, c++ Compiler Configuration 选择Release Target Architectures 里面的Arm64就可以勾选的了，勾选打包即可</li></ol></blockquote><blockquote><ol start="2"><li>Android Asset Bundle优化在Build Sttings 勾选Build App Bundle（Google Play）即可，打出的是aab包 打包运行成功。</li></ol></blockquote><blockquote><p>注：要是测试的话就用Mono打包吧，毕竟IL2CPP打包要慢上几倍</p></blockquote><hr><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://www.cnblogs.com/INSIST-NLJY/p/11044558.html" target="_blank" rel="noopener">https://www.cnblogs.com/INSIST-NLJY/p/11044558.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Unity </category>
          
          <category> 安卓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客搭建过程</title>
      <link href="/2019/08/04/%E6%88%91%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/"/>
      <url>/2019/08/04/%E6%88%91%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>此教程记录了我在Win10上搭建Hexo博客的整个流程，主题是使用了Next，奉上文档以及参考链接：<br><a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo文档</a><br><a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">Next文档</a></p><hr><h2 id="二、-基础准备"><a href="#二、-基础准备" class="headerlink" title="二、 基础准备"></a>二、 基础准备</h2><h3 id="1-准备一个Github账号："><a href="#1-准备一个Github账号：" class="headerlink" title="1. 准备一个Github账号："></a>1. 准备一个Github账号：</h3><p>0v0</p><h3 id="2-Git安装与配置："><a href="#2-Git安装与配置：" class="headerlink" title="2. Git安装与配置："></a>2. Git安装与配置：</h3><p> Git安装自不必多说，简单提一下Git用SSH登录Github：</p><ul><li><p>首先打开CMD，设置你的账号和邮箱，输入：<br><code>git config --global user.name &quot;yourname&quot;</code><br><code>git config --global user.email &quot;youremail&quot;</code><br>其中”yourname”是你的github账号，”youremail”是你的github账号邮箱</p></li><li><p>可以用以下两条命令检查输入<br><code>git config user.name</code><br><code>git config user.email</code></p></li><li><p>然后用以下命令创建SSH秘钥<br><code>ssh-keygen -t rsa -C &quot;youremail&quot;</code><br>后续连敲3次回车，不需要任何输入。<br>完成之后会告诉你生成了.ssh文件夹，找到文件夹，其中id_rsa是秘钥，id_rsa.pub是公钥，用文本编辑器打开id_rsa.pub，复制所有内容。</p></li><li><p>添加公钥到Github<br>登录Github，右上角 头像-&gt;Settings -&gt; SSH and GPG keys -&gt; New SSH key。 把公钥粘贴到key中，填好title并点击Add SSH key。</p></li><li><p>回到CMD，输入命令<br><code>ssh -T git@github.com</code><br>选yes，提示成功。</p></li></ul><h3 id="3-安装Nodejs"><a href="#3-安装Nodejs" class="headerlink" title="3. 安装Nodejs"></a>3. 安装Nodejs</h3><ul><li><p>下载网址<br><a href="https://nodejs.org" target="_blank" rel="noopener">nodejs.org</a><br>选LTS就好了，下载完成后安装。</p></li><li><p>安装完成有两个组件，nodejs和npm，可以使用以下命令查看版本<br><code>node -v</code><br><code>npm -v</code></p></li><li><p>用npm安装cnpm淘宝镜像源<br><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code><br>用以下命令查看版本<br><code>cnpm -v</code><br>安装完成以后就都是用cnpm下载了，国内用npm速度你懂的</p></li></ul><h3 id="4-安装博客框架Hexo"><a href="#4-安装博客框架Hexo" class="headerlink" title="4. 安装博客框架Hexo"></a>4. 安装博客框架Hexo</h3><ul><li><p>安装命令<br><code>cnpm install -g hexo-cli</code></p></li><li><p>查看版本<br><code>hexo -v</code></p></li></ul><hr><h2 id="三、-搭建Hexo博客"><a href="#三、-搭建Hexo博客" class="headerlink" title="三、 搭建Hexo博客"></a>三、 搭建Hexo博客</h2><h3 id="1-Hexo所有常用命令"><a href="#1-Hexo所有常用命令" class="headerlink" title="1. Hexo所有常用命令"></a>1. Hexo所有常用命令</h3><p> <a href="https://hexo.io/zh-cn/docs/commands" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/commands</a></p><h3 id="2-新建Blog文件夹"><a href="#2-新建Blog文件夹" class="headerlink" title="2. 新建Blog文件夹"></a>2. 新建Blog文件夹</h3><p> 以后如果有安装错误，可以直接删掉Blog文件夹，从 三、重新开始</p><h3 id="3-CMD定位到Blog文件夹"><a href="#3-CMD定位到Blog文件夹" class="headerlink" title="3. CMD定位到Blog文件夹"></a>3. CMD定位到Blog文件夹</h3><p> 比如我这里是D:\Blog，那么我的命令是<br> <code>d: blog</code></p><h3 id="4-初始化Hexo"><a href="#4-初始化Hexo" class="headerlink" title="4. 初始化Hexo"></a>4. 初始化Hexo</h3><p> <code>hexo init</code><br> 耐心等待初始化完成</p><h3 id="5-第一次启动博客"><a href="#5-第一次启动博客" class="headerlink" title="5. 第一次启动博客"></a>5. 第一次启动博客</h3><p> <code>hexo s</code><br> s是server的简写，命令执行完之后，打开浏览器，输入<br> <code>localhost:4000</code><br> 即可进入本地服务器中的博客。 结束本地服务器按Ctrl+C。</p><h3 id="6-创建博客文章"><a href="#6-创建博客文章" class="headerlink" title="6. 创建博客文章"></a>6. 创建博客文章</h3><p> <code>hexo n &quot;我的第一篇博客文章&quot;</code><br> n是new的简写，创建完成后文章会放到source/_posts文件夹下面，md文件。</p><p> markdown语法自行学习，推荐使用sublime text3写markdown，宇宙第一文本编辑器。</p><p> 打开文件之后，开头的部分内容官方称为Font-matter，这里你可以设定文章标题，文章创建更新时间，标签，分类等等，具体请查阅<br> <a href="https://hexo.io/zh-cn/docs/front-matter" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/front-matter</a></p><p> 另外这里提一个坑，Next主题的文章目录索引是根据markdown的#多少来实现的，举例来说，如果文章中第一个标题是##，那么所有的##都是一级目录索引，所有的###都是二级目录索引，以此类推。<strong>不能越级！！！</strong>，也就是说，##下一级标题必须是###，否则就会出现 2.0.1这种标题，甚至2.0.0.1，非常丑。而且如果你越级了，还在某一级标题的内容里面使用列表来排版，文章目录索引会出现奇怪的bug，虽然在文章里看起来一切正常。Next的目录解析就是这样的，so，最好不要越级使用标题。</p><h3 id="7-博客撰写流程"><a href="#7-博客撰写流程" class="headerlink" title="7. 博客撰写流程"></a>7. 博客撰写流程</h3><ul><li><p>如6所示，new一个博客文章</p></li><li><p>编辑md文件</p></li><li><p>CMD切到Blog文件夹下</p></li><li><p>清理（必要时清理，不必每次都清理）<br><code>hexo cl</code></p></li><li><p>生成<br><code>hexo g</code></p></li><li><p>启动本地服务器<br><code>hexo s</code></p></li><li><p>浏览器打开localhost：4000 查看效果</p></li></ul><h3 id="8-远程部署到Github"><a href="#8-远程部署到Github" class="headerlink" title="8. 远程部署到Github"></a>8. 远程部署到Github</h3><ul><li><p>登录Github，新建一个仓库<br>右上角+号 -&gt; new repository</p></li><li><p>项目命名必须符合要求<br>必须是”yourname.github.io”，比如我的是”brandonvno.github.io”<br>描述写一下，选public，然后点Create Repository</p></li><li><p>为Hexo安装Github部署插件<br>CMD切到Blog，输入命令<br><code>cnpm install --save hexo-deployer-git</code><br>警告不用管</p></li><li><p>编辑Hexo全局设置文件_config.yml<br>打开Blog下的_config.yml，翻到最后，找到deploy，设置参考如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">repo: https://github.com/brandonvno/yourname.github.io.git</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure><p>yourname改成你的Github昵称，下同理。修改完保存。</p></li><li><p>部署到Github<br><code>hexo d</code><br>d是deploy简写。<br>这个过程就是使用你Git中SSH关联的账号往你刚刚新建的仓库中push文件。<br>如果你的GIt没用SSH的话，这里会让你登陆，而且如果你的Git关联的Github账号和你的仓库所有者账号不一样的话，你就会惊奇的发现：我的仓库怎么不是我push的 Σ(っ°Д°;)っ？？？<br>等待部署完成之后，打开yourname.github.io，即可查看效果。<br>通常部署完成之后要等待一段时间，一些效果比如插件才会正常显示。</p></li></ul><hr><h2 id="四、-主题设置"><a href="#四、-主题设置" class="headerlink" title="四、 主题设置"></a>四、 主题设置</h2><p> Hexo主题一览：<a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a><br> 旧版Next文档：<a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">http://theme-next.iissnan.com/</a><br> 旧版Next地址：<a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">https://github.com/iissnan/hexo-theme-next</a><br> 新版Next文档：<a href="https://theme-next.org/docs/" target="_blank" rel="noopener">https://theme-next.org/docs/</a><br> 新版Next地址：<a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">https://github.com/theme-next/hexo-theme-next</a><br> 本博客使用的主题是新版Next，有四种样式，集成了很多第三方功能，十分方便。另外新版文档内容不多，我是参考旧文档搭建的。<br> 如果你不想用Next，那么只需看一下如何安装主题即可，其它功能自行研究，推荐另一个主题：yilia。</p><p> 以下过程会涉及到两个配置文件，一个是位于Blog文件夹下的Hexo全局配置文件_config.yml，另一个是位于Blog/themes/Next 下的主题配置文件_config.yml，下面就分别称<strong>全局配置文件</strong>和<strong>主题配置文件</strong></p><h3 id="1-安装主题"><a href="#1-安装主题" class="headerlink" title="1. 安装主题"></a>1. 安装主题</h3><ul><li><p>CMD切换到Blog文件夹，输入命令<br><code>git clone https://github.com/theme-next/hexo-theme-next.git themes/next</code><br>这个命令执行过程大概如下：现在themes下新建next文件夹，然后将主题克隆到此文件夹下</p></li><li><p>配置Hexo全局设置文件_config.yml<br>找到theme字段，默认是landscape，改成next。<br>注意如果你的主题文件夹名字改了，这里也要相应的改一下。</p></li><li><p>启动本地服务器查看效果</p></li></ul><h3 id="2-Next主题样式切换"><a href="#2-Next主题样式切换" class="headerlink" title="2. Next主题样式切换"></a>2. Next主题样式切换</h3><p> 打开主题配置文件，搜索scheme settings，这里有四种样式，其中三个被注释掉了，选择你喜欢的样式即可。</p><h3 id="3-设置语言"><a href="#3-设置语言" class="headerlink" title="3. 设置语言"></a>3. 设置语言</h3><p> 打开next文件夹下面的language，找到对应的语言，打开全局配置文件，搜索language，将你想设置的语言文件名填上即可。</p><h3 id="4-菜单栏导航设置"><a href="#4-菜单栏导航设置" class="headerlink" title="4. 菜单栏导航设置"></a>4. 菜单栏导航设置</h3><p> 打开主题配置文件，搜索Menu Setting，默认情况下是只有主页和归档栏没有被注释，选择你想要在导航栏显示的内容将注释去掉即可。<br> 同事你也可以打开上一步提到的语言文件，打开即可看到对应的翻译。<br> 这样配置之后，只是打开了这个选项，而选项链接对应的页面还需要手动创建。下面介绍了标签和分类页面的创建方式。</p><h3 id="5-添加标签页面"><a href="#5-添加标签页面" class="headerlink" title="5. 添加标签页面"></a>5. 添加标签页面</h3><ul><li><p>CMD定位到Blog文件夹下，输入以下命令创建标签页面<br><code>hexo new page tags</code></p></li><li><p>打开Blog/source/tags/index.md<br>在data下面添加如下内容<br><code>type: &quot;tags&quot;</code><br>确保第4步开启了标签选项，启动本地服务器后，点击菜单栏标签选项即可跳转到标签页。</p></li><li><p>在博客文章的Font-matter部分添加自定义标签，比如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tags:</span><br><span class="line"> - Testing</span><br><span class="line"> - Another Tag</span><br></pre></td></tr></table></figure><p>即可在标签页看到对应的标签</p></li></ul><h3 id="6-添加分类页面"><a href="#6-添加分类页面" class="headerlink" title="6. 添加分类页面"></a>6. 添加分类页面</h3><ul><li><p>CMD定位到Blog文件夹下，输入以下命令创建分类页面<br><code>hexo new page categories</code></p></li><li><p>打开Blog/source/tags/index.md<br>在data下面添加如下内容<br><code>type: &quot;categories&quot;</code><br>确保第4步开启了分类选项，启动本地服务器后，点击菜单栏分类选项即可跳转到分类页。</p></li><li><p>在博客文章的Font-matter部分添加自定义分类，比如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">categories: </span><br><span class="line"> - Testing</span><br></pre></td></tr></table></figure></li></ul><h3 id="7-分类和标签的区别"><a href="#7-分类和标签的区别" class="headerlink" title="7. 分类和标签的区别"></a>7. 分类和标签的区别</h3><p> 引用Hexo文档中的两句话:</p><blockquote><p>在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 <code>Foo, Bar</code> 不等于 <code>Bar, Foo</code>；而标签没有顺序和层次。</p></blockquote><blockquote><p>Hexo不支持指定多个同级分类。下面的指定方法：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">categories:</span><br><span class="line">- Diary</span><br><span class="line">- Life</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>会使分类Life成为Diary的子分类，而不是并列分类。因此，有必要为您的文章选择尽可能准确的分类。</p></blockquote><p> 原文链接：<a href="https://hexo.io/zh-cn/docs/front-matter" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/front-matter</a></p><h3 id="8-侧栏部分设置"><a href="#8-侧栏部分设置" class="headerlink" title="8. 侧栏部分设置"></a>8. 侧栏部分设置</h3><p>打开主题配置文件，搜索Sidebar Settings</p><ul><li><p>个人社交链接：<br>找到social字段，将自己想要显示的社交平台注释删掉，然后写入自己的社交平台地址即可</p></li><li><p>友情链接<br>找到links字段，将自己想要显示的内容写入即可，格式：<br><code>title: https://something</code></p></li><li><p>头像<br>找到avata字段，注释文档写的很清楚了，我这里翻译一下：<br>放在next/source/images文件夹下，url字段填<code>/images/avatar.gif</code><br>放在Blog/source/uploads文件夹下，url字段填<code>/uploads/avatar.gif</code></p></li></ul><h3 id="9-作者昵称、网站标题、副标题、描述等"><a href="#9-作者昵称、网站标题、副标题、描述等" class="headerlink" title="9. 作者昵称、网站标题、副标题、描述等"></a>9. 作者昵称、网站标题、副标题、描述等</h3><p>打开全局配置文件，在开头Site内容块内修改对应内容即可</p><h3 id="10-打赏功能"><a href="#10-打赏功能" class="headerlink" title="10. 打赏功能"></a>10. 打赏功能</h3><p>打开主题配置文件，搜索Reward，把enable值设为true即可打开，comment填你想说的话。<br>打赏功能只有在具体文章里才会显示</p><h3 id="11-站点统计"><a href="#11-站点统计" class="headerlink" title="11. 站点统计"></a>11. 站点统计</h3><p>Next写好了各种各样的站点统计，有需要用leancloud实现的，但是需要实名注册，比较麻烦，我是使用不蒜子，修改几个参数即可，简单方便。</p><ul><li><p>不蒜子<br>打开主题配置文件，搜索Statistics and Analytics即可看到统计板块。<br>直接搜索busuanzi即可看到不蒜子设置代码。想要显示的内容对应字段改为true即可。</p></li><li><p>百度统计<br>另外也可以使用谷歌统计、百度统计、腾讯统计等，这里简单介绍下百度统计。<br>登录百度统计，一番折腾之后，定位到站点代码获取页面，大概内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> &lt;script&gt;</span><br><span class="line">var _hmt = _hmt || [];</span><br><span class="line">(function() &#123;</span><br><span class="line">  var hm = document.createElement(&quot;script&quot;);</span><br><span class="line">  hm.src = &quot;https://hm.baidu.com/hm.js?xxxxxxxxxxxxxxxxxxxxxxxxxx&quot;;</span><br><span class="line">  var s = document.getElementsByTagName(&quot;script&quot;)[0]; </span><br><span class="line">  s.parentNode.insertBefore(hm, s);</span><br><span class="line">&#125;)();</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>打开主题配置文件，找到baidu_analytics字段，将xxxxxxxx对应的内容填上即可</p></li></ul><h3 id="12-评论系统"><a href="#12-评论系统" class="headerlink" title="12. 评论系统"></a>12. 评论系统</h3><p> 打开主题配置文件，搜索Comments and Widgets即可看到Hexo支持的评论系统。</p><p> 介绍一下Gitalk，基于github的评论系统。</p><ul><li><p>首先在Comments and Widgets下面找到Gitalk字段<br>enable改为true<br>github_id填你的昵称<br>repo填你自己的远程部署项目，如”yourname.github.io”<br>admin_user填你自己。<br>client_id和client_secret需要到Github上去创建。</p></li><li><p>在Github创建Application<br>打开<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">https://github.com/settings/applications/new</a><br>各项填好之后点注册，然后将id和secret拷贝到上一步的位置<br>生成、启动本地服务器，查看效果，完成。</p></li><li><p>再介绍一下来必力评论系统：<br>官网注册：<a href="https://www.livere.com/" target="_blank" rel="noopener">https://www.livere.com/</a><br>安装免费版，设置你的个人博客地址，获取data-uid，打开主题配置文件，搜索livere_uid，填上你的uid，搞定。<br>参考：<a href="https://lemonxq.cn/2017/11/20/Hexo%E4%B9%8B%E4%BD%BF%E7%94%A8Livere%E8%AF%84%E8%AE%BA%E4%BB%A3%E6%9B%BF%E5%A4%9A%E8%AF%B4%E8%AF%84%E8%AE%BA/" target="_blank" rel="noopener">前辈的博文</a></p></li></ul><h3 id="13-搜索系统"><a href="#13-搜索系统" class="headerlink" title="13. 搜索系统"></a>13. 搜索系统</h3><p>打开主题配置文件，搜索Search Services即可看到所有支持的搜索系统，我使用的是local_search<br>安装文档：<a href="https://github.com/theme-next/hexo-generator-searchdb" target="_blank" rel="noopener">https://github.com/theme-next/hexo-generator-searchdb</a><br>讲的很清楚了，注意的是安装完插件之后记得<code>hexo g</code>一下，否则找不到search.xml<br>找到local_search字段，对应的字段填好就完事了。<br>设置好之后启动本地服务器可以在菜单栏看到一个小小的搜索，点开弹出一个大大的弹框。</p><h3 id="14-RSS"><a href="#14-RSS" class="headerlink" title="14. RSS"></a>14. RSS</h3><p>什么是RSS？我就不班门弄斧了=。=<br>配置RSS需要安装一个插件，安装文档：<a href="https://github.com/hexojs/hexo-generator-feed" target="_blank" rel="noopener">https://github.com/hexojs/hexo-generator-feed</a><br>讲的很清楚了，注意的是安装完插件之后记得<code>hexo g</code>一下，否则找不到atom.xml<br>至于主题配置文件里的rss字段，注释讲可以留空。<br>配置完之后启动本地服务器，如果左侧导航栏出现RSS，点击之后进入到一个写满HTML代码的页面，说明安装成功了。<br>可以自行安装一个RSS阅读器订阅一下，我用的是irreader。</p><h3 id="15-动画"><a href="#15-动画" class="headerlink" title="15. 动画"></a>15. 动画</h3><p>打开主题配置文件，搜索Animation Settings可以看到支持的动画。<br>motion是刚进入博客时各个板块位移的动画，考虑加载速度可以选择关闭<br>three需要安装第三方插件：<a href="https://github.com/theme-next/theme-next-three" target="_blank" rel="noopener">https://github.com/theme-next/theme-next-three</a><br>canvas_net同样也需要安装，但是根据注释的地址安装完之后并没有卵用= =，感兴趣的可以自己试试。</p><h3 id="16-国内外分流以及被百度引擎收录"><a href="#16-国内外分流以及被百度引擎收录" class="headerlink" title="16. 国内外分流以及被百度引擎收录"></a>16. 国内外分流以及被百度引擎收录</h3><p>留个坑，日后有时间再研究<br>参考：<a href="https://blog.csdn.net/sinat_37781304/article/details/82729029" target="_blank" rel="noopener">https://blog.csdn.net/sinat_37781304/article/details/82729029</a></p><h3 id="17-live2d萌妹子"><a href="#17-live2d萌妹子" class="headerlink" title="17. live2d萌妹子"></a>17. live2d萌妹子</h3><p>安装文档：<a href="https://github.com/EYHN/hexo-helper-live2d" target="_blank" rel="noopener">https://github.com/EYHN/hexo-helper-live2d</a></p><p>安装命令： <code>npm install --save hexo-helper-live2d</code></p><p>在全局配置文件中追加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">live2d:</span><br><span class="line">  enable: true</span><br><span class="line">  scriptFrom: local</span><br><span class="line">  # pluginRootPath: live2dw/</span><br><span class="line">  # pluginJsPath: lib/</span><br><span class="line">  # pluginModelPath: public/</span><br><span class="line">  log: false</span><br><span class="line">  model: </span><br><span class="line">    # use: live2d-widget-model-koharu</span><br><span class="line">    scale: 1</span><br><span class="line">    hHeadPos: 0.5</span><br><span class="line">    vHeadPos: 0.618</span><br><span class="line">  display:</span><br><span class="line">    superSample: 2</span><br><span class="line">    width: 200</span><br><span class="line">    height: 400</span><br><span class="line">    position: right</span><br><span class="line">    hOffset: 0</span><br><span class="line">    vOffset: -20</span><br><span class="line"> # mobile:</span><br><span class="line"> #    show: false</span><br><span class="line">  react:</span><br><span class="line">    opacityDefault: 0.5</span><br><span class="line">    opacityOnHover: 0.5</span><br><span class="line">    opacity: 0.7</span><br></pre></td></tr></table></figure><p>mobile不注释掉会报错= =</p><p>下面介绍一下更换模型<br>模型一览：<a href="https://huaji8.top/post/live2d-plugin-2.0/" target="_blank" rel="noopener">https://huaji8.top/post/live2d-plugin-2.0/</a><br>安装方式：<code>npm install {your model&#39;s package name}</code>  比如：<code>npm install live2d-widget-model-koharu</code>  然后到全局配置文件中把model.use 对应的值改成模型名字即可。<br>模型列表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">live2d-widget-model-chitose</span><br><span class="line">live2d-widget-model-epsilon2_1</span><br><span class="line">live2d-widget-model-gf</span><br><span class="line">live2d-widget-model-haru/01 (use npm install --save live2d-widget-model-haru)</span><br><span class="line">live2d-widget-model-haru/02 (use npm install --save live2d-widget-model-haru)</span><br><span class="line">live2d-widget-model-haruto</span><br><span class="line">live2d-widget-model-hibiki</span><br><span class="line">live2d-widget-model-hijiki</span><br><span class="line">live2d-widget-model-izumi</span><br><span class="line">live2d-widget-model-koharu</span><br><span class="line">live2d-widget-model-miku</span><br><span class="line">live2d-widget-model-ni-j</span><br><span class="line">live2d-widget-model-nico</span><br><span class="line">live2d-widget-model-nietzsche</span><br><span class="line">live2d-widget-model-nipsilon</span><br><span class="line">live2d-widget-model-nito</span><br><span class="line">live2d-widget-model-shizuku</span><br><span class="line">live2d-widget-model-tororo</span><br><span class="line">live2d-widget-model-tsumiki</span><br><span class="line">live2d-widget-model-unitychan</span><br><span class="line">live2d-widget-model-wanko</span><br><span class="line">live2d-widget-model-z16</span><br></pre></td></tr></table></figure><hr><h2 id="五、-将博客源文件上传到Github"><a href="#五、-将博客源文件上传到Github" class="headerlink" title="五、 将博客源文件上传到Github"></a>五、 将博客源文件上传到Github</h2><p>此部分内容为转载，非原创。<br>原文：<a href="https://blog.csdn.net/sinat_37781304/article/details/82729029" target="_blank" rel="noopener">https://blog.csdn.net/sinat_37781304/article/details/82729029</a></p><blockquote><p>机制：由于<code>hexo d</code>上传部署到github的其实是hexo编译后的文件，是用来生成网页的，不包含源文件，其他文件 ，包括我们写在source 里面的，和配置文件，主题文件，都没有上传到github</p></blockquote><blockquote><p>所以可以利用git的分支管理，将源文件上传到github的另一个分支即可。<br>首先，先在github上新建一个hexo分支，然后在这个仓库的settings中，选择默认分支为hexo分支（这样每次同步的时候就不用指定分支，比较方便）。<br>然后使用sourceTree将仓库克隆到本地，把除了.git的全部内容都删除掉。把之前我们写的博客源文件全部复制过来，除了.deploy_git。这里应该说一句，复制过来的源文件应该有一个.gitignore，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下内容，表示这些类型文件不需要同步<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> .DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules/</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>注意，如果你之前theme中的主题是通过克隆安装的，那么应该把主题文件中的.git文件夹删掉（或者为了以后方便更新主题，可以在本地快速打包一个.git压缩文件，需要更新的时候解压出来然后git pull），因为git不能嵌套上传，最好是显示隐藏文件，检查一下有没有，否则上传的时候会出错，导致你的主题文件无法上传，这样你的配置在别的电脑上就用不了了。安装的第三方插件同理。<br>然后push到远端。上传完了，可以去你的github上看一看hexo分支有没有上传上去，其中node_modules、public、db.json已经被忽略掉了，没有关系，不需要上传的，因为在别的电脑上需要重新输入命令安装 。</p></blockquote><blockquote><p>到新电脑上，重新搭建环境：安装git、设置git全局邮箱和用户名、设置ssh key、安装nodejs、安装hexo，但是已经不需要初始化了，而是在使用sourceTree将远端内容克隆到本地，然后cnpm安装deploy工具，开始写博客。</p></blockquote><hr><h2 id="六、-写作技巧、踩坑记录等杂项（持续更新）"><a href="#六、-写作技巧、踩坑记录等杂项（持续更新）" class="headerlink" title="六、 写作技巧、踩坑记录等杂项（持续更新）"></a>六、 写作技巧、踩坑记录等杂项（持续更新）</h2><h3 id="1-文章中插入图片"><a href="#1-文章中插入图片" class="headerlink" title="1. 文章中插入图片"></a>1. 文章中插入图片</h3><p>插入图片有两种方式，一种是使用网上的图片，需要借助一些七牛云存储，另一种是放在本地。<br>先写一写放在本地：因为我们使用Github托管博客源文件，也就不怕本地丢失了。</p><p>方法：<br>首先在Blog/source文件夹下新建一个文件夹，名字随意，我这里就用Next的uploads了。<br>然后markdown插入图片语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">![图片alt](图片地址 &apos;&apos;图片title&apos;&apos;)</span><br><span class="line">图片alt就是显示在图片下面的文字，相当于对图片内容的解释。</span><br><span class="line">图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加</span><br><span class="line">地址写/uploads/图片名字.type</span><br></pre></td></tr></table></figure><p>比如<code>![&quot;星际牛仔&quot;](/uploads/image1.jpg &quot;公路全家福&quot;)</code>效果如下<br><img src="/uploads/image1.jpg" alt="&quot;星际牛仔&quot;" title="公路全家福"></p><h3 id="2-文章中插入网易云音乐"><a href="#2-文章中插入网易云音乐" class="headerlink" title="2. 文章中插入网易云音乐"></a>2. 文章中插入网易云音乐</h3><p>网易云用网页打开，可以看到生成外链播放器，点击复制代码即可，这里分享两个星际牛仔的原声</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=592701&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=592701&auto=0&height=66"></iframe><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=22767373&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=22767373&auto=0&height=66"></iframe><h3 id="3-文章中插入B站视频"><a href="#3-文章中插入B站视频" class="headerlink" title="3. 文章中插入B站视频"></a>3. 文章中插入B站视频</h3><p>找到B站视频，点分享，找到嵌入代码，拷贝出aid和cid，然后填入下面代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe id=sbrxp src=&quot;//player.bilibili.com/player.html?aid=488321&amp;cid=735286&amp;page=7&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot; style=&quot;width: 800px; height: 600px; max-width: 100%&quot;&gt; &lt;/iframe&gt;</span><br></pre></td></tr></table></figure><p>效果如下：</p><iframe id="sbrxp" src="//player.bilibili.com/player.html?aid=488321&cid=735286&page=7" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 800px; height: 600px; max-width: 100%"> </iframe><h3 id="4-首页文章折叠"><a href="#4-首页文章折叠" class="headerlink" title="4. 首页文章折叠"></a>4. 首页文章折叠</h3><p>Text主题默认首页文章是显示全文的，非常反人类，修改方法是打开Text的主题配置文件_config.yml，搜索auto_excerpt，值改为true即可</p><hr><h2 id="七、-参考链接："><a href="#七、-参考链接：" class="headerlink" title="七、 参考链接："></a>七、 参考链接：</h2><p>谢谢你一直看到这里。<br><a href="https://www.bilibili.com/video/av44544186" target="_blank" rel="noopener">https://www.bilibili.com/video/av44544186</a><br><a href="https://www.codesheep.cn" target="_blank" rel="noopener">https://www.codesheep.cn</a><br><a href="https://blog.csdn.net/sinat_37781304/article/details/82729029" target="_blank" rel="noopener">https://blog.csdn.net/sinat_37781304/article/details/82729029</a><br><a href="https://blog.csdn.net/LemonXQ/article/details/72676005" target="_blank" rel="noopener">https://blog.csdn.net/LemonXQ/article/details/72676005</a></p><hr>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
          <category> 博客搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 博客搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
