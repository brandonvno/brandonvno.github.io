<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Qt学习资源和踩到的坑</title>
      <link href="/2019/09/02/Qt%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E5%92%8C%E8%B8%A9%E5%88%B0%E7%9A%84%E5%9D%91/"/>
      <url>/2019/09/02/Qt%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E5%92%8C%E8%B8%A9%E5%88%B0%E7%9A%84%E5%9D%91/</url>
      
        <content type="html"><![CDATA[<h2 id="Qt5-8-手把手教程"><a href="#Qt5-8-手把手教程" class="headerlink" title="Qt5.8 手把手教程"></a>Qt5.8 手把手教程</h2><p><a href="https://zhuanlan.zhihu.com/c_119081535" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/c_119081535</a></p><p><a href="https://blog.51cto.com/9291927/2138876" target="_blank" rel="noopener">https://blog.51cto.com/9291927/2138876</a></p><hr><h2 id="Qt-文档"><a href="#Qt-文档" class="headerlink" title="Qt 文档"></a>Qt 文档</h2><p>5.8文档<br><a href="https://doc.qt.io/archives/qt-5.8/index.html" target="_blank" rel="noopener">https://doc.qt.io/archives/qt-5.8/index.html</a></p><hr><h2 id="Qt5-9-C-开发指南及实例"><a href="#Qt5-9-C-开发指南及实例" class="headerlink" title="Qt5.9 C++开发指南及实例"></a>Qt5.9 C++开发指南及实例</h2><p>原贴：<a href="https://www.52pojie.cn/thread-966975-1-1.html" target="_blank" rel="noopener">https://www.52pojie.cn/thread-966975-1-1.html</a><br>链接：<a href="https://pan.baidu.com/s/1X1h4BClbDEo56or6T9aayQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1X1h4BClbDEo56or6T9aayQ</a><br>提取码：r7l0<br>有经济能力还是支持一下正版</p><p>最新示例代码：<br><a href="https://www.epubit.com/bookDetails?id=N25171" target="_blank" rel="noopener">https://www.epubit.com/bookDetails?id=N25171</a></p><hr><h2 id="一些小经验和踩到的坑-随时记录，可能比较杂乱"><a href="#一些小经验和踩到的坑-随时记录，可能比较杂乱" class="headerlink" title="一些小经验和踩到的坑(随时记录，可能比较杂乱)"></a>一些小经验和踩到的坑(随时记录，可能比较杂乱)</h2><ul><li><p>Qt Creator项目移动后，debug和release路径记得重新设置一下</p></li><li><p>Qt 信号与槽绑定的几种方式<br>方法一(Qt4和Qt5都可以用)：<br>connect()</p></li><li><p>QThread心得</p></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> 资源 </category>
          
          <category> Qt </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 资源 </tag>
            
            <tag> Qt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow笔记五</title>
      <link href="/2019/08/27/Tensorflow%E7%AC%94%E8%AE%B0%E4%BA%94/"/>
      <url>/2019/08/27/Tensorflow%E7%AC%94%E8%AE%B0%E4%BA%94/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p> mooc 北京大学曹健老师课程：tensorflow笔记 第八节 卷积神经网络 要点记录<br> 版本：python(3.6.6)， tensorflow(1.3.0)</p><hr><h2 id="VGG-net实现图片识别-千分类"><a href="#VGG-net实现图片识别-千分类" class="headerlink" title="VGG net实现图片识别(千分类)"></a>VGG net实现图片识别(千分类)</h2><h3 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h3><p>依据论文：《VERY DEEP CONVOLUTIONAL NETWORKS FOR<br>LARGE-SCALE IMAGE RECOGNITION》</p><p>论文下载地址：<br><a href="https://nos.netease.com/edu-lesson-pdfsrc/A259F94F8F75436E482FE1C12B6C177F-1525019670967?Signature=s48UpJM5l4ytnZXNrY9CT8zW6nFN1EfE3C53P61XViA%3D&amp;Expires=1566898884&amp;NOSAccessKeyId=7ba71f968e4340f1ab476ecb300190fa&amp;download=VGGNET%E8%AE%BA%E6%96%87.pdf" target="_blank" rel="noopener">https://nos.netease.com/edu-lesson-pdfsrc/A259F94F8F75436E482FE1C12B6C177F-1525019670967?Signature=s48UpJM5l4ytnZXNrY9CT8zW6nFN1EfE3C53P61XViA%3D&amp;Expires=1566898884&amp;NOSAccessKeyId=7ba71f968e4340f1ab476ecb300190fa&amp;download=VGGNET%E8%AE%BA%E6%96%87.pdf</a></p><p>代码相关文档下载地址：<br><a href="https://nos.netease.com/edu-lesson-pdfsrc/991E299B808DBA0CC42F2323E3A589DE-1525019763996?Signature=iI0%2FwD%2F1MPq0FB%2Be6HNm6B3MJga45rgmjvAFF7EPkm0%3D&amp;Expires=1566898929&amp;NOSAccessKeyId=7ba71f968e4340f1ab476ecb300190fa&amp;download=%E5%8A%A9%E6%95%99%E7%9A%84Tensorflow%E7%AC%94%E8%AE%B08.pdf" target="_blank" rel="noopener">https://nos.netease.com/edu-lesson-pdfsrc/991E299B808DBA0CC42F2323E3A589DE-1525019763996?Signature=iI0%2FwD%2F1MPq0FB%2Be6HNm6B3MJga45rgmjvAFF7EPkm0%3D&amp;Expires=1566898929&amp;NOSAccessKeyId=7ba71f968e4340f1ab476ecb300190fa&amp;download=%E5%8A%A9%E6%95%99%E7%9A%84Tensorflow%E7%AC%94%E8%AE%B08.pdf</a></p><h3 id="一些方法的说明"><a href="#一些方法的说明" class="headerlink" title="一些方法的说明"></a>一些方法的说明</h3><p>tensorflow文档：<br><a href="https://tensorflow.google.cn/" target="_blank" rel="noopener">https://tensorflow.google.cn/</a><br><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">https://www.tensorflow.org/</a></p><p>os.getcwd(): 返回当前目录</p><p>os.path.join(a,b,c,…)：拼出整个路径<br>如：vgg16_path = os.path.join(os.getcwd(), “vgg16.npy”)可获得npy文件的完整路径</p><p>np.save(“name.npy”, 数组)：将某数组写入”name.npy”文件(未压缩的二进制形式，文件默认的扩展名是.npy)。</p><p>variable = np.load(“name.npy”, encoding = “”).item():将name.npy文件读出给varibale变量。<br>encoding可以填”latinl”, “ASCII”, “bytes”，不填默认”ASCII”</p><p>.item():遍历(键值对)。</p><p>np.argsort(列表)： 对列表从小到大排序，返回列表索引值</p><p>tf.shape(a)和a.get_shape()比较：<br>    相同点：都可以得到tensor a的尺寸<br>    不同点：tf.shape()中a的数据类型可以是tensor，list，array；而a.get_shape()中a的数据类型只能是tensor，且返回的是一个元组(tuple)</p><p>tf.nn.bias_add(乘加和，bias): 把bias加到乘加和上。</p><p>tf.reshape(tensor,[n行，m列]):改变tensor的形状<br>tf.reshape(tensor,[-1, m列])：-1表示行跟随m列自动计算</p><p>tf.split(切谁, 怎么切, 在哪个维度切):<br>dimension: 输入张量的哪一个维度，如果是0就表示对第0维进行切割<br>num_split: 切割的数量，切割完每一份是一个列表<br>如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># &apos;value&apos; is a tensor with shape [5, 30]</span><br><span class="line"># Split &apos;value&apos; into 3 tensors with sizes [4, 15, 11] along dimension 1</span><br><span class="line">split0, split1, split2 = tf.split(value, [4, 15, 11], 1)</span><br><span class="line">tf.shape(split0) ==&gt; [5, 4]</span><br><span class="line">tf.shape(split1) ==&gt; [5, 15]</span><br><span class="line">tf.shape(split2) ==&gt; [5, 11]</span><br><span class="line"># Split &apos;value&apos; into 3 tensors along dimension 1</span><br><span class="line">split0, split1, split2 = tf.split(value, num_or_size_splits=3, axis=1)</span><br><span class="line">tf.shape(split0) ==&gt; [5, 10]</span><br></pre></td></tr></table></figure><p>red, greed, blue = tf.split(输入, 3, 3)<br>TF卷积的输入shape为：[batch, 长，宽，深]<br>个人理解：第0维度为batch个图片，第1维度为图片的每行，第2维度为图片的每列，第3维度为图片的每个rgb像素</p><p>tf.concat(concat_dim, values):<br>沿着某一个维度连接tensor<br>如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t1 = [[1, 2, 3], [4, 5, 6]]</span><br><span class="line">t2 = [[7, 8, 9], [10, 11, 12]]</span><br><span class="line">tf.concat(0, [t1, t2]) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11,</span><br><span class="line">12]]</span><br><span class="line">tf.concat(1, [t1, t2]) ==&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</span><br></pre></td></tr></table></figure><p>如果想沿着 r tensor 一新轴连结打包, , 那么可以：<br><code>tf.concat(axis, [tf.expand_dims(t, axis) for t in tensors])</code><br>等同于<br><code>tf.pack(tensors, axis=axis)</code></p><p>fig = plt.figure(“名字”)<br>实例化图对象</p><p>ax = fig.add_subplot(m n k)<br>将画布分隔成m行n列，图像画在从左到右，从上到下的第k块</p><p>img =io.imread(图片路径)<br>读入图片</p><p>ax = fig.add_subplot(x,y,z)<br>x,y,z分别是包含几行，包含几列，当前是第几个</p><p>ax.bar(bar的个数,bar的值,每个bar的名字, bar宽度, bar颜色)<br>可以画出柱状图</p><p>ax.set_ylabel(字符串)<br>设置y轴的名字，如果字符串是中文，有时候需要在字符串前加u</p><p>ax.set_title(字符串)<br>设置子图名字</p><p>ax.text(文字x坐标，文字y坐标， 文字内容， ha = ‘center’, va = ‘bottom’, fontsize = 7)<br>ha水平对齐方式：参数可以为center、right、left<br>va垂直对齐方式：参数可以为center、top、bottom</p><p>ax = imshow(图)<br>画出子图</p><hr><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><ul><li><p>app.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import numpy as np</span><br><span class="line">#Linux服务器没有GUI的情况下使用matplotlib绘图，必须置于pyplot之前</span><br><span class="line">import matplotlib</span><br><span class="line">#matplotlib.use(&apos;Agg&apos;)</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">import vgg16</span><br><span class="line">import utils</span><br><span class="line">from Nclasses import labels</span><br><span class="line"></span><br><span class="line">img_path = input(&apos;Input the path and image name:&apos;)</span><br><span class="line">#调用load_image()函数，对待测试的图像做一些预处理操作</span><br><span class="line">img_ready = utils.load_image(img_path)</span><br><span class="line"></span><br><span class="line">#定义一个figure画图窗口，并制定窗口的名称，也可以设置窗口的大小</span><br><span class="line">fig = plt.figure(u&quot;Top-5 预测结果&quot;)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #定义一个维度为[1,224,224,3]，类型为float32的tensor占位符</span><br><span class="line">    x = tf.placeholder(tf.float32, [1,224,224,3])</span><br><span class="line">    #实例化对象</span><br><span class="line">    vgg = vgg16.Vgg16()</span><br><span class="line">    #调用类的成员方法forward()，并传入待测试图像，即前向传播过程</span><br><span class="line">    vgg.forward(x)</span><br><span class="line">    #将一个batch的数据喂入网络，得到网络的预测输出</span><br><span class="line">    probability = sess.run(vgg.prob, feed_dict = &#123;x:img_ready&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    top5 = np.argsort(probability[0])[-1:-6:-1]</span><br><span class="line">    print(&quot;top5:&quot;, top5)</span><br><span class="line"></span><br><span class="line">    #定义两个list--对应的概率值和实际标签(zebra)</span><br><span class="line">    values = []</span><br><span class="line">    bar_label = []</span><br><span class="line"></span><br><span class="line">    #枚举上面取出的5个索引值</span><br><span class="line">    for n,i in enumerate(top5):</span><br><span class="line">        print(&quot;n:&quot;, n)</span><br><span class="line">        print(&quot;i:&quot;, i)</span><br><span class="line">        values.append(probability[0][i])</span><br><span class="line">        bar_label.append(labels[i])</span><br><span class="line">        #打印属于某个类别的概率</span><br><span class="line">        print(i, &quot;:&quot;, labels[i], &quot;---&quot;, utils.percent(probability[0][i]))</span><br><span class="line">    </span><br><span class="line">    #将画布划分为1行1列，并把下图放入其中</span><br><span class="line">    ax = fig.add_subplot(111)</span><br><span class="line">    #bar()函数绘制柱状图，参数range(len(values))是柱子的下标，values表示柱高的列表(也就是五个预测概率值)</span><br><span class="line">    #tick_label是每个柱子上显示的标签（实际对应的标签）， width是柱子的宽度，fc是柱子的颜色</span><br><span class="line">    ax.bar(range(len(values)), values, tick_label = bar_label, width = 0.5, fc = &apos;g&apos;)</span><br><span class="line">    #设置横轴标签</span><br><span class="line">    ax.set_ylabel(u&apos;probability&apos;)</span><br><span class="line">    #添加标题</span><br><span class="line">    ax.set_title(u&apos;Top-5&apos;)</span><br><span class="line"></span><br><span class="line">    for a,b in zip(range(len(values)), values):</span><br><span class="line">        #在每个柱子的顶端添加对应的预测概率值，a,b表示坐标，b+0.0005表示要把文本信息放置在高于每个柱子顶端0.0005的位置。</span><br><span class="line">        #center是表示文本位于顶端水平方向上的中间位置，bottom是将文本水平放置在柱子顶端垂直放心上的地段位置，fontsize是字号</span><br><span class="line">        ax.text(a, b+0.0005, utils.percent(b), ha = &apos;center&apos;, va = &apos;bottom&apos;, fontsize = 7)</span><br><span class="line"></span><br><span class="line">    #保存图片</span><br><span class="line">    plt.savefig(&apos;./result.jpg&apos;)</span><br><span class="line">    #弹窗展示图像，在服务器上需要将下面这行代码注释掉</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></li><li><p>vgg16.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line">#!usr/bin/python</span><br><span class="line">#coding:utf-8</span><br><span class="line">import inspect</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import time</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">#样本RGB的平均值</span><br><span class="line">VGG_MEAN = [103.939, 155.779, 123.68]</span><br><span class="line"></span><br><span class="line">class Vgg16():</span><br><span class="line">    def __init__(self, vgg16_path = None):</span><br><span class="line">        if vgg16_path is None:</span><br><span class="line">            #os.getcwd() 返回当前工作目录</span><br><span class="line">            vgg16_path = os.path.join(os.getcwd(), &quot;vgg16.npy&quot;)</span><br><span class="line">            print(vgg16_path)</span><br><span class="line">            #遍历键值对，导入模型参数</span><br><span class="line">            self.data_dict = np.load(vgg16_path, encoding = &apos;latin1&apos;).item()</span><br><span class="line">        #遍历data_dict中的每个键</span><br><span class="line">        for x in self.data_dict:</span><br><span class="line">            print(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def conv_layer(self, x, name):</span><br><span class="line">        #定义卷积运算</span><br><span class="line"></span><br><span class="line">        #根据命名空间找到对应的卷积层的网络参数</span><br><span class="line">        with tf.variable_scope(name):</span><br><span class="line">            #读取该层的卷积核</span><br><span class="line">            w = self.get_conv_filter(name)</span><br><span class="line">            #卷积计算</span><br><span class="line">            conv = tf.nn.conv2d(x,w,[1,1,1,1], padding = &quot;SAME&quot;)</span><br><span class="line">            conv_biases = self.get_bias(name)</span><br><span class="line">            #加偏置，激活</span><br><span class="line">            result = tf.nn.relu(tf.nn.bias_add(conv, conv_biases))</span><br><span class="line">            return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def get_conv_filter(self, name):</span><br><span class="line">        #根据命名空间name从参数字典中读取对应的卷积核</span><br><span class="line">        return tf.constant(self.data_dict[name][0], name = &apos;filter&apos;)</span><br><span class="line"></span><br><span class="line">    def get_bias(self, name):</span><br><span class="line">        #根据命名空间name从参数字典中渠道对应的偏置项</span><br><span class="line">        return tf.constant(self.data_dict[name][1], name = &apos;biases&apos;)</span><br><span class="line"></span><br><span class="line">    def max_pool_2x2(self, x, name):</span><br><span class="line">        #定义最大池化操作</span><br><span class="line">        return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding = &apos;SAME&apos;, name = name)</span><br><span class="line"></span><br><span class="line">    def get_fc_weight(self, name):</span><br><span class="line">        return tf.constant(self.data_dict[name][0], name = &quot;weights&quot;)</span><br><span class="line"></span><br><span class="line">    def fc_layer(self, x, name):</span><br><span class="line">        #定义全连接的前向传播计算</span><br><span class="line">        with tf.variable_scope(name):</span><br><span class="line">            #根据命名空间name做全连接计算</span><br><span class="line">            shape = x.get_shape().as_list()</span><br><span class="line">            print(&quot;fc_layer shape&quot;, shape)</span><br><span class="line">            dim = 1</span><br><span class="line">            #计算第1,2,3维度下的元素总和</span><br><span class="line">            for i in shape[1:]:</span><br><span class="line">                dim *= i</span><br><span class="line">            #改变特征图的形状，也就是将得到的多维特征做拉伸操作，只在进入第六层全连接层做该操作</span><br><span class="line">            x = tf.reshape(x, [-1, dim])</span><br><span class="line"></span><br><span class="line">            #读出参数</span><br><span class="line">            w = self.get_fc_weight(name)</span><br><span class="line">            b = self.get_bias(name)</span><br><span class="line">            #对该层输入做加权求和，再加上偏置</span><br><span class="line">            result  = tf.nn.bias_add(tf.matmul(x,w), b)</span><br><span class="line">            return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def forward(self, images):</span><br><span class="line">        #plt.figure(&quot;process pictures&quot;)</span><br><span class="line">        print(&quot;bulid model started&quot;)</span><br><span class="line">        #获取前向传播的开始时间</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        #逐像素乘以255.0(依据原论文所叙述的初始化步骤)</span><br><span class="line">        rgb_scaled = images * 255.0</span><br><span class="line"></span><br><span class="line">        #从GRB转换色彩通道到BGR，也可使用cv中的GRBtoBGR</span><br><span class="line">        red, green, blue = tf.split(rgb_scaled, 3, 3)</span><br><span class="line"></span><br><span class="line">        #断言，用来判断每个操作后的维度变化是否和预期一致</span><br><span class="line">        assert red.get_shape().as_list()[1:] == [224, 224, 1]</span><br><span class="line">        assert green.get_shape().as_list()[1:] == [224, 224, 1]</span><br><span class="line">        assert blue.get_shape().as_list()[1:] == [224, 224, 1]</span><br><span class="line"></span><br><span class="line">        #逐样本减去每个通道的像素平均值，这种操作可以移除图像的平均亮度值，该方法常用在灰度图像上</span><br><span class="line">        bgr = tf.concat([</span><br><span class="line">            blue - VGG_MEAN[0],</span><br><span class="line">            green - VGG_MEAN[1],</span><br><span class="line">            red - VGG_MEAN[2]],</span><br><span class="line">            3)</span><br><span class="line">        assert bgr.get_shape().as_list()[1:] == [224, 224, 3]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #接下来构建VGG的16层网络(包含5段卷积，3层全连接)， 并逐层根据命名空间读取网络参数</span><br><span class="line">        #第一段卷积，含有两个卷积层，后面接最大池化层，用来缩小图片尺寸</span><br><span class="line">        self.conv1_1 = self.conv_layer(bgr, &quot;conv1_1&quot;)</span><br><span class="line"></span><br><span class="line">        #传入命名空间的name，用来获取该层的卷积核和偏置，并做卷积运算，最后返回经过激活函数后的值</span><br><span class="line">        self.conv1_2 = self.conv_layer(self.conv1_1, &quot;conv1_2&quot;)</span><br><span class="line"></span><br><span class="line">        #根据传入的pooling名字对该层做相应的池化操作</span><br><span class="line">        self.pool1 = self.max_pool_2x2(self.conv1_2, &quot;pool1&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #下面的前向传播过程与第一段同理</span><br><span class="line">        # 第二段卷积，同样包含两个卷积层，一个最大池化层</span><br><span class="line">        self.conv2_1 = self.conv_layer(self.pool1, &quot;conv2_1&quot;)</span><br><span class="line">        self.conv2_2 = self.conv_layer(self.conv2_1, &quot;conv2_2&quot;)</span><br><span class="line">        self.pool2 = self.max_pool_2x2(self.conv2_2, &quot;pool2&quot;)</span><br><span class="line"></span><br><span class="line">        #第三段卷积，包含三个卷积层，一个最大池化层</span><br><span class="line">        self.conv3_1 = self.conv_layer(self.pool2, &quot;conv3_1&quot;)</span><br><span class="line">        self.conv3_2 = self.conv_layer(self.conv3_1, &quot;conv3_2&quot;)</span><br><span class="line">        self.conv3_3 = self.conv_layer(self.conv3_2, &quot;conv3_3&quot;)</span><br><span class="line">        self.pool3 = self.max_pool_2x2(self.conv3_3, &quot;pool3&quot;)</span><br><span class="line"></span><br><span class="line">        #第四段卷积，包含三个卷积层，一个最大池化层</span><br><span class="line">        self.conv4_1 = self.conv_layer(self.pool3, &quot;conv4_1&quot;)</span><br><span class="line">        self.conv4_2 = self.conv_layer(self.conv4_1, &quot;conv4_2&quot;)</span><br><span class="line">        self.conv4_3 = self.conv_layer(self.conv4_2, &quot;conv4_3&quot;)</span><br><span class="line">        self.pool4 = self.max_pool_2x2(self.conv4_3, &quot;pool4&quot;)</span><br><span class="line"></span><br><span class="line">        #第五段卷积，包含三个卷积层，一个最大池化层</span><br><span class="line">        self.conv5_1 = self.conv_layer(self.pool4, &quot;conv5_1&quot;)</span><br><span class="line">        self.conv5_2 = self.conv_layer(self.conv5_1, &quot;conv5_2&quot;)</span><br><span class="line">        self.conv5_3 = self.conv_layer(self.conv5_2, &quot;conv5_3&quot;)</span><br><span class="line">        self.pool5 = self.max_pool_2x2(self.conv5_3, &quot;pool5&quot;)</span><br><span class="line"></span><br><span class="line">        #第六层全连接</span><br><span class="line">        #根据命名空间name做加权求和运算</span><br><span class="line">        self.fc6 = self.fc_layer(self.pool5, &quot;fc6&quot;)</span><br><span class="line">        #4096是该层输出后的长度</span><br><span class="line">        assert self.fc6.get_shape().as_list()[1:] == [4096]</span><br><span class="line">        #经过relu激活函数</span><br><span class="line">        self.relu6 = tf.nn.relu(self.fc6)</span><br><span class="line"></span><br><span class="line">        #第七层全连接，同上</span><br><span class="line">        self.fc7 = self.fc_layer(self.relu6, &quot;fc7&quot;)</span><br><span class="line">        self.relu7 = tf.nn.relu(self.fc7)</span><br><span class="line"></span><br><span class="line">        #第八层全连接</span><br><span class="line">        self.fc8 = self.fc_layer(self.relu7, &quot;fc8&quot;)</span><br><span class="line">        #经过最后一层的全连接后，再做softmax分类，得到属于各类别的概率</span><br><span class="line"></span><br><span class="line">        self.prob = tf.nn.softmax(self.fc8, name = &quot;prob&quot;)</span><br><span class="line"></span><br><span class="line">        #得到前向传播的结束时间</span><br><span class="line">        end_time = time.time()</span><br><span class="line">        print(&quot;time consuming:%f&quot;%(end_time - start_time))</span><br><span class="line"></span><br><span class="line">        #清空本次读取到的模型参数字典</span><br><span class="line">        self.data_dict = None</span><br></pre></td></tr></table></figure></li><li><p>utils.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python</span><br><span class="line">#coding:utf-8</span><br><span class="line"></span><br><span class="line">from skimage import io, transform</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from pylab import mpl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#正常显示中文标签</span><br><span class="line">mpl.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;]</span><br><span class="line">#正常显示正负号</span><br><span class="line">mpl.rcParams[&apos;axes.unicode_minus&apos;] = False</span><br><span class="line"></span><br><span class="line">def load_image(path):</span><br><span class="line">    fig = plt.figure(&quot;Center and Resize&quot;)</span><br><span class="line">    #根据传入的路径读入图片</span><br><span class="line">    img = io.imread(path)</span><br><span class="line">    #将像素归一化到[0,1]</span><br><span class="line">    img = img / 255.0</span><br><span class="line"></span><br><span class="line">    #131表示：将该画布分为一行三列，将第一个画布赋给ax0</span><br><span class="line">    ax0 = fig.add_subplot(131)</span><br><span class="line">    #添加子标签</span><br><span class="line">    ax0.set_xlabel(u&apos;Original Picture&apos;)</span><br><span class="line">    #在此部分画布中展示img图片</span><br><span class="line">    ax0.imshow(img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    #找到图像的最短边</span><br><span class="line">    short_edge = min(img.shape[:2])</span><br><span class="line">    #把图像的x和h分别减去最短边，并求平均</span><br><span class="line">    y = (img.shape[0] - short_edge)//2</span><br><span class="line">    x = (img.shape[1] - short_edge)//2</span><br><span class="line">    #取出切分出的中心图像</span><br><span class="line">    crop_img = img[y:y+short_edge, x:x+short_edge]</span><br><span class="line">    print(crop_img)</span><br><span class="line"></span><br><span class="line">    #把画布分为1行3列，并将第2个画布赋给ax1</span><br><span class="line">    ax1 = fig.add_subplot(132)</span><br><span class="line">    #添加子标签</span><br><span class="line">    ax1.set_xlabel(u&apos;Center Picture&apos;)</span><br><span class="line">    ax1.imshow(crop_img)</span><br><span class="line"></span><br><span class="line">    #resize 成固定的imag_size</span><br><span class="line">    re_img = transform.resize(crop_img, (224,224))</span><br><span class="line"></span><br><span class="line">    ax2 = fig.add_subplot(133)</span><br><span class="line">    ax2.set_xlabel(u&apos;Resize Picture&apos;)</span><br><span class="line">    ax2.imshow(re_img)</span><br><span class="line">    img_ready = re_img.reshape((1,224,224,3))</span><br><span class="line">    return img_ready</span><br><span class="line"></span><br><span class="line">def percent(value):</span><br><span class="line">    return &apos;%.2f%%&apos;%(value*100)</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>链接：<a href="https://pan.baidu.com/s/1WWNoY-ahajm2qkcCeNNgqg" target="_blank" rel="noopener">https://pan.baidu.com/s/1WWNoY-ahajm2qkcCeNNgqg</a><br>密码：52b2</p><hr>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 机器学习 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow笔记四</title>
      <link href="/2019/08/23/Tensorflow%E7%AC%94%E8%AE%B0%E5%9B%9B/"/>
      <url>/2019/08/23/Tensorflow%E7%AC%94%E8%AE%B0%E5%9B%9B/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p> mooc 北京大学曹健老师课程：tensorflow笔记 第七节  卷积神经网络 要点记录<br> 版本：python(3.6.6)， tensorflow(1.3.0)</p><hr><h2 id="卷积神经网络相关概念和原理："><a href="#卷积神经网络相关概念和原理：" class="headerlink" title="卷积神经网络相关概念和原理："></a>卷积神经网络相关概念和原理：</h2><ul><li><p>全连接NN：每个神经元与前后相邻层的每一个神经元都有连接关系，输入是特征，输出为预测结果。<br>参数个数：\(\sum(前层 x 后层 + 后层)\)</p><p>大多数时候，我们需要对彩色图片，也就是红绿蓝三通道图片进行处理，相比手写数字识别示例中的单通道有更多参数需要优化，而待优化的参数过多会导致模型过拟合。</p><p>所以实际应用中会先对原始图像进行特征提取，再把提取到的特征喂给全连接网络。</p></li><li><p>卷积(convolutional)<br>卷积就是一种有效提取图像特征的方法。<br>一般会用一个正方形的卷积核，遍历图片上的每个点。图片区域内，相对应的每一个像素值，乘以卷积核内相对应点的权重，求和，再加上偏置。<br>输出图片边长 = (输入图片边长 - 卷积核长 +1 ) / 步长<br>如上例：边长= (5-3 +1)/1 = 3<br>单次计算过程如下图所示：<br><img src="/uploads/tensorflow_notes/image13.png" alt></p><p>滑动计算过程如下图所示：<br><img src="/uploads/tensorflow_notes/image14.gif" alt></p><p>有时候可以在输入图片外围一圈进行全零填充(padding)， 使得输出图片边长跟输入图片边长相等。<br><img src="/uploads/tensorflow_notes/image15.png" alt></p><p>tensorflow中计算卷积(三通道示例)：<br><img src="/uploads/tensorflow_notes/image16.png" alt></p></li></ul><ul><li><p>池化(Pooling)<br>使用卷积处理的特征数量有时候仍然巨大，因此需要进一步减少特征数量。<br>池化用于减少特征数量，池化分为最大值池化和均值池化。<br>最大值池化可提取图片纹理，均值池化可保留背景特征。<br>下图是最大值池化和均值池化的图示：<br><img src="/uploads/tensorflow_notes/image17.png" alt></p><p>tensorflow中计算池化：<br><img src="/uploads/tensorflow_notes/image18.png" alt></p></li><li><p>舍弃(Dropout)<br>有时候为了进一步简化计算，在神经网络<strong>训练</strong>过程中，将一部分神经元按照一定概率从神经网络中舍弃。使用时被舍弃的神经元恢复链接。</p><p>tensorflow中设置dropout<br><code>tf.nn.dropout(上层输出，暂时舍弃的概率)</code><br>训练过程中，指定概率的神经元被随机置零，置零的神经元不参与当前轮的参数优化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if train:</span><br><span class="line">   输出 = tf.nn.dropout(上层输出， 暂时舍弃的概率)</span><br></pre></td></tr></table></figure><p>实际应用中常常在前向前向传播构建神经网络时使用dropout来减小过拟合，来加快模型的训练速度。dropout一般在全连接网络中使用</p></li><li><p>卷积神经网络：借助卷积核(kernel)提取特征后，送入全连接网络。<br>CNN模型主要模块：<br>1.卷积(Convolutional)<br>2.激活(Activation)<br>3.池化(Pooling)<br>4.全连接(FC)<br>其中1-3是高层次抽象特征，精简特征点的过程</p></li></ul><p> CNN模型的发展史：<br> Lenet-5 -&gt; AlexNet -&gt; VGGNet -&gt; GoogleNet -&gt; ResNet -&gt; …</p><hr><h2 id="Lenet-5模型实现手写数字识别"><a href="#Lenet-5模型实现手写数字识别" class="headerlink" title="Lenet-5模型实现手写数字识别"></a>Lenet-5模型实现手写数字识别</h2><p>原始模型：<br><img src="/uploads/tensorflow_notes/image19.png" alt></p><p>对应mnist数据集后修改的模型：<br><img src="/uploads/tensorflow_notes/image20.png" alt></p><p>使用lenet-5模型实现手写数字识别代码示例：</p><p>mnist_lenet5_forward.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">IMAGE_SIZE = 28</span><br><span class="line">NUM_CHANNELS = 1</span><br><span class="line">CONV1_SIZE = 5</span><br><span class="line">CONV1_KERNEL_NUM = 32</span><br><span class="line">CONV2_SIZE  = 5</span><br><span class="line">CONV2_KERNEL_NUM = 64</span><br><span class="line">FC_SIZE = 512   #NN第一层网络有512个神经元</span><br><span class="line">OUTPUT_NODE = 10    #NN第二层网络有10个神经元，对应10分类输出</span><br><span class="line"></span><br><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">    w = tf.Variable(tf.truncated_normal(shape, stddev = 0.1))</span><br><span class="line">    if regularizer != None:</span><br><span class="line">        tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line">def get_bias(shape):</span><br><span class="line">    b = tf.Variable(tf.zeros(shape))</span><br><span class="line">    return b</span><br><span class="line"></span><br><span class="line">def conv2d(x, w):</span><br><span class="line">    #x是输入描述，一维张量的四个参数分别是batch_size， 行分辨率，列分辨率，通道数</span><br><span class="line">    #w是卷积核描述，一维张量四个参数分别是行分辨率，列分辨率，通道数，核个数</span><br><span class="line">    #strides是核滑动步长，一维张量第二个和第三个参数代表行和列步长，其他两个参数固定</span><br><span class="line">    #padding零填充，&apos;SAME&apos;代表使用0填充</span><br><span class="line">    return tf.nn.conv2d(x, w, strides = [1, 1, 1, 1], padding = &apos;SAME&apos;)</span><br><span class="line"></span><br><span class="line">def max_pool_2x2(x):</span><br><span class="line">    #x是输入描述，同上</span><br><span class="line">    #ksize是池化核描述，第1和第4个参数固定为1，第2第3个参数代表大小</span><br><span class="line">    #strides是滑动步长描述，同上</span><br><span class="line">    #padding零填充，同上</span><br><span class="line">    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides = [1,2,2,1], padding = &apos;SAME&apos;)</span><br><span class="line"></span><br><span class="line">def forward(x, train, regularizer):</span><br><span class="line">    conv1_w = get_weight([CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_KERNEL_NUM], regularizer)</span><br><span class="line">    conv1_b = get_bias([CONV1_KERNEL_NUM])</span><br><span class="line">    conv1 = conv2d(x, conv1_w)</span><br><span class="line">    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_b))</span><br><span class="line">    pool1 = max_pool_2x2(relu1)</span><br><span class="line"></span><br><span class="line">    conv2_w = get_weight([CONV2_SIZE, CONV2_SIZE, CONV1_KERNEL_NUM, CONV2_KERNEL_NUM], regularizer)</span><br><span class="line">    conv2_b = get_bias([CONV2_KERNEL_NUM])</span><br><span class="line">    conv2 = conv2d(pool1, conv2_w)</span><br><span class="line">    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_b))</span><br><span class="line">    pool2 = max_pool_2x2(relu2)</span><br><span class="line"></span><br><span class="line">    pool_shape = pool2.get_shape().as_list()</span><br><span class="line">    #计算4维张量每个第2,3,4维元素总和，即长度宽度深度乘积,得到所有特征点的个数</span><br><span class="line">    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]</span><br><span class="line">    #reshape为2维张量，第一维不变(为batch)，第二维元素数改为nodes</span><br><span class="line">    reshaped = tf.reshape(pool2, [pool_shape[0], nodes])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    fc1_w = get_weight([nodes, FC_SIZE], regularizer)</span><br><span class="line">    fc1_b = get_bias([FC_SIZE])</span><br><span class="line">    fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_w) + fc1_b)</span><br><span class="line">    if train: fc1 = tf.nn.dropout(fc1, 0.5)</span><br><span class="line"></span><br><span class="line">    fc2_w = get_weight([FC_SIZE, OUTPUT_NODE], regularizer)</span><br><span class="line">    fc2_b = get_bias([OUTPUT_NODE])</span><br><span class="line">    y = tf.matmul(fc1, fc2_w) + fc2_b</span><br><span class="line">    return y</span><br></pre></td></tr></table></figure><p>mnist_lenet5_backward.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import mnist_lenet5_forward as fw</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = 100</span><br><span class="line">LEARNING_RATE_BASE = 0.005</span><br><span class="line">LEARNING_RATE_DECAY = 0.99</span><br><span class="line">REGULARIZER = 0.0001</span><br><span class="line">STEPS = 50000</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br><span class="line">MODEL_SAVE_PATH = &quot;./model/&quot;</span><br><span class="line">MODEL_NAME = &quot;mnist_model&quot;</span><br><span class="line"></span><br><span class="line">def backward(mnist):</span><br><span class="line">    x = tf.placeholder(tf.float32, [</span><br><span class="line">        BATCH_SIZE,</span><br><span class="line">        fw.IMAGE_SIZE,</span><br><span class="line">        fw.IMAGE_SIZE,</span><br><span class="line">        fw.NUM_CHANNELS</span><br><span class="line">        ])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [None, fw.OUTPUT_NODE])</span><br><span class="line">    y = fw.forward(x, True, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(0, trainable = False)</span><br><span class="line"></span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_, 1))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    loss = cem + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE,</span><br><span class="line">        LEARNING_RATE_DECAY,</span><br><span class="line">        staircase = True</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)</span><br><span class="line"></span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name = &apos;train&apos;)</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)</span><br><span class="line">        if ckpt and ckpt.model_checkpoint_path:</span><br><span class="line">            saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line"></span><br><span class="line">        for i in range(STEPS):</span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            reshaped_xs = np.reshape(xs, (</span><br><span class="line">                BATCH_SIZE,</span><br><span class="line">                fw.IMAGE_SIZE,</span><br><span class="line">                fw.IMAGE_SIZE,</span><br><span class="line">                fw.NUM_CHANNELS</span><br><span class="line">                ))</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict=&#123;x:reshaped_xs, y_:ys&#125;)</span><br><span class="line">            if i % 100 == 0:</span><br><span class="line">                print(&quot;After %d training step(s), loss on training batch is %g&quot; %(step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step = global_step)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    mnist = input_data.read_data_sets(&quot;./data/&quot;, one_hot = True)</span><br><span class="line">    backward(mnist)</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>mnist_lenet5_test.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import time</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import mnist_lenet5_forward as fw</span><br><span class="line">import mnist_lenet5_backward as bw</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">TEST_INTERVAL_SECS = 5</span><br><span class="line"></span><br><span class="line">def test(mnist):</span><br><span class="line">    with tf.Graph().as_default() as g:</span><br><span class="line">        x = tf.placeholder(tf.float32, [</span><br><span class="line">            mnist.test.num_examples,</span><br><span class="line">            fw.IMAGE_SIZE,</span><br><span class="line">            fw.IMAGE_SIZE,</span><br><span class="line">            fw.NUM_CHANNELS</span><br><span class="line">            ])</span><br><span class="line">        y_ = tf.placeholder(tf.float32, [None, fw.OUTPUT_NODE])</span><br><span class="line">        y = fw.forward(x, False, None)</span><br><span class="line"></span><br><span class="line">        ema = tf.train.ExponentialMovingAverage(bw.MOVING_AVERAGE_DECAY)</span><br><span class="line">        ema_restore = ema.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(ema_restore)</span><br><span class="line"></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">        while True:</span><br><span class="line">            with tf.Session() as sess:</span><br><span class="line">                ckpt = tf.train.get_checkpoint_state(bw.MODEL_SAVE_PATH)</span><br><span class="line">                if ckpt and ckpt.model_checkpoint_path:</span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line"></span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.split(&apos;/&apos;)[-1].split(&apos;-&apos;)[-1]</span><br><span class="line">                    reshaped_x = np.reshape(mnist.test.images,(</span><br><span class="line">                        mnist.test.num_examples,</span><br><span class="line">                        fw.IMAGE_SIZE,</span><br><span class="line">                        fw.IMAGE_SIZE,</span><br><span class="line">                        fw.NUM_CHANNELS</span><br><span class="line">                        ))</span><br><span class="line">                    accuracy_score = sess.run(accuracy, feed_dict=&#123;x:reshaped_x, y_:mnist.test.labels&#125;)</span><br><span class="line">                    print(&quot;after %s training step(s), test accuracy = %g&quot; %(global_step, accuracy_score))</span><br><span class="line">                else:</span><br><span class="line">                    print(&quot;No checkpoint file found&quot;)</span><br><span class="line">                    return</span><br><span class="line">            time.sleep(TEST_INTERVAL_SECS)</span><br><span class="line">def main():</span><br><span class="line">    mnist = input_data.read_data_sets(&quot;./data/&quot;, one_hot=True)</span><br><span class="line">    test(mnist)</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>如果运行过程中出现了内存分配的异常，应适当减小BATCH</p><hr>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 机器学习 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow笔记三</title>
      <link href="/2019/08/20/Tensorflow%E7%AC%94%E8%AE%B0%E4%B8%89/"/>
      <url>/2019/08/20/Tensorflow%E7%AC%94%E8%AE%B0%E4%B8%89/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p> mooc 北京大学曹健老师课程：tensorflow笔记 第五、六节  全连接网络基础和实践 要点记录<br> 版本：python(3.6.6)， tensorflow(1.3.0)</p><hr><h2 id="前置准备内容"><a href="#前置准备内容" class="headerlink" title="前置准备内容"></a>前置准备内容</h2><h3 id="MNIST-数据集："><a href="#MNIST-数据集：" class="headerlink" title="MNIST 数据集："></a>MNIST 数据集：</h3><p>提供6w张 28 * 28像素点的0-9手写数字图片和标签，用于训练<br>提供1w张 28 * 28像素点的0-9手写数字图片和标签，用于测试<br><img src="/uploads/tensorflow_notes/image12.png" alt></p><p>图片标签以一维数组形式给出，每个元素表示对应分类出现的概率。<br>比如6：[0,0,0,0,0,0,1,0,0,0]</p><p>导入数据集模块<br><code>from tensorflow.examples.tutorials.mnist import imput_data</code></p><p>加载数据集（数据集不存在则下载，分为train数据集和test数据集），指定数据存取路径，指定以读热码的形式存取：<br><code>mnist = input_data.red_data_sets(&#39;./data/&#39;, one_hot = True)</code><br>注：如果运行报错，说明是源不可达，换一个源即可，具体方法：<br>打开mnist.py文件，参考路径如下：<br>miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py<br>定位到第33行左右，找到SOURCE_URL，将其值改为：<br><code>&#39;http://yann.lecun.com/exdb/mnist/&#39;</code><br>然后重新运行以上命令即可</p><p>获取子集样本数<br><code>mnist.train.num_examples</code><br><code>mnist.validation.num_examples</code><br><code>mnist.test.num_examples</code></p><p>获取标签和数据<br><code>mnist.train.labels[0]</code><br><code>mnist.train.images[0]</code></p><p>随机抽取BATCH_SIZE个数据，喂入神经网络并接收结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = 200</span><br><span class="line">xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">print(&quot;xs shape:&quot;, xs.shape)</span><br><span class="line">print(&quot;ys shape:&quot;, ys.shape)</span><br></pre></td></tr></table></figure><h3 id="几个会用到的函数："><a href="#几个会用到的函数：" class="headerlink" title="几个会用到的函数："></a>几个会用到的函数：</h3><p><code>tf.get_collection(&quot;&quot;)</code><br>从集合中取全部变量，生成一个列表</p><p><code>tf.add_n([])</code><br>列表内对应元素相加</p><p><code>tf.cast(x, dtype)</code><br>把x转化为dtype类型</p><p><code>tf.argmax(x, axis)</code><br>返回最大值所在的索引号，如tf.argmax([1,0,0], 1) 返回0</p><p><code>os.path.join(&quot;home&quot;, &quot;name&quot;)</code><br>返回home/name</p><p>字符串.split()<br>按指定拆分符对字符串切片，分为分割后的列表<br>如：<code>&#39;./model/minst_model-1001&#39;.split(&#39;/&#39;)[-1].split(&#39;-&#39;)[-1]</code> 返回1001</p><p><code>with tf.Graph().as_default() as g:</code><br>其内定义的节点在计算图g中</p><h3 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h3><p>保存模型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        if i % 轮数 == 0:</span><br><span class="line">            saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step = global_step)</span><br></pre></td></tr></table></figure><p>实例化可还原滑动平均值的saver：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(滑动平均基数)</span><br><span class="line">ema_restore = ema.variables_to_restore()</span><br><span class="line">saver = tf.train.Saver(ema_restore)</span><br></pre></td></tr></table></figure><p>说明：这样所有参数通过saver对象在会话中被加载时都会被初始化为滑动平均值.</p><p>加载模型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">    # 加载ckpt，如果saver()设置了滑动平均值，在这里将每个参数设置为其对应的滑动平均值</span><br><span class="line">    ckpt = tf.train.get_chekpoint_state(存储路径)</span><br><span class="line">    # 判断ckpt是否成功加载并有模型</span><br><span class="line">    if ckpt and ckpt.model_checkpoint_path:</span><br><span class="line">        #恢复模型到当前会话</span><br><span class="line">        saver.restore(sess, ckpt.model_checkpoint_path)</span><br></pre></td></tr></table></figure><p>准确率计算方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_, 1))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure><p>说明：这里的y，是一个BATCH * n的二维数组，BATCH是一轮训练喂入的数据个数，n是训练的轮数。 argmax()第二个参数为1，是指只在第一个维度寻找最大值的索引号。取得的结果是在所有n个长度为BATCH的一维数组中，最大的一个数组的索引号。<br>accuracy计算得所有数据准确率的平均值，就是NN在这组数据上的准确率。</p><hr><h2 id="MNIST获得训练过程中的准确率，代码示例："><a href="#MNIST获得训练过程中的准确率，代码示例：" class="headerlink" title="MNIST获得训练过程中的准确率，代码示例："></a>MNIST获得训练过程中的准确率，代码示例：</h2><p>mnist_forward.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 每张图分辨率是28×28,共784个像素点</span><br><span class="line">INPUT_NODE = 784</span><br><span class="line">#输出是0-9这十个数字的概率</span><br><span class="line">OUTPUT_NODE = 10</span><br><span class="line">#隐藏层节点个数</span><br><span class="line">LAYER1_NODE = 500</span><br><span class="line"></span><br><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">    #利用正态分布产生随机值，标准差和均值需要自己设置</span><br><span class="line">    w = tf.Variable(tf.truncated_normal(shape, stddev=0.1))</span><br><span class="line">    #正则化</span><br><span class="line">    if regularizer != None:</span><br><span class="line">        tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line">def get_bias(shape):</span><br><span class="line">    b = tf.Variable(tf.zeros(shape))</span><br><span class="line">    return b</span><br><span class="line"></span><br><span class="line">def forward(x, regularizer):</span><br><span class="line">    w1 = get_weight([INPUT_NODE, LAYER1_NODE], regularizer)</span><br><span class="line">    b1 = get_bias([LAYER1_NODE])</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) +b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight([LAYER1_NODE, OUTPUT_NODE], regularizer)</span><br><span class="line">    b2 = get_bias([OUTPUT_NODE])</span><br><span class="line">    y = tf.matmul(y1, w2) + b2</span><br><span class="line">    return y</span><br></pre></td></tr></table></figure><p>mnist_backward.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import mnist_forward as fw</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = 200</span><br><span class="line">LEARNING_RATE_BASE = 0.1</span><br><span class="line">LEARNING_RATE_DECAY = 0.99</span><br><span class="line">REGULARIZER = 0.0001</span><br><span class="line">STEPS = 50000</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br><span class="line">MODEL_SAVE_PATH = &quot;./model/&quot;</span><br><span class="line">MODEL_NAME = &quot;mnist_model&quot;</span><br><span class="line"></span><br><span class="line">def backward(mnist):</span><br><span class="line">    x = tf.placeholder(tf.float32, [None, fw.INPUT_NODE])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [None, fw.OUTPUT_NODE])</span><br><span class="line">    y = fw.forward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(0, trainable=False)</span><br><span class="line"></span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    loss = cem + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE,</span><br><span class="line">        LEARNING_RATE_DECAY,</span><br><span class="line">        staircase=True    </span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name=&apos;train&apos;)</span><br><span class="line">    </span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        for i in range(STEPS):</span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict = &#123;x:xs, y_:ys&#125;)</span><br><span class="line">            if i % 1000 == 0:</span><br><span class="line">                print(&quot;After %d training steps, loss on training batch is%g.&quot;%(step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    mnist = input_data.read_data_sets(&quot;./data/&quot;, one_hot=True)</span><br><span class="line">    backward(mnist)</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>mnist_test.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import time</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import mnist_forward as fw</span><br><span class="line">import mnist_backward as bw</span><br><span class="line">TEST_INTERVAL_SECS = 5</span><br><span class="line"></span><br><span class="line">def test(mnist):</span><br><span class="line">    with tf.Graph().as_default() as g:</span><br><span class="line">        x = tf.placeholder(tf.float32, [None, fw.INPUT_NODE])</span><br><span class="line">        y_ = tf.placeholder(tf.float32, [None, fw.OUTPUT_NODE])</span><br><span class="line">        y = fw.forward(x, None)</span><br><span class="line"></span><br><span class="line">        ema = tf.train.ExponentialMovingAverage(bw.MOVING_AVERAGE_DECAY)</span><br><span class="line">        ema_restore = ema.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(ema_restore)</span><br><span class="line"></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">        while True:</span><br><span class="line">            with tf.Session() as sess:</span><br><span class="line">                ckpt = tf.train.get_checkpoint_state(bw.MODEL_SAVE_PATH)</span><br><span class="line">                if ckpt and ckpt.model_checkpoint_path:</span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.split(&apos;/&apos;)[-1].split(&apos;-&apos;)[-1]</span><br><span class="line">                    accuracy_score = sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y_:mnist.test.labels&#125;)</span><br><span class="line">                    print(&quot;After %s training step, test accuracy = %g&quot;%(global_step, accuracy_score))</span><br><span class="line">                else:</span><br><span class="line">                    print(&quot;No checkpoint file found&quot;)</span><br><span class="line">                time.sleep(TEST_INTERVAL_SECS)</span><br><span class="line">def main():</span><br><span class="line">    mnist = input_data.read_data_sets(&quot;./data/&quot;, one_hot=True)</span><br><span class="line">    test(mnist)</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>运行mnist_backward.py，然后同时运行mnist_test.py<br>可以查看loss值的变化和准确率的变化</p><p>在mnist_backward.py46行处，初始化所有变量之后增加断点续训代码，即可实现断点续训：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#新增代码：从保存的数据中加载训练结果，实现断点续练</span><br><span class="line">ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)</span><br><span class="line">if ckpt and ckpt.model_checkpoint_path:</span><br><span class="line">    saver.restore(sess, ckpt.model_checkpoint_path)</span><br></pre></td></tr></table></figure><hr><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>要让程序实现特定应用，我们需要解决以下两个问题：<br>如何实现对输入的图片做出正确的预测？<br>如何制作数据集，实现特定应用？</p><ul><li><p>如何实现对输入图片的正确预测</p><p>在上一个例程中，我们用三个文件实现了前向传播、反向传播和测试准确率三个功能。<br>下面这段代码在上面例程三个.py文件的基础上，实现输入图片，输出预测值。</p><p>mnist_app.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"> #coding:utf-8</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line">import mnist_backward as bw</span><br><span class="line">import mnist_forward as fw</span><br><span class="line"></span><br><span class="line">def restore_model(testPicArr):</span><br><span class="line">    #默认创建一个图，在该图中进行以下操作</span><br><span class="line">    with tf.Graph().as_default() as tg:</span><br><span class="line">        x = tf.placeholder(tf.float32, [None, fw.INPUT_NODE])</span><br><span class="line">        y = fw.forward(x, None)</span><br><span class="line">        preValue = tf.argmax(y, 1)</span><br><span class="line"></span><br><span class="line">        variable_averages = tf.train.ExponentialMovingAverage(bw.MOVING_AVERAGE_DECAY)</span><br><span class="line">        variables_to_restore = variable_averages.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(variables_to_restore)</span><br><span class="line"></span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            ckpt = tf.train.get_checkpoint_state(bw.MODEL_SAVE_PATH)</span><br><span class="line"></span><br><span class="line">            if ckpt and ckpt.model_checkpoint_path:</span><br><span class="line">                saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line"></span><br><span class="line">                preValue = sess.run(preValue, feed_dict=&#123;x:testPicArr&#125;)</span><br><span class="line">                return preValue</span><br><span class="line">            else:</span><br><span class="line">                print(&quot;No checkpoint file found&quot;)</span><br><span class="line">                return -1</span><br><span class="line"></span><br><span class="line">def pre_pic(picName):</span><br><span class="line">    img = Image.open(picName)</span><br><span class="line">    # 用消除锯齿的方法重新给图片设定大小</span><br><span class="line">    reIm = img.resize((28,28), Image.ANTIALIAS)</span><br><span class="line">    # 把原始图片转化成灰度图，并以矩阵的形式存到变量里</span><br><span class="line">    im_arr = np.array(reIm.convert(&apos;L&apos;))</span><br><span class="line">    #模型要求黑底白字且每个像素点在0-1取值，输入是白底黑字取值0-255，所以需要进行反色和降值处理</span><br><span class="line">    threshold = 50</span><br><span class="line">    for i in range(28):</span><br><span class="line">        for j in range(28):</span><br><span class="line">            im_arr[i][j] = 255- im_arr[i][j]</span><br><span class="line">            if(im_arr[i][j]&lt;threshold):</span><br><span class="line">                im_arr[i][j] = 0</span><br><span class="line">            else:</span><br><span class="line">                im_arr[i][j] = 255</span><br><span class="line">    nm_arr = im_arr.reshape([1, 784])</span><br><span class="line">    nm_arr = nm_arr.astype(np.float32)</span><br><span class="line">    img_ready = np.multiply(nm_arr, 1.0/255.0)</span><br><span class="line"></span><br><span class="line">    return img_ready</span><br><span class="line"></span><br><span class="line">def application():</span><br><span class="line">    testNum = input(&quot;input the num of test pictures:&quot;)</span><br><span class="line">    for i in range(int(testNum)):</span><br><span class="line">        testPic = input(&quot;the path of test picture:&quot;)</span><br><span class="line">        testPicArr = pre_pic(testPic)</span><br><span class="line">        preValue = restore_model(testPicArr)</span><br><span class="line">        print(&quot;The prediction number is: &quot;, preValue)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    application()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>测试图片下载地址：<a href="https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/num.zip" target="_blank" rel="noopener">https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/num.zip</a><br>注意：由于此网络只有两层，比较简单，自己制作图片识别率可能没有那么高。</p></li><li><p>如何制作数据集，实现特定应用？<br>制作数据集可以使用二进制文件：tfrecords，可以先将图片和标签制作成该格式的文件，然后使用tfrecords进行数据读取，会提高内存利用率</p><p>用tf.train.Example的协议存储训练数据。训练数据的特征用键值对的形式表示。<br>如：’img_raw’:值  ‘label’:值 值是Byteslist/FloatList/Int64List</p><p>用SerializeToString()把数据序列化成字符串存储</p><p>生成tfrecords文件代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#新建一个writer</span><br><span class="line">writer = tf.python_io.TFRecordWriter(tfRecordName)</span><br><span class="line">#把每张图片和标签封装到example中</span><br><span class="line">for 循环遍历每张图片和标签：</span><br><span class="line">    example = tr.train.Example(features = tf.train.Features(feature = &#123;</span><br><span class="line">        &apos;img_raw&apos;:tf.train.Feature(bytes_list = tf.train.BytesList(value = [img_raw])),</span><br><span class="line">        &apos;label&apos;:tf.train.Feature(int64_list=tf.train.Int64List(value = labels))</span><br><span class="line">        &#125;))</span><br><span class="line">    #把example序列化</span><br><span class="line">    writer.write(example.SerializeToString())</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>解析tfrecords 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#新建一个文件队列</span><br><span class="line">filename_queue = tf.train.string_input_producer([tfRecord_path])</span><br><span class="line">#新建一个reader</span><br><span class="line">reader = tf.TFRecordReader()</span><br><span class="line">_, serialized_example = reader.read(filename_queue)</span><br><span class="line">features = tf.parse_singel_example(serialized_example, features = &#123;</span><br><span class="line">    &apos;img_raw&apos;:tf.FixedLenFeature([], tf.string),</span><br><span class="line">    &apos;label&apos;:tf.FixedLenFeature([10], tf.int64)</span><br><span class="line">    &#125;)</span><br><span class="line">img = tf.decode_raw(features[&apos;img_raw&apos;, tf.uint8])</span><br><span class="line">img.set_shape([784])</span><br><span class="line">img = tf.cast(img, tf.float32)*(1.0/255)</span><br><span class="line">label = tf.cast(features[&apos;label&apos;], tf.float32)</span><br></pre></td></tr></table></figure><p>下面将用代码展示如何实现数据集生成和读取。<br>下面的代码仍然解决手写数字识别问题。其中<br>mnist_forward.py和mnist_app.py与之前的代码完全相同<br>mnist_backward.py和mnist_test.py需要更改图片和标签获取的方式<br>新增了数据集生成读取文件: mnist_generateds.py</p></li></ul><p> mnist_forward.py<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">INPUT_NODE = 784</span><br><span class="line">OUTPUT_NODE = 10</span><br><span class="line">LAYER1_NODE = 500</span><br><span class="line"></span><br><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))</span><br><span class="line">    if regularizer != None: tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line">def get_bias(shape):  </span><br><span class="line">    b = tf.Variable(tf.zeros(shape))  </span><br><span class="line">    return b</span><br><span class="line">    </span><br><span class="line">def forward(x, regularizer):</span><br><span class="line">    w1 = get_weight([INPUT_NODE, LAYER1_NODE], regularizer)</span><br><span class="line">    b1 = get_bias([LAYER1_NODE])</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight([LAYER1_NODE, OUTPUT_NODE], regularizer)</span><br><span class="line">    b2 = get_bias([OUTPUT_NODE])</span><br><span class="line">    y = tf.matmul(y1, w2) + b2</span><br><span class="line">    return y</span><br></pre></td></tr></table></figure></p><p> mnist_backward.py<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import mnist_forward as fw</span><br><span class="line">import os</span><br><span class="line">import mnist_generateds as gd</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = 200</span><br><span class="line">LEARNING_RATE_BASE = 0.1</span><br><span class="line">LEARNING_RATE_DECAY = 0.99</span><br><span class="line">REGULARIZER = 0.0001</span><br><span class="line">STEPS = 50000</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br><span class="line">MODEL_SAVE_PATH = &quot;./model/&quot;</span><br><span class="line">MODEL_NAME = &quot;mnist_model&quot;</span><br><span class="line">train_num_examples = 60000</span><br><span class="line"></span><br><span class="line">def backward():</span><br><span class="line">    x = tf.placeholder(tf.float32, [None, fw.INPUT_NODE])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [None, fw.OUTPUT_NODE])</span><br><span class="line">    y = fw.forward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(0, trainable=False)</span><br><span class="line"></span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    loss = cem + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line"></span><br><span class="line">    #1 更改代码</span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        train_num_examples / BATCH_SIZE,</span><br><span class="line">        LEARNING_RATE_DECAY,</span><br><span class="line">        staircase=True    </span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name=&apos;train&apos;)</span><br><span class="line">    </span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    #2 新增代码：</span><br><span class="line">    img_batch, label_batch = gd.get_tfrecord(BATCH_SIZE, isTrain=True)</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        #3.新增代码：从保存的数据中加载训练结果，实现断点续练</span><br><span class="line">        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)</span><br><span class="line">        if ckpt and ckpt.model_checkpoint_path:</span><br><span class="line">            saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line"></span><br><span class="line">        #4.新增代码：开启线程协调器</span><br><span class="line">        coord = tf.train.Coordinator()</span><br><span class="line">        threads = tf.train.start_queue_runners(sess = sess, coord = coord)</span><br><span class="line"></span><br><span class="line">        for i in range(STEPS):</span><br><span class="line">            #5.改动代码：</span><br><span class="line">            xs, ys = sess.run([img_batch, label_batch])</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict = &#123;x:xs, y_:ys&#125;)</span><br><span class="line">            if i % 1000 == 0:</span><br><span class="line">                print(&quot;After %d training steps, loss on training batch is%g.&quot;%(step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)</span><br><span class="line">        #6新增代码：关闭线程协调器</span><br><span class="line">        coord.request_stop()</span><br><span class="line">        coord.join(threads)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    backward()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p><p> mnist_test.py<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import time</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import mnist_forward as fw</span><br><span class="line">import mnist_backward as bw</span><br><span class="line">import mnist_generateds as gd</span><br><span class="line"></span><br><span class="line">TEST_INTERVAL_SECS = 5</span><br><span class="line">TEST_NUM = 10000</span><br><span class="line"></span><br><span class="line">def test():</span><br><span class="line">    with tf.Graph().as_default() as g:</span><br><span class="line">        x = tf.placeholder(tf.float32, [None, fw.INPUT_NODE])</span><br><span class="line">        y_ = tf.placeholder(tf.float32, [None, fw.OUTPUT_NODE])</span><br><span class="line">        y = fw.forward(x, None)</span><br><span class="line"></span><br><span class="line">        ema = tf.train.ExponentialMovingAverage(bw.MOVING_AVERAGE_DECAY)</span><br><span class="line">        ema_restore = ema.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(ema_restore)</span><br><span class="line"></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">        #新增代码1：</span><br><span class="line">        img_batch, label_batch = gd.get_tfrecord(TEST_NUM, True)</span><br><span class="line"></span><br><span class="line">        while True:</span><br><span class="line">            with tf.Session() as sess:</span><br><span class="line">                ckpt = tf.train.get_checkpoint_state(bw.MODEL_SAVE_PATH)</span><br><span class="line">                if ckpt and ckpt.model_checkpoint_path:</span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.split(&apos;/&apos;)[-1].split(&apos;-&apos;)[-1]</span><br><span class="line"></span><br><span class="line">                    #2.新增代码：开启线程协调器</span><br><span class="line">                    coord = tf.train.Coordinator()</span><br><span class="line">                    threads = tf.train.start_queue_runners(sess = sess, coord = coord)</span><br><span class="line"></span><br><span class="line">                    #3.改动代码：</span><br><span class="line">                    xs, ys = sess.run([img_batch, label_batch])</span><br><span class="line"></span><br><span class="line">                    accuracy_score = sess.run(accuracy, feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line">                    print(&quot;After %s training step, test accuracy = %g&quot;%(global_step, accuracy_score))</span><br><span class="line"></span><br><span class="line">                    #4新增代码：关闭线程协调器</span><br><span class="line">                    coord.request_stop()</span><br><span class="line">                    coord.join(threads)</span><br><span class="line">                else:</span><br><span class="line">                    print(&quot;No checkpoint file found&quot;)</span><br><span class="line">                time.sleep(TEST_INTERVAL_SECS)</span><br><span class="line">def main():</span><br><span class="line">    test()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p><p> mnist_app.py<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line">import mnist_backward as bw</span><br><span class="line">import mnist_forward as fw</span><br><span class="line"></span><br><span class="line">def restore_model(testPicArr):</span><br><span class="line">    #默认创建一个图，在该图中进行以下操作</span><br><span class="line">    with tf.Graph().as_default() as tg:</span><br><span class="line">        x = tf.placeholder(tf.float32, [None, fw.INPUT_NODE])</span><br><span class="line">        y = fw.forward(x, None)</span><br><span class="line">        preValue = tf.argmax(y, 1)</span><br><span class="line"></span><br><span class="line">        variable_averages = tf.train.ExponentialMovingAverage(bw.MOVING_AVERAGE_DECAY)</span><br><span class="line">        variables_to_restore = variable_averages.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(variables_to_restore)</span><br><span class="line"></span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            ckpt = tf.train.get_checkpoint_state(bw.MODEL_SAVE_PATH)</span><br><span class="line"></span><br><span class="line">            if ckpt and ckpt.model_checkpoint_path:</span><br><span class="line">                saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line"></span><br><span class="line">                preValue = sess.run(preValue, feed_dict=&#123;x:testPicArr&#125;)</span><br><span class="line">                return preValue</span><br><span class="line">            else:</span><br><span class="line">                print(&quot;No checkpoint file found&quot;)</span><br><span class="line">                return -1</span><br><span class="line"></span><br><span class="line">def pre_pic(picName):</span><br><span class="line">    img = Image.open(picName)</span><br><span class="line">    # 用消除锯齿的方法重新给图片设定大小</span><br><span class="line">    reIm = img.resize((28,28), Image.ANTIALIAS)</span><br><span class="line">    # 把原始图片转化成灰度图，并以矩阵的形式存到变量里</span><br><span class="line">    im_arr = np.array(reIm.convert(&apos;L&apos;))</span><br><span class="line">    #模型要求黑底白字且每个像素点在0-1取值，输入是白底黑字取值0-255，所以需要进行反色和降值处理</span><br><span class="line">    threshold = 50</span><br><span class="line">    for i in range(28):</span><br><span class="line">        for j in range(28):</span><br><span class="line">            im_arr[i][j] = 255- im_arr[i][j]</span><br><span class="line">            if(im_arr[i][j]&lt;threshold):</span><br><span class="line">                im_arr[i][j] = 0</span><br><span class="line">            else:</span><br><span class="line">                im_arr[i][j] = 255</span><br><span class="line">    nm_arr = im_arr.reshape([1, 784])</span><br><span class="line">    nm_arr = nm_arr.astype(np.float32)</span><br><span class="line">    img_ready = np.multiply(nm_arr, 1.0/255.0)</span><br><span class="line"></span><br><span class="line">    return img_ready</span><br><span class="line"></span><br><span class="line">def application():</span><br><span class="line">    testNum = input(&quot;input the num of test pictures:&quot;)</span><br><span class="line">    for i in range(int(testNum)):</span><br><span class="line">        testPic = input(&quot;the path of test picture:&quot;)</span><br><span class="line">        testPicArr = pre_pic(testPic)</span><br><span class="line">        preValue = restore_model(testPicArr)</span><br><span class="line">        print(&quot;The prediction number is: &quot;, preValue)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    application()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p><p> mnist_generateds.py<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">image_train_path = &apos;./mnist_data_jpg/mnist_train_jpg_60000&apos;</span><br><span class="line">label_train_path = &apos;./mnist_data_jpg/mnist_train_jpg_60000.txt&apos;</span><br><span class="line">tfRecord_train = &apos;./data/mnist_train.tfrecords&apos;</span><br><span class="line">image_test_path = &apos;./mnist_data_jpg/mnist_test_jpg_10000/&apos;</span><br><span class="line">label_test_path = &apos;./mnist_data_jpg/mnist_test_jpg_10000.txt&apos;</span><br><span class="line">tfRecord_test = &apos;./data/mnist_test.tfrecords&apos;</span><br><span class="line">data_path = &apos;./data&apos;</span><br><span class="line">resize_height = 28</span><br><span class="line">resize_width = 28</span><br><span class="line"></span><br><span class="line">def write_tfRecord(tfRecordName, image_path, label_path):</span><br><span class="line">    writer = tf.python_io.TFRecordWriter(tfRecordName)</span><br><span class="line">    #用来计数</span><br><span class="line">    num_pic = 0</span><br><span class="line">    #以读方式打开label文件，lebel文件是个txt文件，每行由图片名和标签组成，中间用空格隔开</span><br><span class="line">    f = open(label_path, &apos;r&apos;)</span><br><span class="line">    contents = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line">    for content in contents:</span><br><span class="line">        #用空格分隔每行的内容</span><br><span class="line">        value = content.split()</span><br><span class="line">        #取第一个元素，即为图片名，组成路径</span><br><span class="line">        img_path = image_path + value[0]</span><br><span class="line">        #打开图片并转化成二进制数组</span><br><span class="line">        img = Image.open(img_path)</span><br><span class="line">        img_raw = img.tobytes()</span><br><span class="line">        #初始化labels所有元素为0</span><br><span class="line">        labels = [0]*10</span><br><span class="line">        #把图片对应的标签位赋值为1</span><br><span class="line">        labels[int(value[1])] = 1</span><br><span class="line"></span><br><span class="line">        #创建example</span><br><span class="line">        example = tf.train.Example(features = tf.train.Features(feature = &#123;</span><br><span class="line">            &apos;img_raw&apos;: tf.train.Feature(bytes_list = tf.train.BytesList(value = [img_raw])),</span><br><span class="line">            &apos;label&apos;: tf.train.Feature(int64_list = tf.train.Int64List(value = labels))</span><br><span class="line">            &#125;))</span><br><span class="line">        writer.write(example.SerializeToString())</span><br><span class="line">        num_pic += 1</span><br><span class="line">        print(&quot;the number of picture&quot;, num_pic)</span><br><span class="line">    writer.close()</span><br><span class="line">    print(&quot;write tfrecord successfully&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_tfRecord():</span><br><span class="line">    isExists = os.path.exists(data_path)</span><br><span class="line">    if not isExists:</span><br><span class="line">        os.makedirs(data_path)</span><br><span class="line">        print(&quot;The directory was created successfully&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;Directory already exists&quot;)</span><br><span class="line">    write_tfRecord(tfRecord_train, image_train_path, label_train_path)</span><br><span class="line">    write_tfRecord(tfRecord_test, image_test_path, label_test_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def read_tfRecord(tfRecord_path):</span><br><span class="line">    filename_queue = tf.train.string_input_producer([tfRecord_path], shuffle = True)</span><br><span class="line">    reader = tf.TFRecordReader()</span><br><span class="line">    _, serialized_example = reader.read(filename_queue)</span><br><span class="line">    features = tf.parse_single_example(serialized_example, features = &#123;</span><br><span class="line">        &apos;label&apos; : tf.FixedLenFeature([10], tf.int64),</span><br><span class="line">        &apos;img_raw&apos; : tf.FixedLenFeature([], tf.string)</span><br><span class="line">        &#125;)</span><br><span class="line">    img = tf.decode_raw(features[&apos;img_raw&apos;], tf.uint8)</span><br><span class="line">    img.set_shape([784])</span><br><span class="line">    img = tf.cast(img, tf.float32) * (1.0/255)</span><br><span class="line">    label = tf.cast(features[&apos;label&apos;], tf.float32)</span><br><span class="line">    return img, label</span><br><span class="line"></span><br><span class="line">def get_tfrecord(num, isTrain = True):</span><br><span class="line">    if isTrain:</span><br><span class="line">        tfRecord_path = tfRecord_train</span><br><span class="line">    else:</span><br><span class="line">        tfRecord_path = tfRecord_test</span><br><span class="line">    img, label = read_tfRecord(tfRecord_path)</span><br><span class="line">    img_batch, label_batch = tf.train.shuffle_batch(</span><br><span class="line">        [img, label],</span><br><span class="line">        batch_size = num,</span><br><span class="line">        num_threads = 2, </span><br><span class="line">        capacity = 1000,</span><br><span class="line">        min_after_dequeue = 700)</span><br><span class="line">    return img_batch, label_batch</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    generate_tfRecord()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p><p>数据集下载：<br><a href="https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/fc4.zip" target="_blank" rel="noopener">https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/fc4.zip</a><br><a href="https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/fc4.z01" target="_blank" rel="noopener">https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/fc4.z01</a><br><a href="https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/fc4.z02" target="_blank" rel="noopener">https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/fc4.z02</a><br><a href="https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/fc4.z03" target="_blank" rel="noopener">https://github.com/cj0012/AI-Practice-Tensorflow-Notes/blob/master/fc4.z03</a></p><hr><hr>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 机器学习 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow笔记二</title>
      <link href="/2019/08/15/Tensorflow%E7%AC%94%E8%AE%B0%E4%BA%8C/"/>
      <url>/2019/08/15/Tensorflow%E7%AC%94%E8%AE%B0%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p> mooc 北京大学曹健老师课程：tensorflow笔记 第四节 神经网络优化 要点记录<br> 版本：python(3.6.6)， tensorflow(1.3.0)</p><hr><h2 id="模型、激活函数、NN复杂度"><a href="#模型、激活函数、NN复杂度" class="headerlink" title="模型、激活函数、NN复杂度"></a>模型、激活函数、NN复杂度</h2><p> 常用模型<br><img src="/uploads/tensorflow_notes/image3.png" alt="&quot;模型说明&quot;" title="模型说明"></p><p> 三个常用的激活函数<br><img src="/uploads/tensorflow_notes/image4.png" alt="&quot;激活函数&quot;" title="激活函数"></p><p> NN复杂度多用NN层数和NN参数的个数表示<br> 层数 = 隐藏层的个数+1个输出层（下图层数为2）<br> 总参数 = 总W+总b（下图总参数为：3 * 4 + 4 + 4 *2 + 2 = 26）<br><img src="/uploads/tensorflow_notes/image5.png" alt="&quot;NN复杂度辅图&quot;" title="NN复杂度辅图"></p><hr><h2 id="损失函数-loss-：预测值-y-与已知答案-y-的差距"><a href="#损失函数-loss-：预测值-y-与已知答案-y-的差距" class="headerlink" title="损失函数(loss)：预测值(y)与已知答案(y_)的差距"></a>损失函数(loss)：预测值(y)与已知答案(y_)的差距</h2><p> NN优化目标：loss最小。</p><p> 三种loss计算：均方误差mse(mean squared error)、 自定义、 交叉熵ce(Cross Entropy)</p><h3 id="均方误差MSE：MSE-y-y-frac-sum-i-1-n-y-y-2-n"><a href="#均方误差MSE：MSE-y-y-frac-sum-i-1-n-y-y-2-n" class="headerlink" title="均方误差MSE：MSE(y_, y) = \(\frac{\sum_{i=1}^{n}(y-y\_)^{2}}{n}\)"></a>均方误差MSE：MSE(y_, y) = \(\frac{\sum_{i=1}^{n}(y-y\_)^{2}}{n}\)</h3><p> 均方误差计算损失函数代码：<br> <code>loss = tf.reduce_mean(tf.square(y-y_))</code></p><p> 下面举了一个栗子来说明损失函数：<br> 预测酸奶日销量y。x1、x2是影响日销量的因素。<br> 建模前，应预先采集的数据有：每日x1、x2和销量y_（即已知答案，最佳情况：产量=销量）<br> 拟造数据集X，Y_：y_= x1 + x2  噪声：-0.05 ~ +0.05  拟合可以预测销量的函数。<br> 示例代码<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#假设预测多了与预测少了结果一样</span><br><span class="line">#导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32, 2)</span><br><span class="line">Y_ = [[x1+x2+(rdm.rand()/10-0.05)] for (x1, x2) in X]</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入，参数和输出，定义前向传播过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape = (None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (None, 1))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line">#定义损失函数及反向传播方法。</span><br><span class="line">#定义损失函数为MSE 反向传播方法为梯度下降</span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line">#生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 20000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%32</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i%500 == 0:</span><br><span class="line">            print(&quot;After %d training steps, w1 is:&quot;%(i))</span><br><span class="line">            print(sess.run(w1), &quot;\n&quot;)</span><br><span class="line">    print(&quot;final w1 is:\n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure></p><p>运行以上代码最终结果为：[[0.98019385], [1.0159807 ]]</p><h3 id="自定义损失函数："><a href="#自定义损失函数：" class="headerlink" title="自定义损失函数："></a>自定义损失函数：</h3><p>接着上一个例子说，如果预测商品销量的时候，预测多了，损失成本，预测少了损失利润<br>如果利润≠成本，则mse产生的loss无法将利益最大化。这时候我们可以使用自定义损失函数</p><p>自定义损失函数  loss(y_, y) = \(\sum_{n}f(y,y\_)\)<br>  <img src="/uploads/tensorflow_notes/image6.png" alt></p><p>自定义损失函数示例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#酸奶成本1元，利润9元</span><br><span class="line">#预测少了损失大，所以应该避免预测少。</span><br><span class="line">#导入模块，生成数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">SEED = 23455</span><br><span class="line">COST = 1</span><br><span class="line">PROFIT = 9</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(32, 2)</span><br><span class="line">Y_ = [[x1+x2+(rdm.rand()/10-0.05)] for (x1, x2) in X]</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入，参数和输出，定义前向传播过程。</span><br><span class="line">x = tf.placeholder(tf.float32, shape = (None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (None, 1))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line">#定义损失函数及反向传播方法。</span><br><span class="line">#定义损失函数为MSE 反向传播方法为梯度下降</span><br><span class="line">loss_mse = tf.reduce_sum(tf.where(tf.greater(y, y_), (y-y_)*COST, (y_-y)*PROFIT))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line">#生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 20000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%32</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i%500 == 0:</span><br><span class="line">            print(&quot;After %d training steps, w1 is:&quot;%(i))</span><br><span class="line">            print(sess.run(w1), &quot;\n&quot;)</span><br><span class="line">    print(&quot;final w1 is:\n&quot;, sess.run(w1))</span><br></pre></td></tr></table></figure><p>以上代码只添加了成本和利润两个参数，修改了损失函数。<br>代码运行最终结果为：[[1.020171 ], [1.0425103]] ，可见w1的两个参数都比原来大了，神经网络在尽量多的预测。</p><p>将成本和利润的值互换之后，运行结果为：[[0.9661967 ], [0.97694933]]， 可见神经网络在尽量少的预测。</p><h3 id="交叉熵ce-Cross-Entropy-：表征两个概率分布之间的距离"><a href="#交叉熵ce-Cross-Entropy-：表征两个概率分布之间的距离" class="headerlink" title="交叉熵ce(Cross Entropy)：表征两个概率分布之间的距离"></a>交叉熵ce(Cross Entropy)：表征两个概率分布之间的距离</h3><p> H(y_, y) = \(-\sum\) y_*log y</p><p> 举例说明：二分类 已知答案y_= (1, 0), 预测y1 = (0.6, 0.4)  y2 = (0.8, 0.2) 哪个更接近标准答案？<br> H1((1,0), (0.6, 0.4)) = -(1 * log0.6 + 0 * log0.4) ≈ -(-0.222+0) = 0.222<br> H2((1,0), (0.8, 0.2)) = -(1 * log0.8 + 0 * log0.2) ≈ -(-0.097+0) = 0.097<br> 所以y2预测更准<br> <code>cem = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y, 1e-12, 1.0)))</code></p><p> n分类的n个输出（y1,y2,…yn）通过使用softmax()函数，便满足了概率分布要求：\(\forall P(X=x)\in [0,1] 且\sum_{x}P(X=x)=1\)<br> softmax(yi) = \(\frac{e^{y_{i}}}{\sum_{j=1}^{n}e^{y_{j}}}\)</p><p> tensorflow中代码实现使输出经过softmax函数处理后得到满足概率分布要求的结果，再与标准答案求交叉熵：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.spare_softmax_cross_entropy_with_logits(logits=y, labels = tf.argmax(y_, 1))</span><br><span class="line">cem = tf.reduce_mean(ce)</span><br></pre></td></tr></table></figure></p><p> spare_softmax_cross_entropy_with_logits()的一些说明：<br> logits参数是经softmax()处理前的数据，即神经网络训练得到的数据y。<br> argmax(y_, 1)是获得y_的第2个维度的张量中最大值的索引<br> labels参数是y对应的原始数据x的标签的整数列表，类似[2,3,6,4]，个人瞎邒推测spare…函数将整数列表转换成了one-hot即类似[0,0,1,0]这样的张量(其实也就是y_的one-hot)。<br> spare_softmax_cross_entropy_with_logits()首先用softmax()处理logits，然后计算labels和处理后的logits的交叉熵。</p><hr><h2 id="学习率-learning-rate-：每次参数更新的幅度"><a href="#学习率-learning-rate-：每次参数更新的幅度" class="headerlink" title="学习率(learning_rate)：每次参数更新的幅度"></a>学习率(learning_rate)：每次参数更新的幅度</h2><p><img src="/uploads/tensorflow_notes/image7.png" alt></p><ul><li><p>学习率对传播过程影响示例代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> #conding:utf-8</span><br><span class="line">#设定损失函数 loss = (w+1)^2  ,令w初值为5，反向传播就是求最优w，即求最小loss对应的w值</span><br><span class="line">import tensorflow as tf</span><br><span class="line">#定义带优化参数w，初始值赋为5</span><br><span class="line">w = tf.Variable(tf.constant(5, dtype = tf.float32))</span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss =  tf.square(w+1)</span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#生成会话，训练40轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    for i in range(40):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        print(&quot;After %s steps: w is %f, loss is %f.&quot;%(i, w_val, loss_val))</span><br></pre></td></tr></table></figure><p>更改学习率0.2为1，观察学习情况；更改学习率为0.001，再观察情况。可以直观的看到学习率的影响。</p></li><li><p>学习率设置多少合适？<br>由上例代码可知，学习率大了（比如设置为1）可能会导致震荡不收敛，学习率小了（比如设置0.001）收敛速度慢，所以可以考虑动态学习率。</p></li><li><p>指数衰减学习率<br>learning_rate = LEARNING_RATE_BASE * LEARNING_RATE_DECAY ^ (global_step/LEARNING_RATE_STEP)</p><p>LEARNING_RATE_BASE 指学习率初始值<br>LEARNING_RATE_DECAY 指学习率衰减率(一般取0-1，开区间)<br>global_step 指运行的总轮数<br>LEARNING_RATE_STEP 指多少轮更新一次学习率，计算方式为：总样本数/BATCH_SIZE</p><p>函数代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0, trainable = false)  #记录当前运行轮数的计数器，trainable为False即标注为此参数不可训练</span><br><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    LEARNING_RATE_STEP,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase = True</span><br><span class="line">    )</span><br><span class="line">    #staircase 取True时, global_step/LEARNING_RATE_STEP取整数，学习率阶梯型衰减，取False时，学习率下降沿一条平滑曲线</span><br></pre></td></tr></table></figure><p>指数衰减学习率代码示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"> #coding:utf-8</span><br><span class="line">#设损失函数loss = (w+1)^2, 令w初始值是常数10,，反向传播就是求最优w，即求最小loss对应的w值</span><br><span class="line">#使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度。</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = 0.1  #最初学习率</span><br><span class="line">LEARNING_RATE_DECAY = 0.99 #学习率衰减率</span><br><span class="line">LEARNING_RATE_STEP = 1 #喂入多少轮BATCH_SIZE后，更新一次学习率， 一般设为：总样本数/BATCH_SIZE</span><br><span class="line"></span><br><span class="line">#运行了几轮BATCH_SIZE的计数器，初始值为0, 设为不被训练</span><br><span class="line">global_step = tf.Variable(0, trainable= False)</span><br><span class="line"></span><br><span class="line">#定义指数下降学习率</span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP,</span><br><span class="line"> LEARNING_RATE_DECAY, staircase= True)</span><br><span class="line"></span><br><span class="line">#定义待优化参数w，初始值为0</span><br><span class="line">w = tf.Variable(10, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">#定义损失函数loss</span><br><span class="line">loss = tf.square(w+1)</span><br><span class="line"></span><br><span class="line">#定义反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,</span><br><span class="line"> global_step=global_step)</span><br><span class="line"></span><br><span class="line">#生成会话，训练40轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    for i in range(40):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        learning_rate_val = sess.run(learning_rate)</span><br><span class="line">        global_step_val = sess.run(global_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        print(&quot;After %s steps: global_step is %f, w is %f, learning_rate is %f, loss is %f&quot; </span><br><span class="line">        %(i, global_step_val, w_val, learning_rate_val, loss_val))</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。"><a href="#滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。" class="headerlink" title="滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。"></a>滑动平均（影子值）：记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。</h2><p>针对所有参数进行优化，包括所有的w和b（像是给参数加了影子，参数变化，影子缓慢跟随）<br>计算方法： 影子 = 衰减率 * 影子 + (1- 衰减率) * 参数<br>影子初值 = 参数初值<br>衰减率 = min{MOVING_AVERAGE_DECAY, (1+轮数)/(10+轮数)}<br>MOVING_AVERAGE_DECAY是滑动平均衰减率，是一个超参数</p><p>滑动平均计算过程举例：<br><img src="/uploads/tensorflow_notes/image8.png" alt></p><p>滑动平均计算常用代码：<br>定义滑动平均参数：<br><code>ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</code></p><p>求所有待优化的参数的滑动平均值<br><code>ema_op = ema.apply(tf.trainable_variables())</code><br>ema.apply() 可以求指定参数的滑动平均值<br>tf.trainable_variables()可以将所有待优化的参数汇总成一个列表</p><p>常用以下代码将训练过程和计算滑动平均值绑定成一个训练节点：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">    train_op = tf.no_op(name = &apos;train&apos;)</span><br></pre></td></tr></table></figure></p><p> control_dependencies作用是当你运行train_op时会先运行train_step和ema_op，即设置train_op的依赖。<br> 对tf.no_op的解释引用stackflow上的一句话：</p><blockquote><p>As the documentation says, tf.no_op() does nothing. However, when you create a tf.no_op() inside a with tf.control_dependencies([x, y, z]): block, the op will gain control dependencies on ops x, y, and z. Therefore it can be used to group together a set of side effecting ops, and give you a single op to pass to sess.run() in order to run all of them in a single step.</p></blockquote><p>查看某参数的滑动平均值：<br><code>ema.average(参数名)</code></p><p>示例代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#1. 定义变量及滑动平均类</span><br><span class="line">w1 = tf.Variable(0, dtype = tf.float32)</span><br><span class="line">#定义一个32位浮点变量， 初始值喂0.0, 这个代码就是不断更新w1参数，优化w1参数，滑动平均做了w1的影子</span><br><span class="line">w1 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line">#定义num_updates (NN的迭代轮数), 初始值为0, 不可被优化（训练）</span><br><span class="line">global_step = tf.Variable(0, trainable = False)</span><br><span class="line">#实例化滑动平均类，给衰减率为0.99, 当前轮数global_step</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">#ema.apply()括号里的内容是更新列表，每次运行sess.run(ema_op)时， 对更新列表中的元素求滑动平均值</span><br><span class="line">#在实际应用中会使用tf.trainable_varibales()自动将所有待训练的参数汇总喂列表</span><br><span class="line">#ema_op = ema.apply([w1])</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line">#2. 查看不同迭代中变量取值的变化。</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #初始化</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    #用ema.average(w1)获取w1滑动平均值（要运行多个节点，作为列表中的元素列出，写在sess.run中）</span><br><span class="line">    #打印出当前参数w1和w1的滑动平均值</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    #参数w1的值赋为1</span><br><span class="line">    sess.run(tf.assign(w1, 1))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    #更新step和w1的值，模拟出100轮迭代后，参数w1变为10</span><br><span class="line">    sess.run(tf.assign(global_step, 100))</span><br><span class="line">    sess.run(tf.assign(w1, 10))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line"></span><br><span class="line">    #每次sess.run会更新一次w1的滑动平均值</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br></pre></td></tr></table></figure><hr><h2 id="正则化缓解过拟合"><a href="#正则化缓解过拟合" class="headerlink" title="正则化缓解过拟合"></a>正则化缓解过拟合</h2><p>当模型在训练数据集上的正确率非常高，而很难对新数据集做出正确的相应时，可能是出现了过拟合现象。使用正则化可以有效缓解过拟合。<br>正则化在损失函数中引入模型复杂度指标，利用给w加权值，弱化了训练数据的噪声(一般不正则化b)</p><p>正则化公式：<br>loss = loss(y与y_)① + REGULARIZER② * loss(w)③<br>①指模型中所有参数的损失函数，如：交叉熵、均方误差等。<br>②指用超参数REGULARIZER给出参数w在总loss中的比例，即正则化的权重<br>③是需要正则化的参数<br>loss(w)有两种求法：</p><p>L1正则化：<br>\(loss_{L1}(w) = \sum_{i}|w_{i}|\)<br><code>loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER(w))</code></p><p>L2正则化：<br>\((loss_{L2}(w) = \sum_{i}|w_{i}^{2}|\)<br><code>loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER(w))</code>       </p><p>将得到的loss(w)加到losses集合中：<br><code>tf.add_to_collection(&#39;losses&#39;, tf.contrib.layers.l2_regularizer(regularizer)(w))</code><br>获得最终损失函数：<br><code>loss = cem + tf.add_n(tf.get_collection(&#39;losses&#39;))</code></p><p>正则化示例问题描述：<br>画一条线将红色点和蓝色点隔离开来<br><img src="/uploads/tensorflow_notes/image9.png" alt="&quot;&quot;"></p><p>正则化示例中用到的模块matplotlib介绍：<br><code>import matplotlib as plt</code></p><p><code>plt.scatter(x坐标，y坐标，c=“颜色”)</code><br><code>plt.show()</code><br>上面两个语句用来设置散点，并显示出来：</p><p>下面的语句用来初始化网格，喂入神经网络并得到结果：</p><p><code>xx, yy = np.mgrid[起:止:步长, 起:止:步长]</code><br>隔步长取起止位置之间的所有坐标。</p><p><code>grid = np.c_[xx.ravel(), yy.ravel()]</code><br>先用ravel()将所有横纵坐标拉直，即将所有坐标组成一维张量，然后用np.c_()将两个一维张量纵向(列方向，c为column缩写)组成矩阵，即n行2列的矩阵，即n个坐标。</p><p><code>probs = sess.run(y, feed_dict = {x:gird})</code><br>将grid喂入神经网络，计算得各个点的标志(0或1)，并存到probs中</p><p><code>probs = probs.reshape(xx.shape)</code><br>将probs设置为xx的shape，即n行1列的2维张量</p><p><code>plt.contour(x轴坐标值，y轴坐标值， 该点的高度， levels = [等高线的高度])</code><br><code>plt.show()</code><br>使用上面语句把所有坐标点都设置好高度（0或者1），这样将level设置成两个高度中间值（0.5），就可以将线画出来</p><p>正则化示例代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">BATCH_SIZE = 30</span><br><span class="line">seed = 2</span><br><span class="line"></span><br><span class="line">#基于seed产生随机数</span><br><span class="line">rdm = np.random.RandomState(seed)</span><br><span class="line">#随机数返回300行2列的矩阵，表示300组坐标点（x0, x1）， 作为输入数据集</span><br><span class="line">X = rdm.randn(300, 2)</span><br><span class="line">#从X这个300行2列的矩阵中取出1行，判断如果两个坐标的平方和小于2,给Y赋值1, 其余赋值0，作为输入数据集的标签</span><br><span class="line">Y_ = [int(x0*x0 + x1*x1 &lt;2) for (x0, x1) in X]</span><br><span class="line">#遍历Y中的每个元素，1赋值&apos;red&apos;， 其余赋值&apos;blue&apos;， 这样可视化显示时人可以直观区分</span><br><span class="line">Y_c = [[&apos;red&apos; if y else &apos;blue&apos;] for y in Y_]</span><br><span class="line">#对数据集X和标签Y进行shape整理，第一个元素为-1，表示n行，随第二个参数计算得到， 第二个元素表示多少列，把X整理为n行2列，把Y整理为n行1列</span><br><span class="line">X = np.vstack(X).reshape(-1, 2)</span><br><span class="line">Y_ = np.vstack(Y_).reshape(-1, 1)</span><br><span class="line">print(X)</span><br><span class="line">print(Y_)</span><br><span class="line">print(Y_c)</span><br><span class="line"></span><br><span class="line">#用plt.scatter画出数据集X各行中第0列元素和第一列元素的点，即各行的(x0, x1)，用各行Y_c对应的值表示颜色(c是color的缩写)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=np.squeeze(Y_c))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入，参数和输出，定义前向传播过程</span><br><span class="line"># 生成权重，输入：w的shape和正则化权重</span><br><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line">def get_bias(shape):</span><br><span class="line">    b = tf.Variable(tf.constant(0.01, shape = shape))</span><br><span class="line">    return b</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line"></span><br><span class="line">w1 = get_weight([2,11], 0.01)</span><br><span class="line">b1 = get_bias([11])</span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">w2 = get_weight([11,1], 0.01)</span><br><span class="line">b2 = get_bias([1])</span><br><span class="line">y = tf.matmul(y1,w2) + b2</span><br><span class="line"></span><br><span class="line">#定义损失函数</span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">loss_total = loss_mse + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line"></span><br><span class="line">#定义反向传播方法：不含正则化</span><br><span class="line">train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 40000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%300</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i%2000 == 0:</span><br><span class="line">            loss_mse_v = sess.run(loss_mse, feed_dict=&#123;x:X, y_:Y_&#125;)</span><br><span class="line">            print(&quot;After %d steps, loss is %f&quot;%(i, loss_mse_v))</span><br><span class="line">    #xx在-3到3之间以步长为0.01, yy在-3到3之间以步长0.01,生成二维网格坐标点</span><br><span class="line">    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]</span><br><span class="line">    #将xx，yy拉直，并合并成一个2列的矩阵，得到一个网格坐标点的集合</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    #将网格坐标点喂入神经网络，probs为输出</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    print(probs.size)</span><br><span class="line">    #将probs的shape调整成xx的样子</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;b1:\n&quot;, sess.run(b1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br><span class="line">    print(&quot;b2:\n&quot;, sess.run(b2))</span><br><span class="line">    </span><br><span class="line">plt.scatter(X[:,0], X[:,1], c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx, yy, probs, levels=[.5])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义反向传播方法：含正则化</span><br><span class="line">train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_total)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = 40000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE)%300</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        if i% 2000 == 0:</span><br><span class="line">            loss_v = sess.run(loss_total, feed_dict=&#123;x:X, y_:Y_&#125;)</span><br><span class="line">            print(&quot;After %d steps, loss is: %f&quot;%(i, loss_v))</span><br><span class="line">    </span><br><span class="line">    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;b1:\n&quot;, sess.run(b1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br><span class="line">    print(&quot;b2:\n&quot;, sess.run(b2))</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx, yy, probs, levels = [.5])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行即可看到不使用正则化的结果和使用正则化的结果：<br>未使用正则化：<br><img src="/uploads/tensorflow_notes/image10.png" alt><br>使用正则化：<br><img src="/uploads/tensorflow_notes/image11.png" alt></p><p>可以明显看到，使用正则化能够明显减弱噪点的影响。</p><hr><h2 id="搭建模块化的神经网络八股"><a href="#搭建模块化的神经网络八股" class="headerlink" title="搭建模块化的神经网络八股"></a>搭建模块化的神经网络八股</h2><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>前向传播的过程就是搭建网络，设计网络结构的过程。通常用forward.py文件定义前向传播过程。在文件中通常定义三个函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def forward(x, regularizer):</span><br><span class="line"> w = </span><br><span class="line"> b = </span><br><span class="line"> y = </span><br><span class="line"> return y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def get_weight(shape, regularizer):</span><br><span class="line"> w = tf.Variable()</span><br><span class="line"> tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line"> return w</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def get_bias(shape):</span><br><span class="line"> b = tf.Variable()</span><br><span class="line"> return b</span><br></pre></td></tr></table></figure><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播的过程就是训练网络，优化网络参数的过程。通常用backward.py文件定义反向传播过程。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">def backward():</span><br><span class="line">    x = tf.placeholder(  )</span><br><span class="line">    y_ = tf.placeholder(  )</span><br><span class="line">    y = forward.forward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(0, trainable = false)</span><br><span class="line">    loss = </span><br><span class="line"></span><br><span class="line">    #loss()函数有三种选择：</span><br><span class="line">    #均方误差: </span><br><span class="line">    loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">    #自定义：暂无代码</span><br><span class="line">    #交叉熵：</span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_, 1))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    </span><br><span class="line">    #加入正则化</span><br><span class="line">    loss = 三种损失函数之一 + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br><span class="line"></span><br><span class="line">    #使用指数衰减学习率：</span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    数据集总样本数/BATCH_SIZE,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase=True</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    #定义反向传播训练过程：</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)</span><br><span class="line"></span><br><span class="line">    #计算滑动平均：</span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name = &apos;train&apos;)</span><br><span class="line"></span><br><span class="line">    #with结构初始化所有参数并开始训练</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        for i in range(STEPS):</span><br><span class="line">            sess.run(train_step, feed_dict=&#123;x:, y_:&#125;)</span><br><span class="line">            if i%轮数 == 0:</span><br><span class="line">                print</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;</span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure><h3 id="用模块化思想实现正则化示例的代码："><a href="#用模块化思想实现正则化示例的代码：" class="headerlink" title="用模块化思想实现正则化示例的代码："></a>用模块化思想实现正则化示例的代码：</h3><p>generateds.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">seed = 2</span><br><span class="line">def generateds():</span><br><span class="line">    #基于seed产生随机数</span><br><span class="line">    rdm = np.random.RandomState(seed)</span><br><span class="line">    #随机数(标准正态分布)返回300行2列的矩阵，表示300组坐标点(x0, x1)作为输入数据集</span><br><span class="line">    X = rdm.randn(300, 2)</span><br><span class="line">    #从X这个300行2列的矩阵中取出一行，判断如该这两个坐标的平方和小于2,给Y_赋值1, 其余赋值0</span><br><span class="line">    #作为输入数据集的标签(正确答案)</span><br><span class="line">    Y_ = [(int)(x0*x0 + x1*x1 &lt; 2) for (x0, x1) in X]</span><br><span class="line">    #遍历Y中的每个元素，1赋值&apos;red&apos;其余赋值&apos;blue&apos;，这样可视化显示时人可以直观区分</span><br><span class="line">    Y_c = [[&apos;red&apos; if y else &apos;blue&apos;] for y in Y_]</span><br><span class="line">    #对数据集X和标签Y进行形状整理，第一个元素喂-1表示跟随第二列计算，第二个元素表示多少列。X为n行2列，Y_为n行1列</span><br><span class="line">    X = np.vstack(X).reshape(-1, 2)</span><br><span class="line">    Y_ = np.vstack(Y_).reshape(-1, 1)</span><br><span class="line"></span><br><span class="line">    plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    return X, Y_, Y_c</span><br></pre></td></tr></table></figure><p>forward.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line"># 定义神经网络的输入，参数和输出，定义前向传播过程</span><br><span class="line"></span><br><span class="line">def get_weight(shape, regularizer):</span><br><span class="line">    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    tf.add_to_collection(&apos;losses&apos;, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line">def get_bias(shape):</span><br><span class="line">    b = tf.Variable(tf.constant(0.01, shape=shape))</span><br><span class="line">    return b</span><br><span class="line"></span><br><span class="line">def forward(x, regularizer):</span><br><span class="line">    w1 = get_weight([2,11], regularizer)</span><br><span class="line">    b1 = get_bias([11])</span><br><span class="line">    #计算层需要经过激活函数处理</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight([11,1], regularizer)</span><br><span class="line">    b2 = get_bias([1])</span><br><span class="line">    #结果层不需要经过激活函数处理</span><br><span class="line">    y = tf.matmul(y1, w2) + b2</span><br><span class="line">    return y</span><br></pre></td></tr></table></figure><p>backward.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">#coding:utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import opt4_8_generateds as ge</span><br><span class="line">import opt4_8_forward as fw</span><br><span class="line"></span><br><span class="line">STEPS = 40000</span><br><span class="line">BATCH_SIZE = 30</span><br><span class="line">LEARNING_RATE_BASE = 0.001</span><br><span class="line">LEARNING_RATE_DACY = 0.999</span><br><span class="line">REGULARIZER = 0.01</span><br><span class="line"></span><br><span class="line">def backward():</span><br><span class="line">    x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">    y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line">    </span><br><span class="line">    X, Y_, Y_c = ge.generateds()</span><br><span class="line"></span><br><span class="line">    y = fw.forward(x, REGULARIZER)</span><br><span class="line"></span><br><span class="line">    global_step = tf.Variable(0, trainable=False)</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE, </span><br><span class="line">        global_step,</span><br><span class="line">        300/BATCH_SIZE,</span><br><span class="line">        LEARNING_RATE_DACY,</span><br><span class="line">        staircase= True</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    #定义损失函数</span><br><span class="line">    loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">    loss_total = tf.add_n(tf.get_collection(&apos;losses&apos;)) + loss_mse</span><br><span class="line">    </span><br><span class="line">    #定义包含正则化的反向传播方法</span><br><span class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_total)</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        for i in range(STEPS):</span><br><span class="line">            start = (i*BATCH_SIZE)%300</span><br><span class="line">            end = start + BATCH_SIZE</span><br><span class="line">            sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">            if i % 2000 == 0:</span><br><span class="line">                loss_v = sess.run(loss_total, feed_dict=&#123;x:X, y_:Y_&#125;)</span><br><span class="line">                print(&quot;After %d steps, loss is %f&quot;%(i, loss_v))</span><br><span class="line">        </span><br><span class="line">        xx, yy = np.mgrid[-3:3:0.01, -3:3:0.01]</span><br><span class="line">        grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">        probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">        probs = probs.reshape(xx.shape)</span><br><span class="line">    plt.scatter(X[:,0], X[:,1], c = np.squeeze(Y_c))</span><br><span class="line">    plt.contour(xx, yy, probs, levels = [.5])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure><hr>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 机器学习 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在markdown中使用数学公式</title>
      <link href="/2019/08/14/%E5%9C%A8markdown%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
      <url>/2019/08/14/%E5%9C%A8markdown%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本文介绍使用MathJax引擎在markdown中插入数学公式<br>参考链接：<a href="https://www.jianshu.com/p/054484d0892a" target="_blank" rel="noopener">https://www.jianshu.com/p/054484d0892a</a></p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>首先在markdown头部添加如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>然后到LaTeX网站上写出自己想要的公式，网址：<a href="http://latex.codecogs.com/eqneditor/editor.php" target="_blank" rel="noopener">http://latex.codecogs.com/eqneditor/editor.php</a></p><p>在markdown中有两种插入数学公式的方法：<br>行间公式：<code>$$公式$$</code><br>行内公式(两个\转义成一个\)：<code>\\(公式\\)</code></p><p>将LaTeX网站上生成的公式，放在上面两种方式对应的位置上即可。</p><p><strong>注意网站上生成的公式中如果使用了转义字符，比如原公式中使用了<code>\_</code>来转义成<code>_</code>，那么放到markdown中就需要改成<code>\\_</code>来转义成<code>_</code>。</strong></p><hr>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
          <category> markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow笔记一</title>
      <link href="/2019/08/13/Tensorflow%E7%AC%94%E8%AE%B0%E4%B8%80/"/>
      <url>/2019/08/13/Tensorflow%E7%AC%94%E8%AE%B0%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p> mooc 北京大学曹健老师课程：tensorflow笔记 第三节 Tensorflow框架 要点记录<br> 版本：python(3.6.6)， tensorflow(1.3.0)</p><hr><h2 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h2><h3 id="基于TensorFlow的NN："><a href="#基于TensorFlow的NN：" class="headerlink" title="基于TensorFlow的NN："></a>基于TensorFlow的NN：</h3><p> 用张量表示数据，用计算图搭建神经网络，用回话执行计算图，优化线上的权重（参数），得到模型。</p><h3 id="张量-tensor-：多维数组-列表"><a href="#张量-tensor-：多维数组-列表" class="headerlink" title="张量(tensor)：多维数组(列表)"></a>张量(tensor)：多维数组(列表)</h3><p> 张量的维数称为<strong>阶</strong><br> 0阶张量称为标量(scalar)，如123<br> 1阶张量称为向量(vector)<br> 2阶张量称为矩阵(matrix)<br> 张量可以表示0阶到n阶数组</p><h3 id="计算图-Graph-："><a href="#计算图-Graph-：" class="headerlink" title="计算图(Graph)："></a>计算图(Graph)：</h3><p> 搭建神经网络的计算过程，只搭建，不计算</p><h3 id="会话-Session-："><a href="#会话-Session-：" class="headerlink" title="会话(Session)："></a>会话(Session)：</h3><p> 执行计算图中的节点运算<br> 用python实现计算的语法：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tensorflow.Session() as sess:</span><br><span class="line">   print(sess.run(y))</span><br></pre></td></tr></table></figure></p><h3 id="参数：是指神经元线上的权重，用标量表示，随机赋给初值"><a href="#参数：是指神经元线上的权重，用标量表示，随机赋给初值" class="headerlink" title="参数：是指神经元线上的权重，用标量表示，随机赋给初值"></a>参数：是指神经元线上的权重，用标量表示，随机赋给初值</h3><p> 随机赋值举例：<br> <code>w = tf.Variable(tf.randon_normal([2,3], stddev = 2, mean = 0, seed = 1))</code><br> random_normal表示正态分布，stddev是标准差，mean是平均值， seed是随机种子。<br> 标准差，平均值，随机种子可不写，随机种子不写每次随机的值都不一样。</p><p> 另外几个常用方法：<br> <code>tf.truncated_normal()</code><br> 去掉过大偏离点的正态分布（随机出来的值与平均值差距超过两个标准差，则舍弃）<br> <code>tf.random_uniform()</code><br> 平均分布<br> <code>tf.zeros()</code><br> 生成全0数组，如tf.zeros([3,2], int32)<br> <code>tf.ones()</code><br> 生成全1数组，如tf.ones([3,2], int32)<br> <code>tf.fill()</code><br> 生成定值数组，如tf.fill([3,2], 6)<br> <code>tf.constant()</code><br> 生成指定数组</p><hr><h2 id="神经网络实现过程"><a href="#神经网络实现过程" class="headerlink" title="神经网络实现过程"></a>神经网络实现过程</h2><ol><li>准备数据集，提取特征，作为输入喂给神经网络（Neural Network，NN）</li><li>搭建NN结构，从输入到输出（先搭建计算图，再用会话执行）<br>（NN前向传播算法 -&gt; 计算输出）</li><li>大量特征数据喂给NN，迭代优化NN参数<br>（NN反向传播算法 -&gt; 优化参数训练模型）</li><li>使用训练好的模型预测和分类</li></ol><hr><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p> 示例：<br> <img src="/uploads/tensorflow_notes/image1.png" alt="&quot;示例&quot;" title="前向传播讲解示例"><br> 推导：<br> <img src="/uploads/tensorflow_notes/image2.png" alt="&quot;推导&quot;" title="前向传播示例推导"><br> W矩阵，前面有m个节点，后面有n个节点，则为mXn阶矩阵。<br> 输入不计入神经网络层，a层是第一个计算层，也是神经网络的第一层，这里是1X3阶矩阵</p><p> 说明：</p><ul><li><p>变量初始化、计算图节点运算都要使用会话（with结构）实现<br><code>with tf.Session() as sess</code></p></li><li><p>变量初始化<br><code>init_op = tf.global_variables_initializer()</code><br><code>sess.run(init_op)</code></p></li><li><p>计算图节点运算: sess.run()中写入带运算节点<br><code>sess.run(y)</code></p></li><li><p>给神经网络喂数据：用tf.placeholder占位，在sess.run()中使用feed_dict喂数据<br>喂一组数据：<br><code>x = tf.placeholder(tf.float32, shape=(1,2))</code><br><code>sess.run(y, feed_dict={x:[[0.5, 0.6]]}</code></p><p>喂多组数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">sess.run(y, feed_dict=&#123;x:[[0.1, 0.2], [0.2, 0.3], [0.3, 0.4]]&#125;)</span><br></pre></td></tr></table></figure><p>shape第二个参数是模型的特征数，比如此模型有重量和体积两个特征，故为2。</p></li></ul><p>代码示例1：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> #coding:utf-8</span><br><span class="line">#两层简单神经网络</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#定义输入和参数</span><br><span class="line">x = tf.constant([[0.7, 0.5]])</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,3], stddev = 1, seed=1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3,1], stddev = 1, seed = 1))</span><br><span class="line"></span><br><span class="line">#定义前向传播过程</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">#用会话计算结果</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(&quot;result y in this file is:\n&quot;, sess.run(y))</span><br></pre></td></tr></table></figure></p><p>代码示例2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#两层简单神经网络</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#定义输入和参数</span><br><span class="line">#用placeholder实现输入定义</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(1,2))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed = 1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed = 1))</span><br><span class="line"></span><br><span class="line">#定义前向传播过程</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">#用会话计算结果</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(&quot;y in this file is:\n&quot;, sess.run(y, feed_dict = &#123;x:[[0.7, 0.5]]&#125;))</span><br></pre></td></tr></table></figure><p>代码示例3：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line">#两层简单神经网络(喂多组数据)</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#定义输入和参数</span><br><span class="line">#用placeholder定义输入(sess.run()喂多组数据)</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))</span><br><span class="line"></span><br><span class="line">#定义前向传播过程</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">#调用会话计算结果</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer();</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(&quot;y in this file is:\n&quot;, sess.run(y, feed_dict = &#123;x:[[0.7, 0.5],[0.2, 0.3],[0.3, 0.4], [0.4, 0.5]]&#125;))</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w2))</span><br></pre></td></tr></table></figure><hr><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p> 反向传播是一个不断训练模型参数，在所有参数上使用梯度下降，使NN模型在训练数据上的损失函数最小</p><p> 损失函数(loss)：预测值(y)与已知答案(y_)的差距。 均方误差MSE是计算损失函数的一种方法</p><p> 均方误差MSE：MSE(y_, y) = \(\frac{\sum_{i=1}^{n}(y-y\_)^{2}}{n}\)</p><p> 均方误差计算损失函数代码：<br> <code>loss = tf.reduce_mean(tf.square(y-y_))</code><br> 其中y和y_都是张量</p><p> 反向传播的训练方法：以减小loss值为优化目标<br> 三种训练方法：<br> <code>train_step = tf.train.GiadientDescentOptimizer(learning_rate).minimize(loss)</code><br> <code>train_step = tr.train.MomentumOptimizer(learning_rate, momentum).minimize(loss)</code><br> <code>train_step = tr.train.AdamOptimizer(learning_rate),minimize(loss)</code><br> learning_rate是指学习率，它决定每次更新的幅度，一开始可以选一个较小值，比如0.001</p><p> 反向传播代码示例：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf_8</span><br><span class="line">#反向传播过程实例</span><br><span class="line">#导入模块，生成模拟数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">BATCH_SIZE = 8</span><br><span class="line">seed = 23455</span><br><span class="line"></span><br><span class="line">#基于seed生成随机数</span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line">#随机数返回32行2列的矩阵 表示32组 体积和重量 作为输入数据集</span><br><span class="line">X = rng.rand(32, 2)</span><br><span class="line">#从X这个32行2列的矩阵中 取出一行 判断如果和小于1 给Y赋值1 如该和不小于1 给Y赋值0 作为输入数据集的标签</span><br><span class="line">Y = [[int(x0 + x1 &lt;1)] for (x0, x1) in X]</span><br><span class="line">print(&quot;X:\n&quot;, X)</span><br><span class="line">print(&quot;Y:\n&quot;, Y)</span><br><span class="line"></span><br><span class="line">#定义神经网络的输入</span><br><span class="line">x = tf.placeholder(tf.float32, shape=(None, 2))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(None, 1))</span><br><span class="line"></span><br><span class="line">#定义神经网络的参数</span><br><span class="line">w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))</span><br><span class="line"></span><br><span class="line">#定义神经网络前向传播过程</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">#定义损失函数</span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">#三种反向传播方法</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)</span><br><span class="line">#train_step = tf.train.MomentumOptimizer(0.001, 0.9).minimize(loss)</span><br><span class="line">#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)</span><br><span class="line"></span><br><span class="line">#生成会话，训练STEPS轮</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    #输出当前（未经训练）的参数取值</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    #训练模型</span><br><span class="line">    STEPS = 3000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) %32</span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y[start:end]&#125;)</span><br><span class="line">        if i%500 == 0:</span><br><span class="line">            total_loss = sess.run(loss, feed_dict=&#123;x:X, y_:Y&#125;)</span><br><span class="line">            print(&quot;After %d training steps, loss on all data is %g&quot;%(i, total_loss))</span><br><span class="line">    #输出训练后的参数取值</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(&quot;w1:\n&quot;, sess.run(w1))</span><br><span class="line">    print(&quot;w2:\n&quot;, sess.run(w2))</span><br></pre></td></tr></table></figure></p><hr><h2 id="搭建神经网络的八股：准备、前向传播、反向传播、迭代"><a href="#搭建神经网络的八股：准备、前向传播、反向传播、迭代" class="headerlink" title="搭建神经网络的八股：准备、前向传播、反向传播、迭代"></a>搭建神经网络的八股：准备、前向传播、反向传播、迭代</h2><ul><li><p>准备<br>import 相关模块、 定义常量、 生成数据集</p></li><li><p>前向传播<br>定义输入：特征输入x，标准答案y_<br>定义参数：（一般是随机）定义第一层网络参数w1和第二程网络参数w2<br>定义输出：定义计算图(Graph)，即定义第一层网络a和结果y的计算过程</p></li><li><p>反向传播<br>定义损失函数：loss<br>定义反向传播方法：train_step</p></li><li><p>迭代：生成会话(Session)，训练STEPS轮</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess_run(init_op)</span><br><span class="line">    </span><br><span class="line">    STEPS = 3000</span><br><span class="line">    for i in range(STEPS):</span><br><span class="line">        start = </span><br><span class="line">        end = </span><br><span class="line">        sess.run(train_step, feed_dict=)</span><br></pre></td></tr></table></figure></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 机器学习 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unity用IL2Cpp打包64位版本aab文件，上线Google商店</title>
      <link href="/2019/08/06/Unity%E7%94%A8IL2Cpp%E6%89%93%E5%8C%8564%E4%BD%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E4%B8%8A%E7%BA%BFGoogle%E5%95%86%E5%BA%97/"/>
      <url>/2019/08/06/Unity%E7%94%A8IL2Cpp%E6%89%93%E5%8C%8564%E4%BD%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E4%B8%8A%E7%BA%BFGoogle%E5%95%86%E5%BA%97/</url>
      
        <content type="html"><![CDATA[<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>本文转载自<a href="https://www.cnblogs.com/INSIST-NLJY/p/11044558.html" target="_blank" rel="noopener">https://www.cnblogs.com/INSIST-NLJY/p/11044558.html</a> 部分内容，侵删</p><p>安卓Apk上线Google要求64位且打包成Android Asset Bundle（即aab格式），以下是相关设置</p><blockquote><ol><li>在PlaySettings-&gt;other settings-&gt;Scriptiing Backend 选择IL2CPP（默认是Mono）, c++ Compiler Configuration 选择Release Target Architectures 里面的Arm64就可以勾选的了，勾选打包即可</li></ol></blockquote><blockquote><ol start="2"><li>Android Asset Bundle优化在Build Sttings 勾选Build App Bundle（Google Play）即可，打出的是aab包 打包运行成功。</li></ol></blockquote><blockquote><p>注：要是测试的话就用Mono打包吧，毕竟IL2CPP打包要慢上几倍</p></blockquote><hr><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://www.cnblogs.com/INSIST-NLJY/p/11044558.html" target="_blank" rel="noopener">https://www.cnblogs.com/INSIST-NLJY/p/11044558.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Unity </category>
          
          <category> 安卓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客搭建过程</title>
      <link href="/2019/08/04/%E6%88%91%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/"/>
      <url>/2019/08/04/%E6%88%91%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>此教程记录了我在Win10上搭建Hexo博客的整个流程，主题是使用了Next，奉上文档以及参考链接：<br><a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo文档</a><br><a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">Next文档</a></p><hr><h2 id="二、-基础准备"><a href="#二、-基础准备" class="headerlink" title="二、 基础准备"></a>二、 基础准备</h2><h3 id="1-准备一个Github账号："><a href="#1-准备一个Github账号：" class="headerlink" title="1. 准备一个Github账号："></a>1. 准备一个Github账号：</h3><p>0v0</p><h3 id="2-Git安装与配置："><a href="#2-Git安装与配置：" class="headerlink" title="2. Git安装与配置："></a>2. Git安装与配置：</h3><p> Git安装自不必多说，简单提一下Git用SSH登录Github：</p><ul><li><p>首先打开CMD，设置你的账号和邮箱，输入：<br><code>git config --global user.name &quot;yourname&quot;</code><br><code>git config --global user.email &quot;youremail&quot;</code><br>其中”yourname”是你的github账号，”youremail”是你的github账号邮箱</p></li><li><p>可以用以下两条命令检查输入<br><code>git config user.name</code><br><code>git config user.email</code></p></li><li><p>然后用以下命令创建SSH秘钥<br><code>ssh-keygen -t rsa -C &quot;youremail&quot;</code><br>后续连敲3次回车，不需要任何输入。<br>完成之后会告诉你生成了.ssh文件夹，找到文件夹，其中id_rsa是秘钥，id_rsa.pub是公钥，用文本编辑器打开id_rsa.pub，复制所有内容。</p></li><li><p>添加公钥到Github<br>登录Github，右上角 头像-&gt;Settings -&gt; SSH and GPG keys -&gt; New SSH key。 把公钥粘贴到key中，填好title并点击Add SSH key。</p></li><li><p>回到CMD，输入命令<br><code>ssh -T git@github.com</code><br>选yes，提示成功。</p></li></ul><h3 id="3-安装Nodejs"><a href="#3-安装Nodejs" class="headerlink" title="3. 安装Nodejs"></a>3. 安装Nodejs</h3><ul><li><p>下载网址<br><a href="https://nodejs.org" target="_blank" rel="noopener">nodejs.org</a><br>选LTS就好了，下载完成后安装。</p></li><li><p>安装完成有两个组件，nodejs和npm，可以使用以下命令查看版本<br><code>node -v</code><br><code>npm -v</code></p></li><li><p>用npm安装cnpm淘宝镜像源<br><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code><br>用以下命令查看版本<br><code>cnpm -v</code><br>安装完成以后就都是用cnpm下载了，国内用npm速度你懂的</p></li></ul><h3 id="4-安装博客框架Hexo"><a href="#4-安装博客框架Hexo" class="headerlink" title="4. 安装博客框架Hexo"></a>4. 安装博客框架Hexo</h3><ul><li><p>安装命令<br><code>cnpm install -g hexo-cli</code></p></li><li><p>查看版本<br><code>hexo -v</code></p></li></ul><hr><h2 id="三、-搭建Hexo博客"><a href="#三、-搭建Hexo博客" class="headerlink" title="三、 搭建Hexo博客"></a>三、 搭建Hexo博客</h2><h3 id="1-Hexo所有常用命令"><a href="#1-Hexo所有常用命令" class="headerlink" title="1. Hexo所有常用命令"></a>1. Hexo所有常用命令</h3><p> <a href="https://hexo.io/zh-cn/docs/commands" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/commands</a></p><h3 id="2-新建Blog文件夹"><a href="#2-新建Blog文件夹" class="headerlink" title="2. 新建Blog文件夹"></a>2. 新建Blog文件夹</h3><p> 以后如果有安装错误，可以直接删掉Blog文件夹，从 三、重新开始</p><h3 id="3-CMD定位到Blog文件夹"><a href="#3-CMD定位到Blog文件夹" class="headerlink" title="3. CMD定位到Blog文件夹"></a>3. CMD定位到Blog文件夹</h3><p> 比如我这里是D:\Blog，那么我的命令是<br> <code>d: blog</code></p><h3 id="4-初始化Hexo"><a href="#4-初始化Hexo" class="headerlink" title="4. 初始化Hexo"></a>4. 初始化Hexo</h3><p> <code>hexo init</code><br> 耐心等待初始化完成</p><h3 id="5-第一次启动博客"><a href="#5-第一次启动博客" class="headerlink" title="5. 第一次启动博客"></a>5. 第一次启动博客</h3><p> <code>hexo s</code><br> s是server的简写，命令执行完之后，打开浏览器，输入<br> <code>localhost:4000</code><br> 即可进入本地服务器中的博客。 结束本地服务器按Ctrl+C。</p><h3 id="6-创建博客文章"><a href="#6-创建博客文章" class="headerlink" title="6. 创建博客文章"></a>6. 创建博客文章</h3><p> <code>hexo n &quot;我的第一篇博客文章&quot;</code><br> n是new的简写，创建完成后文章会放到source/_posts文件夹下面，md文件。</p><p> markdown语法自行学习，推荐使用sublime text3写markdown，宇宙第一文本编辑器。</p><p> 打开文件之后，开头的部分内容官方称为Font-matter，这里你可以设定文章标题，文章创建更新时间，标签，分类等等，具体请查阅<br> <a href="https://hexo.io/zh-cn/docs/front-matter" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/front-matter</a></p><p> 另外这里提一个坑，Next主题的文章目录索引是根据markdown的#多少来实现的，举例来说，如果文章中第一个标题是##，那么所有的##都是一级目录索引，所有的###都是二级目录索引，以此类推。<strong>不能越级！！！</strong>，也就是说，##下一级标题必须是###，否则就会出现 2.0.1这种标题，甚至2.0.0.1，非常丑。而且如果你越级了，还在某一级标题的内容里面使用列表来排版，文章目录索引会出现奇怪的bug，虽然在文章里看起来一切正常。Next的目录解析就是这样的，so，最好不要越级使用标题。</p><h3 id="7-博客撰写流程"><a href="#7-博客撰写流程" class="headerlink" title="7. 博客撰写流程"></a>7. 博客撰写流程</h3><ul><li><p>如6所示，new一个博客文章</p></li><li><p>编辑md文件</p></li><li><p>CMD切到Blog文件夹下</p></li><li><p>清理（必要时清理，不必每次都清理）<br><code>hexo cl</code></p></li><li><p>生成<br><code>hexo g</code></p></li><li><p>启动本地服务器<br><code>hexo s</code></p></li><li><p>浏览器打开localhost：4000 查看效果</p></li></ul><h3 id="8-远程部署到Github"><a href="#8-远程部署到Github" class="headerlink" title="8. 远程部署到Github"></a>8. 远程部署到Github</h3><ul><li><p>登录Github，新建一个仓库<br>右上角+号 -&gt; new repository</p></li><li><p>项目命名必须符合要求<br>必须是”yourname.github.io”，比如我的是”brandonvno.github.io”<br>描述写一下，选public，然后点Create Repository</p></li><li><p>为Hexo安装Github部署插件<br>CMD切到Blog，输入命令<br><code>cnpm install --save hexo-deployer-git</code><br>警告不用管</p></li><li><p>编辑Hexo全局设置文件_config.yml<br>打开Blog下的_config.yml，翻到最后，找到deploy，设置参考如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">repo: https://github.com/brandonvno/yourname.github.io.git</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure><p>yourname改成你的Github昵称，下同理。修改完保存。</p></li><li><p>部署到Github<br><code>hexo d</code><br>d是deploy简写。<br>这个过程就是使用你Git中SSH关联的账号往你刚刚新建的仓库中push文件。<br>如果你的GIt没用SSH的话，这里会让你登陆，而且如果你的Git关联的Github账号和你的仓库所有者账号不一样的话，你就会惊奇的发现：我的仓库怎么不是我push的 Σ(っ°Д°;)っ？？？<br>等待部署完成之后，打开yourname.github.io，即可查看效果。<br>通常部署完成之后要等待一段时间，一些效果比如插件才会正常显示。</p></li></ul><hr><h2 id="四、-主题设置"><a href="#四、-主题设置" class="headerlink" title="四、 主题设置"></a>四、 主题设置</h2><p> Hexo主题一览：<a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a><br> 旧版Next文档：<a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">http://theme-next.iissnan.com/</a><br> 旧版Next地址：<a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">https://github.com/iissnan/hexo-theme-next</a><br> 新版Next文档：<a href="https://theme-next.org/docs/" target="_blank" rel="noopener">https://theme-next.org/docs/</a><br> 新版Next地址：<a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">https://github.com/theme-next/hexo-theme-next</a><br> 本博客使用的主题是新版Next，有四种样式，集成了很多第三方功能，十分方便。另外新版文档内容不多，我是参考旧文档搭建的。<br> 如果你不想用Next，那么只需看一下如何安装主题即可，其它功能自行研究，推荐另一个主题：yilia。</p><p> 以下过程会涉及到两个配置文件，一个是位于Blog文件夹下的Hexo全局配置文件_config.yml，另一个是位于Blog/themes/Next 下的主题配置文件_config.yml，下面就分别称<strong>全局配置文件</strong>和<strong>主题配置文件</strong></p><h3 id="1-安装主题"><a href="#1-安装主题" class="headerlink" title="1. 安装主题"></a>1. 安装主题</h3><ul><li><p>CMD切换到Blog文件夹，输入命令<br><code>git clone https://github.com/theme-next/hexo-theme-next.git themes/next</code><br>这个命令执行过程大概如下：现在themes下新建next文件夹，然后将主题克隆到此文件夹下</p></li><li><p>配置Hexo全局设置文件_config.yml<br>找到theme字段，默认是landscape，改成next。<br>注意如果你的主题文件夹名字改了，这里也要相应的改一下。</p></li><li><p>启动本地服务器查看效果</p></li></ul><h3 id="2-Next主题样式切换"><a href="#2-Next主题样式切换" class="headerlink" title="2. Next主题样式切换"></a>2. Next主题样式切换</h3><p> 打开主题配置文件，搜索scheme settings，这里有四种样式，其中三个被注释掉了，选择你喜欢的样式即可。</p><h3 id="3-设置语言"><a href="#3-设置语言" class="headerlink" title="3. 设置语言"></a>3. 设置语言</h3><p> 打开next文件夹下面的language，找到对应的语言，打开全局配置文件，搜索language，将你想设置的语言文件名填上即可。</p><h3 id="4-菜单栏导航设置"><a href="#4-菜单栏导航设置" class="headerlink" title="4. 菜单栏导航设置"></a>4. 菜单栏导航设置</h3><p> 打开主题配置文件，搜索Menu Setting，默认情况下是只有主页和归档栏没有被注释，选择你想要在导航栏显示的内容将注释去掉即可。<br> 同时你也可以打开上一步提到的语言文件，打开即可看到对应的翻译。<br> 这样配置之后，只是打开了这个选项，而选项链接对应的页面还需要手动创建。下面介绍了标签和分类页面的创建方式。</p><h3 id="5-添加标签页面"><a href="#5-添加标签页面" class="headerlink" title="5. 添加标签页面"></a>5. 添加标签页面</h3><ul><li><p>CMD定位到Blog文件夹下，输入以下命令创建标签页面<br><code>hexo new page tags</code></p></li><li><p>打开Blog/source/tags/index.md<br>在data下面添加如下内容<br><code>type: &quot;tags&quot;</code><br>确保第4步开启了标签选项，启动本地服务器后，点击菜单栏标签选项即可跳转到标签页。</p></li><li><p>在博客文章的Font-matter部分添加自定义标签，比如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tags:</span><br><span class="line"> - Testing</span><br><span class="line"> - Another Tag</span><br></pre></td></tr></table></figure><p>即可在标签页看到对应的标签</p></li></ul><h3 id="6-添加分类页面"><a href="#6-添加分类页面" class="headerlink" title="6. 添加分类页面"></a>6. 添加分类页面</h3><ul><li><p>CMD定位到Blog文件夹下，输入以下命令创建分类页面<br><code>hexo new page categories</code></p></li><li><p>打开Blog/source/categories/index.md<br>在data下面添加如下内容<br><code>type: &quot;categories&quot;</code><br>确保第4步开启了分类选项，启动本地服务器后，点击菜单栏分类选项即可跳转到分类页。</p></li><li><p>在博客文章的Font-matter部分添加自定义分类，比如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">categories: </span><br><span class="line"> - Testing</span><br></pre></td></tr></table></figure></li></ul><h3 id="7-分类和标签的区别"><a href="#7-分类和标签的区别" class="headerlink" title="7. 分类和标签的区别"></a>7. 分类和标签的区别</h3><p> 引用Hexo文档中的两句话:</p><blockquote><p>在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 <code>Foo, Bar</code> 不等于 <code>Bar, Foo</code>；而标签没有顺序和层次。</p></blockquote><blockquote><p>Hexo不支持指定多个同级分类。下面的指定方法：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">categories:</span><br><span class="line">- Diary</span><br><span class="line">- Life</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>会使分类Life成为Diary的子分类，而不是并列分类。因此，有必要为您的文章选择尽可能准确的分类。</p></blockquote><p> 原文链接：<a href="https://hexo.io/zh-cn/docs/front-matter" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/front-matter</a></p><h3 id="8-侧栏部分设置"><a href="#8-侧栏部分设置" class="headerlink" title="8. 侧栏部分设置"></a>8. 侧栏部分设置</h3><p>打开主题配置文件，搜索Sidebar Settings</p><ul><li><p>个人社交链接：<br>找到social字段，将自己想要显示的社交平台注释删掉，然后写入自己的社交平台地址即可</p></li><li><p>友情链接<br>找到links字段，将自己想要显示的内容写入即可，格式：<br><code>title: https://something</code></p></li><li><p>头像<br>找到avata字段，注释文档写的很清楚了，我这里翻译一下：<br>放在next/source/images文件夹下，url字段填<code>/images/avatar.gif</code><br>放在Blog/source/uploads文件夹下，url字段填<code>/uploads/avatar.gif</code></p></li></ul><h3 id="9-作者昵称、网站标题、副标题、描述等"><a href="#9-作者昵称、网站标题、副标题、描述等" class="headerlink" title="9. 作者昵称、网站标题、副标题、描述等"></a>9. 作者昵称、网站标题、副标题、描述等</h3><p>打开全局配置文件，在开头Site内容块内修改对应内容即可</p><h3 id="10-打赏功能"><a href="#10-打赏功能" class="headerlink" title="10. 打赏功能"></a>10. 打赏功能</h3><p>打开主题配置文件，搜索Reward，把enable值设为true即可打开，comment填你想说的话。<br>打赏功能只有在具体文章里才会显示</p><h3 id="11-站点统计"><a href="#11-站点统计" class="headerlink" title="11. 站点统计"></a>11. 站点统计</h3><p>Next写好了各种各样的站点统计，有需要用leancloud实现的，但是需要实名注册，比较麻烦，我是使用不蒜子，修改几个参数即可，简单方便。</p><ul><li><p>不蒜子<br>打开主题配置文件，搜索Statistics and Analytics即可看到统计板块。<br>直接搜索busuanzi即可看到不蒜子设置代码。想要显示的内容对应字段改为true即可。</p></li><li><p>百度统计<br>另外也可以使用谷歌统计、百度统计、腾讯统计等，这里简单介绍下百度统计。<br>登录百度统计，一番折腾之后，定位到站点代码获取页面，大概内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> &lt;script&gt;</span><br><span class="line">var _hmt = _hmt || [];</span><br><span class="line">(function() &#123;</span><br><span class="line">  var hm = document.createElement(&quot;script&quot;);</span><br><span class="line">  hm.src = &quot;https://hm.baidu.com/hm.js?xxxxxxxxxxxxxxxxxxxxxxxxxx&quot;;</span><br><span class="line">  var s = document.getElementsByTagName(&quot;script&quot;)[0]; </span><br><span class="line">  s.parentNode.insertBefore(hm, s);</span><br><span class="line">&#125;)();</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>打开主题配置文件，找到baidu_analytics字段，将xxxxxxxx对应的内容填上即可</p></li></ul><h3 id="12-评论系统"><a href="#12-评论系统" class="headerlink" title="12. 评论系统"></a>12. 评论系统</h3><p> 打开主题配置文件，搜索Comments and Widgets即可看到Hexo支持的评论系统。</p><p> 介绍一下Gitalk，基于github的评论系统。</p><ul><li><p>首先在Comments and Widgets下面找到Gitalk字段<br>enable改为true<br>github_id填你的昵称<br>repo填你自己的远程部署项目，如”yourname.github.io”<br>admin_user填你自己。<br>client_id和client_secret需要到Github上去创建。</p></li><li><p>在Github创建Application<br>打开<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">https://github.com/settings/applications/new</a><br>各项填好之后点注册，然后将id和secret拷贝到上一步的位置<br>生成、启动本地服务器，查看效果，完成。</p></li><li><p>再介绍一下来必力评论系统：<br>官网注册：<a href="https://www.livere.com/" target="_blank" rel="noopener">https://www.livere.com/</a><br>安装免费版，设置你的个人博客地址，获取data-uid，打开主题配置文件，搜索livere_uid，填上你的uid，搞定。<br>参考：<a href="https://lemonxq.cn/2017/11/20/Hexo%E4%B9%8B%E4%BD%BF%E7%94%A8Livere%E8%AF%84%E8%AE%BA%E4%BB%A3%E6%9B%BF%E5%A4%9A%E8%AF%B4%E8%AF%84%E8%AE%BA/" target="_blank" rel="noopener">前辈的博文</a></p></li></ul><h3 id="13-搜索系统"><a href="#13-搜索系统" class="headerlink" title="13. 搜索系统"></a>13. 搜索系统</h3><p>打开主题配置文件，搜索Search Services即可看到所有支持的搜索系统，我使用的是local_search<br>安装文档：<a href="https://github.com/theme-next/hexo-generator-searchdb" target="_blank" rel="noopener">https://github.com/theme-next/hexo-generator-searchdb</a><br>讲的很清楚了，注意的是安装完插件之后记得<code>hexo g</code>一下，否则找不到search.xml<br>找到local_search字段，对应的字段填好就完事了。<br>设置好之后启动本地服务器可以在菜单栏看到一个小小的搜索，点开弹出一个大大的弹框。</p><h3 id="14-RSS"><a href="#14-RSS" class="headerlink" title="14. RSS"></a>14. RSS</h3><p>什么是RSS？我就不班门弄斧了=。=<br>配置RSS需要安装一个插件，安装文档：<a href="https://github.com/hexojs/hexo-generator-feed" target="_blank" rel="noopener">https://github.com/hexojs/hexo-generator-feed</a><br>讲的很清楚了，注意的是安装完插件之后记得<code>hexo g</code>一下，否则找不到atom.xml<br>至于主题配置文件里的rss字段，注释讲可以留空。<br>配置完之后启动本地服务器，如果左侧导航栏出现RSS，点击之后进入到一个写满HTML代码的页面，说明安装成功了。<br>可以自行安装一个RSS阅读器订阅一下，我用的是irreader。</p><h3 id="15-动画"><a href="#15-动画" class="headerlink" title="15. 动画"></a>15. 动画</h3><p>打开主题配置文件，搜索Animation Settings可以看到支持的动画。<br>motion是刚进入博客时各个板块位移的动画，考虑加载速度可以选择关闭<br>three需要安装第三方插件：<a href="https://github.com/theme-next/theme-next-three" target="_blank" rel="noopener">https://github.com/theme-next/theme-next-three</a><br>canvas_net同样也需要安装，但是根据注释的地址安装完之后并没有卵用= =，感兴趣的可以自己试试。</p><h3 id="16-国内外分流以及被百度引擎收录"><a href="#16-国内外分流以及被百度引擎收录" class="headerlink" title="16. 国内外分流以及被百度引擎收录"></a>16. 国内外分流以及被百度引擎收录</h3><p>留个坑，日后有时间再研究<br>参考：<a href="https://blog.csdn.net/sinat_37781304/article/details/82729029" target="_blank" rel="noopener">https://blog.csdn.net/sinat_37781304/article/details/82729029</a></p><h3 id="17-live2d萌妹子"><a href="#17-live2d萌妹子" class="headerlink" title="17. live2d萌妹子"></a>17. live2d萌妹子</h3><p>安装文档：<a href="https://github.com/EYHN/hexo-helper-live2d" target="_blank" rel="noopener">https://github.com/EYHN/hexo-helper-live2d</a></p><p>安装命令： <code>npm install --save hexo-helper-live2d</code></p><p>在全局配置文件中追加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">live2d:</span><br><span class="line">  enable: true</span><br><span class="line">  scriptFrom: local</span><br><span class="line">  # pluginRootPath: live2dw/</span><br><span class="line">  # pluginJsPath: lib/</span><br><span class="line">  # pluginModelPath: public/</span><br><span class="line">  log: false</span><br><span class="line">  model: </span><br><span class="line">    # use: live2d-widget-model-koharu</span><br><span class="line">    scale: 1</span><br><span class="line">    hHeadPos: 0.5</span><br><span class="line">    vHeadPos: 0.618</span><br><span class="line">  display:</span><br><span class="line">    superSample: 2</span><br><span class="line">    width: 200</span><br><span class="line">    height: 400</span><br><span class="line">    position: right</span><br><span class="line">    hOffset: 0</span><br><span class="line">    vOffset: -20</span><br><span class="line"> # mobile:</span><br><span class="line"> #    show: false</span><br><span class="line">  react:</span><br><span class="line">    opacityDefault: 0.5</span><br><span class="line">    opacityOnHover: 0.5</span><br><span class="line">    opacity: 0.7</span><br></pre></td></tr></table></figure><p>mobile不注释掉会报错= =</p><p>下面介绍一下更换模型<br>模型一览：<a href="https://huaji8.top/post/live2d-plugin-2.0/" target="_blank" rel="noopener">https://huaji8.top/post/live2d-plugin-2.0/</a><br>安装方式：<code>npm install {your model&#39;s package name}</code>  比如：<code>npm install live2d-widget-model-koharu</code>  然后到全局配置文件中把model.use 对应的值改成模型名字即可。<br>模型列表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">live2d-widget-model-chitose</span><br><span class="line">live2d-widget-model-epsilon2_1</span><br><span class="line">live2d-widget-model-gf</span><br><span class="line">live2d-widget-model-haru/01 (use npm install --save live2d-widget-model-haru)</span><br><span class="line">live2d-widget-model-haru/02 (use npm install --save live2d-widget-model-haru)</span><br><span class="line">live2d-widget-model-haruto</span><br><span class="line">live2d-widget-model-hibiki</span><br><span class="line">live2d-widget-model-hijiki</span><br><span class="line">live2d-widget-model-izumi</span><br><span class="line">live2d-widget-model-koharu</span><br><span class="line">live2d-widget-model-miku</span><br><span class="line">live2d-widget-model-ni-j</span><br><span class="line">live2d-widget-model-nico</span><br><span class="line">live2d-widget-model-nietzsche</span><br><span class="line">live2d-widget-model-nipsilon</span><br><span class="line">live2d-widget-model-nito</span><br><span class="line">live2d-widget-model-shizuku</span><br><span class="line">live2d-widget-model-tororo</span><br><span class="line">live2d-widget-model-tsumiki</span><br><span class="line">live2d-widget-model-unitychan</span><br><span class="line">live2d-widget-model-wanko</span><br><span class="line">live2d-widget-model-z16</span><br></pre></td></tr></table></figure><hr><h2 id="五、-将博客源文件上传到Github"><a href="#五、-将博客源文件上传到Github" class="headerlink" title="五、 将博客源文件上传到Github"></a>五、 将博客源文件上传到Github</h2><p>此部分内容为转载，非原创。<br>原文：<a href="https://blog.csdn.net/sinat_37781304/article/details/82729029" target="_blank" rel="noopener">https://blog.csdn.net/sinat_37781304/article/details/82729029</a></p><blockquote><p>机制：由于<code>hexo d</code>上传部署到github的其实是hexo编译后的文件，是用来生成网页的，不包含源文件，其他文件 ，包括我们写在source 里面的，和配置文件，主题文件，都没有上传到github</p></blockquote><blockquote><p>所以可以利用git的分支管理，将源文件上传到github的另一个分支即可。<br>首先，先在github上新建一个hexo分支，然后在这个仓库的settings中，选择默认分支为hexo分支（这样每次同步的时候就不用指定分支，比较方便）。<br>然后使用sourceTree将仓库克隆到本地，把除了.git的全部内容都删除掉。把之前我们写的博客源文件全部复制过来，除了.deploy_git。这里应该说一句，复制过来的源文件应该有一个.gitignore，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下内容，表示这些类型文件不需要同步<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> .DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules/</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>注意，如果你之前theme中的主题是通过克隆安装的，那么应该把主题文件中的.git文件夹删掉（或者为了以后方便更新主题，可以在本地快速打包一个.git压缩文件，需要更新的时候解压出来然后git pull），因为git不能嵌套上传，最好是显示隐藏文件，检查一下有没有，否则上传的时候会出错，导致你的主题文件无法上传，这样你的配置在别的电脑上就用不了了。安装的第三方插件同理。<br>然后push到远端。上传完了，可以去你的github上看一看hexo分支有没有上传上去，其中node_modules、public、db.json已经被忽略掉了，没有关系，不需要上传的，因为在别的电脑上需要重新输入命令安装 。</p></blockquote><blockquote><p>到新电脑上，重新搭建环境：安装git、设置git全局邮箱和用户名、设置ssh key、安装nodejs、安装hexo，但是已经不需要初始化了，而是在使用sourceTree将远端内容克隆到本地，然后cnpm安装deploy工具，开始写博客。</p></blockquote><hr><h2 id="六、-写作技巧、踩坑记录等杂项（持续更新）"><a href="#六、-写作技巧、踩坑记录等杂项（持续更新）" class="headerlink" title="六、 写作技巧、踩坑记录等杂项（持续更新）"></a>六、 写作技巧、踩坑记录等杂项（持续更新）</h2><h3 id="1-文章中插入图片"><a href="#1-文章中插入图片" class="headerlink" title="1. 文章中插入图片"></a>1. 文章中插入图片</h3><p>插入图片有两种方式，一种是使用网上的图片，需要借助一些七牛云存储，另一种是放在本地。<br>先写一写放在本地：因为我们使用Github托管博客源文件，也就不怕本地丢失了。</p><p>方法：<br>首先在Blog/source文件夹下新建一个文件夹，名字随意，我这里就用Next的uploads了。<br>然后markdown插入图片语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">![图片alt](图片地址 &apos;&apos;图片title&apos;&apos;)</span><br><span class="line">图片alt就是显示在图片下面的文字，相当于对图片内容的解释。</span><br><span class="line">图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加</span><br><span class="line">地址写/uploads/图片名字.type</span><br></pre></td></tr></table></figure><p>比如<code>![&quot;星际牛仔&quot;](/uploads/image1.jpg &quot;公路全家福&quot;)</code>效果如下<br><img src="/uploads/image1.jpg" alt="&quot;星际牛仔&quot;" title="公路全家福"></p><h3 id="2-文章中插入网易云音乐"><a href="#2-文章中插入网易云音乐" class="headerlink" title="2. 文章中插入网易云音乐"></a>2. 文章中插入网易云音乐</h3><p>网易云用网页打开，可以看到生成外链播放器，点击复制代码即可，这里分享两个星际牛仔的原声</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=592701&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=592701&auto=0&height=66"></iframe><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=22767373&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=22767373&auto=0&height=66"></iframe><h3 id="3-文章中插入B站视频"><a href="#3-文章中插入B站视频" class="headerlink" title="3. 文章中插入B站视频"></a>3. 文章中插入B站视频</h3><p>找到B站视频，点分享，找到嵌入代码，拷贝出aid和cid，然后填入下面代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe id=sbrxp src=&quot;//player.bilibili.com/player.html?aid=488321&amp;cid=735286&amp;page=7&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot; style=&quot;width: 800px; height: 600px; max-width: 100%&quot;&gt; &lt;/iframe&gt;</span><br></pre></td></tr></table></figure><p>效果如下：</p><iframe id="sbrxp" src="//player.bilibili.com/player.html?aid=488321&cid=735286&page=7" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 800px; height: 600px; max-width: 100%"> </iframe><h3 id="4-首页文章折叠"><a href="#4-首页文章折叠" class="headerlink" title="4. 首页文章折叠"></a>4. 首页文章折叠</h3><p>Text主题默认首页文章是显示全文的，非常反人类，修改方法是打开Text的主题配置文件_config.yml，搜索auto_excerpt，值改为true即可</p><hr><h2 id="七、-参考链接："><a href="#七、-参考链接：" class="headerlink" title="七、 参考链接："></a>七、 参考链接：</h2><p>谢谢你一直看到这里。<br><a href="https://www.bilibili.com/video/av44544186" target="_blank" rel="noopener">https://www.bilibili.com/video/av44544186</a><br><a href="https://www.codesheep.cn" target="_blank" rel="noopener">https://www.codesheep.cn</a><br><a href="https://blog.csdn.net/sinat_37781304/article/details/82729029" target="_blank" rel="noopener">https://blog.csdn.net/sinat_37781304/article/details/82729029</a><br><a href="https://blog.csdn.net/LemonXQ/article/details/72676005" target="_blank" rel="noopener">https://blog.csdn.net/LemonXQ/article/details/72676005</a></p><hr>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
          <category> 博客搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 博客搭建 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
